[{"categories":["customization","paige"],"date":"2023-09-18T21:21:05-07:00","description":"Demonstration of a customization that shows all data.","keywords":null,"link":"/customizations/show/","tags":["show","singles"],"text":"This page shows some data that the rest of the site hides. Parameters This page has the following parameters: paige: file_link: disable: false style: | #paige-authors, #paige-credit, #paige-date, #paige-file, #paige-keywords, #paige-prev-next, #paige-reading-time, #paige-series, #paige-toc, .paige-authors, .paige-date, .paige-keywords, .paige-reading-time, .paige-series, .paige-summary { display: block; }","title":"Show Data"},{"categories":["customization","paige"],"date":"2023-09-18T20:21:05-07:00","description":"Demonstration of a customization that styles all data.","keywords":null,"link":"/customizations/style/","tags":["singles","style"],"text":"This page styles some data differently than the rest of the site. Parameters This page has the following parameters: paige: file_link: disable: false style: | #paige-authors, #paige-credit, #paige-date, #paige-file, #paige-keywords, #paige-prev-next, #paige-reading-time, #paige-series, #paige-toc, .paige-authors, .paige-date, .paige-keywords, .paige-reading-time, .paige-series, .paige-summary { display: block; } #paige-authors { font-style: italic; } #paige-content { font-style: italic; } #paige-date { font-style: italic; } #paige-description { font-style: italic; } #paige-file { font-style: italic; } #paige-keywords { font-style: italic; } #paige-reading-time { font-style: italic; } #paige-series { font-style: italic; } #paige-title { font-style: italic; } #paige-toc { font-style: italic; } #paige-copyright { font-style: italic; } #paige-credit { font-style: italic; } .paige-authors { font-style: italic; } .paige-date { font-style: italic; } .paige-keyword { font-style: italic; } .paige-series { font-style: italic; } .paige-summary { font-style: italic; }","title":"Style Data"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"Book provides great insights what it means to be a senior level engineer","keywords":null,"link":"/books/software-profession/staff-engineers-path/","tags":null,"text":"I. THE BIG PICTURE Chapter 1: What Would You Say You Do Here? Staff : 1st line manager, Senior Staff : Senior Manager; Principle : Director Do we even need a staff engineer position? a job title vests authority in people who might not automatically receive saves them the time and energy they would otherwise have to spend proving themselves again and again Why do we staff need to see big picture? To find a ==global maximum==, which is the good for the org and not 1 team over time Good decisions need context. Experienced engineers know that the answer to most technology choices is “it depends.” Knowing the pros and cons of a particular technology isn’t enough—you need to know the local details too. What are you trying to do? How much ==time, money, and patience== do you have? What’s your ==risk tolerance==? What does the business need? What’s the staff engineer’s role across projects? Use their experience to ==anticipate risks and ask hard questions== When the project gets stuck, they have enough perspective to ==track down the causes and unblock== Evengalize: telling the story of what’s happening and why, selling the vision to the rest of the company, and explaining what the work will make possible and how the new project affects everyone Why the staff engineer needs to be good at influencing Within engineer circle, Staff engineers are ==role models==. Managers may be responsible for setting culture on their teams, enforcing good behavior, and ensuring standards are met. But engineering norms are set by the behavior of the most respected engineers on the project. What is your Role? Staff can report to VP, Directors, Senior Manager, Manager Which level one is on influences what problem you are asked of what information you are privy to how much time you may get from your manager What is Your Scope Your reporting chain will likely affect your scope: the domain, team, or teams that you pay close attention to and have some responsibility for Pros and Cons of scopes scope too narrow -\u003e lack of impoact, overshadowing other engineers scope too broad -\u003e lack of impact, becoming a bottleneck, decision fatigue What’s Your Primary Focus? What’s important? our work needs to be important” doesn’t mean you should only work on the fanciest, most glamorous technologies and VP-sponsored initiatives. The work that’s most important will often be the work that nobody else see Be wary of choosing a project that already has a lot of senior people on it Chapter 2: Three Maps Map Where we came from Where we started working together Where are are Where we’re going Recap Practice the skills of intentionally looking for a bigger picture and seeing what’s happening. Understand your work in context: know your customers, talk with peers outside your group, understand your success metrics, and be clear on what’s actually important. Know how your organization works and how decisions get made within it. Build or discover paths to allow information you need to come to you. Be clear about what goals everyone is aiming for. Think about your own work and what your journey is. Chapter 3: Creating the Big Picture This chapter addresses when the goal is not clear OR if plan is disputed? A Common Reincarnation of the Same Problem All teams share the same platform (ie Salesforce Hyperforce) Con Hyperforce deployment can become a copy and paste exercise; Code repo is a single source of error; so needs more verification/validationshuman approvals. can suffer from generalization penalty, Decentralized, every org use their own proprietary (ie Ebay core Search) cons: There can be a “tragedy of the commons”, where teams pursuing their own best action without coordinating with others leads to an outcome that is bad for everyone. ==There’s that local maximum again==. Shared concerns can be neglected because no one group has the authority or an incentive to fix them alone. Teams may be missing enough context to make the best decision. The people taking action might be different from the people experiencing the outcomes, or might be separated in time from them. Technical vision vs Strategy Technical vision describes the future one strives once the objectives have been achieved and biggest problem solved Examples of better future states: better state for your architecture, code, processes, technology, and teams With so many staff and architecits, it is crucial to write down and be detailed about Strategy is plan of action. It is the method and metrics in V2MOM. Strategies needs to be realistic and acknowledge the constraints of your situation. Aspects of a strategy (from Good Strategy/Bad Strategy, Rumelt) the diagnosis “What’s going on here?” The diagnosis of your situation needs to be simpler than the messy reality, perhaps by finding patterns in the noise or using metaphors or mental models to make the problem easy to understand. You’re trying to distill the situation you’re in down to its most essential characteristics so that it’s possible to really comprehend it. This is difficult. guiding policy / design philosopy The guiding policy is your approach to bypassing the obstacles described in the diagnosis. It should give you a clear direction and make the decisions that follow easier. Rumelt says it should be short and clear, “a signpost, marking the direction forward.” coherent actions you are going to take Prep work Embrace the boring/existing ideas I kind of think writing about engineering strategy is hard because good strategy is pretty boring, and it’s kind of boring to write about. Also I think when people hear “strategy” they think “innovation.” If you write something interesting, it’s probably wrong. This seems directionally correct, certainly most strategy is synthesis and that point isn’t often made! Join an Expedition in Progress Get a Sponsor Maximize your chances by bringing the potential sponsor something they want' Find out what’s blocking their goals, and see if the problem you’re trying to solve lines up with those. The sponsor will also have some “objectives that are always true”: if you can make their teams more productive or (genuinely) happier Choose Your Core Group Keep each accountable Working in a group also lets you pool your contacts, social capital, and credibility, so you can reach a broader swathe of the organization Who do you need on your side? Who’s going to be opposed? If there are dissenters who will hate any decision made without them, you have two options: make very sure that you have enough support to counter their naysaying, or bring them along from the start. Be clear from the start about whether you consider yourself the lead and ultimate decider of this project or more of a first among equals If you’re later going to want to use tiebreaker powers or pull rank, highlight your role as lead from the start, perhaps by adding a “roles” section to whatever kickoff documentation you’re creating. But whether you’re the lead or not, let your group have ideas, drive the project forward, and talk about it without redirecting all questions to you Choose Your Secondary Group As for your broader group of allies, keep them engaged: interview them and use the information they give you. Send them updates on how the document is going. Invite them to comment on early drafts. Your path will be much easier if you have a group of influential people who support what you’re doing, and the knowledge they bring from across the organization will yield better results, too Make sure it’s achievable Writing Cycle graph TD; 1_Initial_Ideas --\u003e 2_Write; 4_Think_Process_Synthesize_Decide --\u003e 2_Write; 2_Write --\u003e 3_TalkToPeople_Interview_Align; 2_Write --\u003e 5_Share_Broadly; 3_TalkToPeople_Interview_Align --\u003e 4_Think_Process_Synthesize_Decide; Initial Ideas What documents already exist? What needs to change? What’s difficult now? Knowing what your company does and where the team should invest. What’s Important? What will Future You wish Present You had done? Writing Interviews Early questions: We’re creating a plan for X. What’s important to include? What’s it like working with…?” If you could wave a magic wand, what would be different about…? Arriving to Decisions Tradeoffs decisions are hard because no matter what you choose, there will be disadvantages. Priorities help decide which disadvantage you’re willing to accept, and vice versa. use even or statement We will optmize X even over Y Building Consensus ==determining== consensus and ==coming to== consensus are different things than ==having== consensus IETF The key is to separate those choices that are simply unappealing from those that are truly problematic Application: Rather than asking, “Is everyone OK with choice A?” ask, “Can anyone not live with choice A?” Sometimes it is more important moving forward than one person’s arguent carrying the day. Getting and Staying Aligned Understand who your final audience is? local team;, product maanger Keep sponser up to date and what you’re planning and how it’s going Take the time to get your thoughts together so that you can bring them a summary of how you’re approaching various problems and what your options ar Be aspirational but not impossible Nemawashi : means sharing information and layting the foudnations so that by the time a decision is made, there’s already a consensus of opinion. Alignment goes both directions Make sure the story is comprehensible, relatable, and comfortable. II. EXECUTION Chapter 4: Finite Time You can’t do it all. As a senior person, you can see a host of problems: architectures that won’t scale, processes that waste everyone’s time, missed opportunities If you commit to something, what are you implicitly saying no to? Your 5 resources Energy Credibility You can build credibility by solving hard problems, being visibly competent, and consistently showing good technical judgment. One cause of loss of credibility is absolutism. Ex: always advocating one technology in all situations. If you’re polite (even to annoying people), communicate clearly, and stay calm in stressful situations, other people will trust you to be the adult in the room. At staff+ levels, you’re often trying to find a balance between seeing the big picture and accepting pragmatic local solutions. Quality of Life money can improve your quality of life by giving you control and acheive other priorities. Skills Our industry moves fast. Any technical skill set slowly becomes less relevant and eventually gets out of date. Three ways to improve skills Structured: class, buy a book, hack on a project con: eats into your free time Working with someone who is really skilled (most common) Learn by doing; ie on a project, sprint tickets Social Capital Vs Credibility While credibility is whether others think you’re capable of doing whatever you’re trying to do, social capital reflects whether they want to help you do it Social capital is a mix of trust, friendship, and that feeling of owing someone a favor, or of believing they’ll remember that they owe you one. Some ways to build capital spend time with people, have good conversations, help them out, support each other Completing projects; \"nothing breeds succeeds likes success\" ==When you take on a project, you need to be aware of how the project add/subtract to the 5 resources: (skills, quality of life, social capitol, credibility, energy)==. If you’re leading the project, you’ll be filling in the gap. (Remember Ebay) By the time you reach the staff+ level, you will be largely (but probably not entirely) responsible for choosing your own work. How do you choose? Look at the 5 resources. Examples and nuances are in book Is this fight worth wit Sponsorhip; goes both ways as a sponsor and and sponsered: you need to deliver to get more of it. Recap By the time you reach the staff+ level, you will be largely (but probably not entirely) responsible for choosing your own work. This includes deciding on the extent and duration of your involvement with any given project. You can’t do everything, so you’ll need to choose your battles. You are responsible for choosing work that aligns with your life and career needs as well as advancing your company’s goals. You are responsible for managing your energy. Some projects will make you happier than others, or improve your quality of life more. Your social capital and peer credibility are “bank accounts” that you can pay into and spend from. You can lend credibility and social capital to other people, but be conscious of whom you’re lending to. Skills come from taking on projects and from deliberately learning. Make sure you’re building the skills you want to have. Free up your resources by giving other people opportunities to grow, including starting projects and handing them off. Focus means sometimes saying no. Learn how. Chapter 5: Leading Big Projects 5.1 Life of a Project 5.2 At the Start of a Project Building Context Goals: why is this the one that is happening and not others Customer needs: Success metrics: describe sucess is measured? Constraints: deadlines, resources, other responsibilities, other team’s dependencies Risks: Try to predict some of the risks What could happen that could prevent you from reaching your goals on deadline? Are there unknowns, dependencies, key people who will doom the project if they quit? You can mitigate risk by being clear about your areas of uncertainty. What don’t you know, and what approaches can you take that will make those less ambiguous? Is this the sort of thing you can prototype? Has someone done this kind of project before History: where did the idea come from? Have we tried it before? If I am new to existing project, respect what came before findout what system you’ve can use understand people’s feelings and expectations, and learn from their experiences Project Structure Defining Roles on your Team Via Responsibility Assignment Matrix (RACI) Responsible: The person actually doing the work. Accountable: The person ultimately delivering the work and responsible for signing off that it’s done. There’s supposed to be only one accountable person per task, and it will often be the same person as whoever is “Responsible.” Consulted: People who are asked for their opinion. Informed: People who will be kept up to date on progress. Recruiting people/leads When you’re responsible for a really big project, you’re kind of building an organization and you’re steering an organization. It’s important to get the needs of the organization right. I spent a lot of time trying to make sure that I had the best leads in all the different domains that were involved in that one specific project. And, when you’re looking for leads, you’re looking for people that ==don’t just have good technical judgment==, but also ==have the attitudes: they are optimistic, good at conflict resolution, good at communication== Estimating Time Pragmatic Programmer: We find that often the only way to determine the timetable for a project is by gaining experience on that same project Take into consideration the team you depend on; how is their plate? their history? Logistics of Team When, where, and how you’ll meet Channels of informal communication How you share status Where the documentation home will be What is the development practice like? 5.3 Driving the Project Exploring When project just start off, you won’t have enough information about it on day one to make granular decisions (ie use technologyX to build featureY) What are some important aspects of the project? Bigger projects with multiple teams, where each team may have a different mental model. Get to the point where you can ==concisely== explain what different teams in the project want in a way that they’ll agree is accurate. Alighning and framing takes time and effort. talk to users and stakeholders. listen to what they say and how they say it. ==Develop a crisp definition of what you’re doing.== Understanding the problem well helps you frame it for other people What possible approaches can you take? Be open to existing solutions, even if less convenient or interesting Clarifying As lead, it is your responsibility to create a mental model to what you are doing How can you hook this project into their current knowledge Mental models: Connecting an abstract idea back to something I understand well removes some of the cognitive cost of retaining and describing the idea Describe interfaces without the messy details Give people a head start by providing a convenient, memorable name for the concept, using an analogy, or connecting it back to something they already understand Naming Two people can use the same words and mean quite different things Picutre and graphs reduce complexity with pictures Designing Design doc s the most efficient way to make sure everyone is aligned. ven if you’re not hearing objections when you talk about them, your colleagues may not have internalized the plan and their implicit agreement may not mean anything Be precise It is better to be wrong than be vague Tips: Use active voice; Be clear ==who/what== is doing the action before: the jason payload will be unpacked better: the parse component will upack the JSON payload Use a few extra words or even repeat yourself if it means avoiding ambiguity Ex: add a word after ambiguous words like this or that To solve this ==shortage==, we should order more. Writing References [[Tool - Technical Writing]] Security/Privacy/Compliance What data are being exposed? Common Design Pitalls Building from scratch. Most problems are not brand new; learn from other people and consider reuse existing problems Lack of understanding can make a hard task appear trivial Building for the present without consideration for the future Building for the distant distant future Ignoring the difficult part Being non-realistic of the workload Adopting an approach/system that is not really designed for it. Create something that is hard to maintain and understand (ie mc-kernel) bikeshedding; spening the most resource on the most trivial, easiest reversible decisions. Coding (as a staff engineer) Staff engineer sets the example Meet or exceed the testing standards Add useful comments and documentation Should you code? if it excites you and you are not the bottleneck as a cartographer; to gain deeper knowledge and develop better road maps and action items. To develop a pattern, so other people can implement Be careful not to take the most important and biggest work; you could be dragged into other issues and you want to give people room to grow Communicating Communicating well is key for delivering a project in time. If the teams aren’t talking to each other, you won’t make progress. And if you’re not talking to people outside the project, your stakeholders won’t know how it’s going Talking to each other (especially for remote) Talking to each other builds familarity, which helps people safer to ask clarifying questions. To help people talk friendly slack channels demo social events Sharing status to stakeholders, sponsers, and customers Explain status in terms of impact On information the audience actually want to know Navigating (when something goes wrong) Tech leads are responsible for rerouting, escalating to someone who can help, or, if you really have to, breaking the news to your stakeholders that the goal is now unreachable The bigger the change, the more likely it is that you’ll need to go back to the start of this chapter, deal with the overwhelm all over again, build context, and treat the project like it’s restarting. Don’t create panic or let rumors start. Instead, give people facts and be clear about what you want them to do with the information. Chapter 6: Why Have We Stopped Recap As the project lead, you are responsible for understanding why your project has stopped and getting it started again. As a leader in your organization, you can help restart other people’s projects too. You can unblock projects by explaining what needs to happen reducing what other people need to do clarifying the organizational support, escalating making alternative plans. You can bring clarity to a project that’s adrift by defining your destination, agreeing on roles, and asking for help when you need it. Don’t declare victory unless the project is truly complete. Code completeness is just one milestone. Whether you’re ready or not, sometimes it’s time for the project to end. Celebrate, retrospect, and clean up. III. LEVELING UP I believe title refers to leveling the team up Note: to level up team, you also need to level yourself up because the staff engineer sets the standard Chapter 7: You are a Role Model 7.1 Be Competent (Technically) Know Things Your big-picture thinking, project execution, credibility, and influence are underpinned by knowledge and experience Build Experience Technical skill comes from study and practice Experience comes through time, exposure (projects), and study Be aware of too early of promotion to staff, as it may rob you of hands on building experience Being competent and knowledgeable doesn’t mean you have to know the most about every topic. Can you learn the technology, domain etc.. Build domain knowledge Leverage your experience/technical foundation to come up to speed in the domain Learn the technology in whatever way is most effective for you. Get a sense of the appropriate trade-offs; the resource constraints; the common arguments, biases, and in-jokes. Know which technologies are on the market and how they might be used. Read the core literature. Know who the “famous people” in the domain are and what they advocate for. Twitter is good for this Stay up to date It’s embarrassing for everyone when a technical leader insists on a “best practice” that has been debunked for a decade or a technology that everyone else has moved on from Even if you aren’t deep in the code any more, your spidey-sense should stay sharp for “code smells” or problems waiting to happen be wary of drifting so far from the technology that you’re only learning how your company operates Be Self Aware Admit what you know: be ==confident== and ==honest== with yourself with what you know and what you don’t Admit what you don’t know be ok with asking, explain it like I’m 5 year old Understand your own context You have a perspective, that your context is not the universal context, and that your opinions and knowledge are specific to you. Have High Standards Your standard will serve as a model for how other people work Seek out constructive criticism ask for code reviews, design review, and peer evaluation Own your mistake If you react well and fix the problem you caused, you could even end up with more esteem from your colleague Be reilable Finish what you start 7.2 Be Responsible Take Ownership Senior people own the whole problem, not just the parts that go as planned. You’re not running someone else’s project for them: it’s yours and you don’t passively let it sink or swim Avoid “Cover Your Ass” Mature engineers stand up and accept the responsibility given to them. If they find they don’t have the requisite authority to be held accountable for their work, they seek out ways to rectify that. An example of CYAE is “It’s not my fault. They broke it, they used it wrong. I built it to spec, I can’t be held responsible for their mistakes or improper specification.” Use you own judgement; don’t need to constantly ask for permission Ask obvious questions; as senior, you can ask questions that are so obvious, no one else is willing to ask Do the glue work; tasks that are needed to make team successful a lot of the work that’s not anybody’s job but that furthers your goals. Redirect your junior colleagues toward tasks that will develop their careers instead. Take Charge When you se a gap, step in to fill the gap Step up in an emergency Drive meetings with a clear agenda When something disrespectful or offensive in a public channel, one should speak up Create Calm defuse, don’t amplify when you’re dealing with a big problem, try to make it smaller be cautious where/how you share your anxieties/frustration. acknowledge there are problems be clear about the intention of the sharing want someone to take action bounce ideas sharing context Be consistent and predictable since it’s hard to be consistent when stresed, make sure to take time off, get enough rest. 7.3 Remember the Goal There’s the Business adapt to the situation phases: grow, acquisition, new markets has a budget so spend mindfully There’s the User Know who uses your software and how they use it. Make sure the feature you are creating is actually needed There’s the Team You are working as a team; don’t be single point of failure be aware of the capability of your team 7.4 Look Ahead Anticipate what the Future You wish Present You had done Be clear what the your broad direction is, even if you don’t know the details Take time to optimize your workflow codebase style guide documentation Keep your tools sharp make your environment better measured by velocity and reliability Create institutional memory Expect Failure Plan in advance for major incidents by adding some conventions around how you work together during an emergency: introduce the incident command system I mentioned earlier, for example, and practice the response before you need it Optimize for Maintenance, Not Creation Make it understandable classes and hands on experiences make it as easy as possible for people to understand the system when they need to documentation(s) short intro a big deep one Make system observable easy to inspect, analyze, and debug Make system simple Good programmers write code that humans can understand.”12 Senior engineers sometimes think they can demonstrate their prowess with the flashiest, most complicated solutions. Make things simple by spending more time on it; working solution is just the first iteration 7.5 Recap Your words and actions carry more weight now. Be deliberate. Invest the time to build knowledge and expertise. Competence comes from experience. Be self-aware about what you know and what you don’t. Strive to be consistent, reliable, and trustworthy. Get comfortable taking charge when nobody else is, including during a crisis or an ambiguous project. When someone needs to say something, say something. Create calm. Make problems smaller, not bigger. Be aware of your business, budgets, user needs, and the capabilities of your Chapter 8: Good Influence At Scale 8.1 Good Influence 8.2 Advice 8.3 Teaching 8.4 Guardrails 8.5 Opportunity 8.6 Recap You can help your colleagues by providing advice, teaching, guardrails, or opportunities. Understand what’s most helpful for the specific situation. Think about whether you want to help one-on-one, level up your team, or influence further. Offer your experience and advice, but make sure it’s welcome. Writing and public speaking can send your message further. Teach through pairing, shadowing, review, and coaching. Teaching classes or writing codelabs can scale your teaching time. Guardrails can let people work autonomously. Offer review, or be a project guardrail for your colleagues. Codify guardrails using processes, automation, and culture change. Opportunities can be much more valuable than advice. Think about who you’re sponsoring and delegating to. Share the spotlight in your team. Plan to give away your job.","title":"Staff Engineer’s Path"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"Good software is all about making tradeoffs.  This book helps you make better choices and reviews common software mistakes.","keywords":null,"link":"/books/software-profession/software-mistakes-and-tradeoffs/","tags":null,"text":"I. THE BIG PICTURE Chapter 1: What Would You Say You Do Here? Staff : 1st line manager, Senior Staff : Senior Manager; Principle : Director Do we even need a staff engineer position? a job title vests authority in people who might not automatically receive saves them the time and energy they would otherwise have to spend proving themselves again and again Why do we staff need to see big picture? To find a ==global maximum==, which is the good for the org and not 1 team over time Good decisions need context. Experienced engineers know that the answer to most technology choices is “it depends.” Knowing the pros and cons of a particular technology isn’t enough—you need to know the local details too. What are you trying to do? How much ==time, money, and patience== do you have? What’s your ==risk tolerance==? What does the business need? What’s the staff engineer’s role across projects? Use their experience to ==anticipate risks and ask hard questions== When the project gets stuck, they have enough perspective to ==track down the causes and unblock== Evengalize: telling the story of what’s happening and why, selling the vision to the rest of the company, and explaining what the work will make possible and how the new project affects everyone Why the staff engineer needs to be good at influencing Within engineer circle, Staff engineers are ==role models==. Managers may be responsible for setting culture on their teams, enforcing good behavior, and ensuring standards are met. But engineering norms are set by the behavior of the most respected engineers on the project. What is your Role? Staff can report to VP, Directors, Senior Manager, Manager Which level one is on influences what problem you are asked of what information you are privy to how much time you may get from your manager What is Your Scope Your reporting chain will likely affect your scope: the domain, team, or teams that you pay close attention to and have some responsibility for Pros and Cons of scopes scope too narrow -\u003e lack of impoact, overshadowing other engineers scope too broad -\u003e lack of impact, becoming a bottleneck, decision fatigue What’s Your Primary Focus? What’s important? our work needs to be important” doesn’t mean you should only work on the fanciest, most glamorous technologies and VP-sponsored initiatives. The work that’s most important will often be the work that nobody else see Be wary of choosing a project that already has a lot of senior people on it Chapter 2: Three Maps Map Where we came from Where we started working together Where are are Where we’re going Recap Practice the skills of intentionally looking for a bigger picture and seeing what’s happening. Understand your work in context: know your customers, talk with peers outside your group, understand your success metrics, and be clear on what’s actually important. Know how your organization works and how decisions get made within it. Build or discover paths to allow information you need to come to you. Be clear about what goals everyone is aiming for. Think about your own work and what your journey is. Chapter 3: Creating the Big Picture This chapter addresses when the goal is not clear OR if plan is disputed? A Common Reincarnation of the Same Problem All teams share the same platform (ie Salesforce Hyperforce) Con Hyperforce deployment can become a copy and paste exercise; Code repo is a single source of error; so needs more verification/validationshuman approvals. can suffer from generalization penalty, Decentralized, every org use their own proprietary (ie Ebay core Search) cons: There can be a “tragedy of the commons”, where teams pursuing their own best action without coordinating with others leads to an outcome that is bad for everyone. ==There’s that local maximum again==. Shared concerns can be neglected because no one group has the authority or an incentive to fix them alone. Teams may be missing enough context to make the best decision. The people taking action might be different from the people experiencing the outcomes, or might be separated in time from them. Technical vision vs Strategy Technical vision describes the future one strives once the objectives have been achieved and biggest problem solved Examples of better future states: better state for your architecture, code, processes, technology, and teams With so many staff and architecits, it is crucial to write down and be detailed about Strategy is plan of action. It is the method and metrics in V2MOM. Strategies needs to be realistic and acknowledge the constraints of your situation. Aspects of a strategy (from Good Strategy/Bad Strategy, Rumelt) the diagnosis “What’s going on here?” The diagnosis of your situation needs to be simpler than the messy reality, perhaps by finding patterns in the noise or using metaphors or mental models to make the problem easy to understand. You’re trying to distill the situation you’re in down to its most essential characteristics so that it’s possible to really comprehend it. This is difficult. guiding policy / design philosopy The guiding policy is your approach to bypassing the obstacles described in the diagnosis. It should give you a clear direction and make the decisions that follow easier. Rumelt says it should be short and clear, “a signpost, marking the direction forward.” coherent actions you are going to take Prep work Embrace the boring/existing ideas I kind of think writing about engineering strategy is hard because good strategy is pretty boring, and it’s kind of boring to write about. Also I think when people hear “strategy” they think “innovation.” If you write something interesting, it’s probably wrong. This seems directionally correct, certainly most strategy is synthesis and that point isn’t often made! Join an Expedition in Progress Get a Sponsor Maximize your chances by bringing the potential sponsor something they want' Find out what’s blocking their goals, and see if the problem you’re trying to solve lines up with those. The sponsor will also have some “objectives that are always true”: if you can make their teams more productive or (genuinely) happier Choose Your Core Group Keep each accountable Working in a group also lets you pool your contacts, social capital, and credibility, so you can reach a broader swathe of the organization Who do you need on your side? Who’s going to be opposed? If there are dissenters who will hate any decision made without them, you have two options: make very sure that you have enough support to counter their naysaying, or bring them along from the start. Be clear from the start about whether you consider yourself the lead and ultimate decider of this project or more of a first among equals If you’re later going to want to use tiebreaker powers or pull rank, highlight your role as lead from the start, perhaps by adding a “roles” section to whatever kickoff documentation you’re creating. But whether you’re the lead or not, let your group have ideas, drive the project forward, and talk about it without redirecting all questions to you Choose Your Secondary Group As for your broader group of allies, keep them engaged: interview them and use the information they give you. Send them updates on how the document is going. Invite them to comment on early drafts. Your path will be much easier if you have a group of influential people who support what you’re doing, and the knowledge they bring from across the organization will yield better results, too Make sure it’s achievable Writing Cycle graph TD; 1_Initial_Ideas --\u003e 2_Write; 4_Think_Process_Synthesize_Decide --\u003e 2_Write; 2_Write --\u003e 3_TalkToPeople_Interview_Align; 2_Write --\u003e 5_Share_Broadly; 3_TalkToPeople_Interview_Align --\u003e 4_Think_Process_Synthesize_Decide; Initial Ideas What documents already exist? What needs to change? What’s difficult now? Knowing what your company does and where the team should invest. What’s Important? What will Future You wish Present You had done? Writing Interviews Early questions: We’re creating a plan for X. What’s important to include? What’s it like working with…?” If you could wave a magic wand, what would be different about…? Arriving to Decisions Tradeoffs decisions are hard because no matter what you choose, there will be disadvantages. Priorities help decide which disadvantage you’re willing to accept, and vice versa. use even or statement We will optmize X even over Y Building Consensus ==determining== consensus and ==coming to== consensus are different things than ==having== consensus IETF The key is to separate those choices that are simply unappealing from those that are truly problematic Application: Rather than asking, “Is everyone OK with choice A?” ask, “Can anyone not live with choice A?” Sometimes it is more important moving forward than one person’s arguent carrying the day. Getting and Staying Aligned Understand who your final audience is? local team;, product maanger Keep sponser up to date and what you’re planning and how it’s going Take the time to get your thoughts together so that you can bring them a summary of how you’re approaching various problems and what your options ar Be aspirational but not impossible Nemawashi : means sharing information and layting the foudnations so that by the time a decision is made, there’s already a consensus of opinion. Alignment goes both directions Make sure the story is comprehensible, relatable, and comfortable. II. EXECUTION Chapter 4: Finite Time You can’t do it all. As a senior person, you can see a host of problems: architectures that won’t scale, processes that waste everyone’s time, missed opportunities If you commit to something, what are you implicitly saying no to? Your 5 resources Energy Credibility You can build credibility by solving hard problems, being visibly competent, and consistently showing good technical judgment. One cause of loss of credibility is absolutism. Ex: always advocating one technology in all situations. If you’re polite (even to annoying people), communicate clearly, and stay calm in stressful situations, other people will trust you to be the adult in the room. At staff+ levels, you’re often trying to find a balance between seeing the big picture and accepting pragmatic local solutions. Quality of Life money can improve your quality of life by giving you control and acheive other priorities. Skills Our industry moves fast. Any technical skill set slowly becomes less relevant and eventually gets out of date. Three ways to improve skills Structured: class, buy a book, hack on a project con: eats into your free time Working with someone who is really skilled (most common) Learn by doing; ie on a project, sprint tickets Social Capital Vs Credibility While credibility is whether others think you’re capable of doing whatever you’re trying to do, social capital reflects whether they want to help you do it Social capital is a mix of trust, friendship, and that feeling of owing someone a favor, or of believing they’ll remember that they owe you one. Some ways to build capital spend time with people, have good conversations, help them out, support each other Completing projects; \"nothing breeds succeeds likes success\" ==When you take on a project, you need to be aware of how the project add/subtract to the 5 resources: (skills, quality of life, social capitol, credibility, energy)==. If you’re leading the project, you’ll be filling in the gap. (Remember Ebay) By the time you reach the staff+ level, you will be largely (but probably not entirely) responsible for choosing your own work. How do you choose? Look at the 5 resources. Examples and nuances are in book Is this fight worth wit Sponsorhip; goes both ways as a sponsor and and sponsered: you need to deliver to get more of it. Recap By the time you reach the staff+ level, you will be largely (but probably not entirely) responsible for choosing your own work. This includes deciding on the extent and duration of your involvement with any given project. You can’t do everything, so you’ll need to choose your battles. You are responsible for choosing work that aligns with your life and career needs as well as advancing your company’s goals. You are responsible for managing your energy. Some projects will make you happier than others, or improve your quality of life more. Your social capital and peer credibility are “bank accounts” that you can pay into and spend from. You can lend credibility and social capital to other people, but be conscious of whom you’re lending to. Skills come from taking on projects and from deliberately learning. Make sure you’re building the skills you want to have. Free up your resources by giving other people opportunities to grow, including starting projects and handing them off. Focus means sometimes saying no. Learn how. Chapter 5: Leading Big Projects 5.1 Life of a Project 5.2 At the Start of a Project Building Context Goals: why is this the one that is happening and not others Customer needs: Success metrics: describe sucess is measured? Constraints: deadlines, resources, other responsibilities, other team’s dependencies Risks: Try to predict some of the risks What could happen that could prevent you from reaching your goals on deadline? Are there unknowns, dependencies, key people who will doom the project if they quit? You can mitigate risk by being clear about your areas of uncertainty. What don’t you know, and what approaches can you take that will make those less ambiguous? Is this the sort of thing you can prototype? Has someone done this kind of project before History: where did the idea come from? Have we tried it before? If I am new to existing project, respect what came before findout what system you’ve can use understand people’s feelings and expectations, and learn from their experiences Project Structure Defining Roles on your Team Via Responsibility Assignment Matrix (RACI) Responsible: The person actually doing the work. Accountable: The person ultimately delivering the work and responsible for signing off that it’s done. There’s supposed to be only one accountable person per task, and it will often be the same person as whoever is “Responsible.” Consulted: People who are asked for their opinion. Informed: People who will be kept up to date on progress. Recruiting people/leads When you’re responsible for a really big project, you’re kind of building an organization and you’re steering an organization. It’s important to get the needs of the organization right. I spent a lot of time trying to make sure that I had the best leads in all the different domains that were involved in that one specific project. And, when you’re looking for leads, you’re looking for people that ==don’t just have good technical judgment==, but also ==have the attitudes: they are optimistic, good at conflict resolution, good at communication== Estimating Time Pragmatic Programmer: We find that often the only way to determine the timetable for a project is by gaining experience on that same project Take into consideration the team you depend on; how is their plate? their history? Logistics of Team When, where, and how you’ll meet Channels of informal communication How you share status Where the documentation home will be What is the development practice like? 5.3 Driving the Project Exploring When project just start off, you won’t have enough information about it on day one to make granular decisions (ie use technologyX to build featureY) What are some important aspects of the project? Bigger projects with multiple teams, where each team may have a different mental model. Get to the point where you can ==concisely== explain what different teams in the project want in a way that they’ll agree is accurate. Alighning and framing takes time and effort. talk to users and stakeholders. listen to what they say and how they say it. ==Develop a crisp definition of what you’re doing.== Understanding the problem well helps you frame it for other people What possible approaches can you take? Be open to existing solutions, even if less convenient or interesting Clarifying As lead, it is your responsibility to create a mental model to what you are doing How can you hook this project into their current knowledge Mental models: Connecting an abstract idea back to something I understand well removes some of the cognitive cost of retaining and describing the idea Describe interfaces without the messy details Give people a head start by providing a convenient, memorable name for the concept, using an analogy, or connecting it back to something they already understand Naming Two people can use the same words and mean quite different things Picutre and graphs reduce complexity with pictures Designing Design doc s the most efficient way to make sure everyone is aligned. ven if you’re not hearing objections when you talk about them, your colleagues may not have internalized the plan and their implicit agreement may not mean anything Be precise It is better to be wrong than be vague Tips: Use active voice; Be clear ==who/what== is doing the action before: the jason payload will be unpacked better: the parse component will upack the JSON payload Use a few extra words or even repeat yourself if it means avoiding ambiguity Ex: add a word after ambiguous words like this or that To solve this ==shortage==, we should order more. Writing References [[Tool - Technical Writing]] Security/Privacy/Compliance What data are being exposed? Common Design Pitalls Building from scratch. Most problems are not brand new; learn from other people and consider reuse existing problems Lack of understanding can make a hard task appear trivial Building for the present without consideration for the future Building for the distant distant future Ignoring the difficult part Being non-realistic of the workload Adopting an approach/system that is not really designed for it. Create something that is hard to maintain and understand (ie mc-kernel) bikeshedding; spening the most resource on the most trivial, easiest reversible decisions. Coding (as a staff engineer) Staff engineer sets the example Meet or exceed the testing standards Add useful comments and documentation Should you code? if it excites you and you are not the bottleneck as a cartographer; to gain deeper knowledge and develop better road maps and action items. To develop a pattern, so other people can implement Be careful not to take the most important and biggest work; you could be dragged into other issues and you want to give people room to grow Communicating Communicating well is key for delivering a project in time. If the teams aren’t talking to each other, you won’t make progress. And if you’re not talking to people outside the project, your stakeholders won’t know how it’s going Talking to each other (especially for remote) Talking to each other builds familarity, which helps people safer to ask clarifying questions. To help people talk friendly slack channels demo social events Sharing status to stakeholders, sponsers, and customers Explain status in terms of impact On information the audience actually want to know Navigating (when something goes wrong) Tech leads are responsible for rerouting, escalating to someone who can help, or, if you really have to, breaking the news to your stakeholders that the goal is now unreachable The bigger the change, the more likely it is that you’ll need to go back to the start of this chapter, deal with the overwhelm all over again, build context, and treat the project like it’s restarting. Don’t create panic or let rumors start. Instead, give people facts and be clear about what you want them to do with the information. Chapter 6: Why Have We Stopped Recap As the project lead, you are responsible for understanding why your project has stopped and getting it started again. As a leader in your organization, you can help restart other people’s projects too. You can unblock projects by explaining what needs to happen reducing what other people need to do clarifying the organizational support, escalating making alternative plans. You can bring clarity to a project that’s adrift by defining your destination, agreeing on roles, and asking for help when you need it. Don’t declare victory unless the project is truly complete. Code completeness is just one milestone. Whether you’re ready or not, sometimes it’s time for the project to end. Celebrate, retrospect, and clean up. III. LEVELING UP I believe title refers to leveling the team up Note: to level up team, you also need to level yourself up because the staff engineer sets the standard Chapter 7: You are a Role Model 7.1 Be Competent (Technically) Know Things Your big-picture thinking, project execution, credibility, and influence are underpinned by knowledge and experience Build Experience Technical skill comes from study and practice Experience comes through time, exposure (projects), and study Be aware of too early of promotion to staff, as it may rob you of hands on building experience Being competent and knowledgeable doesn’t mean you have to know the most about every topic. Can you learn the technology, domain etc.. Build domain knowledge Leverage your experience/technical foundation to come up to speed in the domain Learn the technology in whatever way is most effective for you. Get a sense of the appropriate trade-offs; the resource constraints; the common arguments, biases, and in-jokes. Know which technologies are on the market and how they might be used. Read the core literature. Know who the “famous people” in the domain are and what they advocate for. Twitter is good for this Stay up to date It’s embarrassing for everyone when a technical leader insists on a “best practice” that has been debunked for a decade or a technology that everyone else has moved on from Even if you aren’t deep in the code any more, your spidey-sense should stay sharp for “code smells” or problems waiting to happen be wary of drifting so far from the technology that you’re only learning how your company operates Be Self Aware Admit what you know: be ==confident== and ==honest== with yourself with what you know and what you don’t Admit what you don’t know be ok with asking, explain it like I’m 5 year old Understand your own context You have a perspective, that your context is not the universal context, and that your opinions and knowledge are specific to you. Have High Standards Your standard will serve as a model for how other people work Seek out constructive criticism ask for code reviews, design review, and peer evaluation Own your mistake If you react well and fix the problem you caused, you could even end up with more esteem from your colleague Be reilable Finish what you start 7.2 Be Responsible Take Ownership Senior people own the whole problem, not just the parts that go as planned. You’re not running someone else’s project for them: it’s yours and you don’t passively let it sink or swim Avoid “Cover Your Ass” Mature engineers stand up and accept the responsibility given to them. If they find they don’t have the requisite authority to be held accountable for their work, they seek out ways to rectify that. An example of CYAE is “It’s not my fault. They broke it, they used it wrong. I built it to spec, I can’t be held responsible for their mistakes or improper specification.” Use you own judgement; don’t need to constantly ask for permission Ask obvious questions; as senior, you can ask questions that are so obvious, no one else is willing to ask Do the glue work; tasks that are needed to make team successful a lot of the work that’s not anybody’s job but that furthers your goals. Redirect your junior colleagues toward tasks that will develop their careers instead. Take Charge When you se a gap, step in to fill the gap Step up in an emergency Drive meetings with a clear agenda When something disrespectful or offensive in a public channel, one should speak up Create Calm defuse, don’t amplify when you’re dealing with a big problem, try to make it smaller be cautious where/how you share your anxieties/frustration. acknowledge there are problems be clear about the intention of the sharing want someone to take action bounce ideas sharing context Be consistent and predictable since it’s hard to be consistent when stresed, make sure to take time off, get enough rest. 7.3 Remember the Goal There’s the Business adapt to the situation phases: grow, acquisition, new markets has a budget so spend mindfully There’s the User Know who uses your software and how they use it. Make sure the feature you are creating is actually needed There’s the Team You are working as a team; don’t be single point of failure be aware of the capability of your team 7.4 Look Ahead Anticipate what the Future You wish Present You had done Be clear what the your broad direction is, even if you don’t know the details Take time to optimize your workflow codebase style guide documentation Keep your tools sharp make your environment better measured by velocity and reliability Create institutional memory Expect Failure Plan in advance for major incidents by adding some conventions around how you work together during an emergency: introduce the incident command system I mentioned earlier, for example, and practice the response before you need it Optimize for Maintenance, Not Creation Make it understandable classes and hands on experiences make it as easy as possible for people to understand the system when they need to documentation(s) short intro a big deep one Make system observable easy to inspect, analyze, and debug Make system simple Good programmers write code that humans can understand.”12 Senior engineers sometimes think they can demonstrate their prowess with the flashiest, most complicated solutions. Make things simple by spending more time on it; working solution is just the first iteration 7.5 Recap Your words and actions carry more weight now. Be deliberate. Invest the time to build knowledge and expertise. Competence comes from experience. Be self-aware about what you know and what you don’t. Strive to be consistent, reliable, and trustworthy. Get comfortable taking charge when nobody else is, including during a crisis or an ambiguous project. When someone needs to say something, say something. Create calm. Make problems smaller, not bigger. Be aware of your business, budgets, user needs, and the capabilities of your Chapter 8: Good Influence At Scale 8.1 Good Influence 8.2 Advice 8.3 Teaching 8.4 Guardrails 8.5 Opportunity 8.6 Recap You can help your colleagues by providing advice, teaching, guardrails, or opportunities. Understand what’s most helpful for the specific situation. Think about whether you want to help one-on-one, level up your team, or influence further. Offer your experience and advice, but make sure it’s welcome. Writing and public speaking can send your message further. Teach through pairing, shadowing, review, and coaching. Teaching classes or writing codelabs can scale your teaching time. Guardrails can let people work autonomously. Offer review, or be a project guardrail for your colleagues. Codify guardrails using processes, automation, and culture change. Opportunities can be much more valuable than advice. Think about who you’re sponsoring and delegating to. Share the spotlight in your team. Plan to give away your job.","title":"Software Mistakes and Tradeoffs"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"A collection of software best practices at Google","keywords":null,"link":"/books/software-profession/software-engineering-at-google/","tags":null,"text":"1 What is Software Engineering 1.1 Time and Change Software engineering is programming integrated over time. new features requests, depedency changes, people changes Engineers need to be more concerned with the passage of time and the eventual need for change Hyrum’s Law With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody. 1.2 Scale and Efficiency we need to be more concerned about scale and efficiency, both for the software we produce as well as for the organization that is producing it Everything your organization relies upon to produce and maintain code should be scalable in terms of overall cost and resource consumption scalable in terms of human effort Policies that do not scale well development branches that requires merge to the main branch but what’s the solution? Policies that scale well ==knowledge sharing== We’ve found that expertise and shared communication forums offer great value as an organization scales. Beyonce rule: if you like it, you should put a CI test on it. 1.3 Tradeoffs and Costs we are asked to make more ==complex decisions== with higher-stakes outcomes, often based on ==imprecise estimates== of ==time== and ==growth==. Different types of costs Financial costs (e.g., money) Resource costs (e.g., CPU time) Personnel costs (e.g., engineering effort) Transaction costs (e.g., what does it cost to take action?) Opportunity costs (e.g., what does it cost to not take action?) Societal costs (e.g., what impact will this choice have on society at large?) Shift left: find errors earlier decreases the cost concept - design - development - testing - commit - integration - production 2 common decision scenaiors All of the quantities involved are measurable or can at least be estimated. This usually means that we’re evaluating trade-offs between CPU and network, or dollars and RAM, or considering whether to spend two weeks of engineer-time in order to save N CPUs across our datacenters. Some of the quantities are subtle, or we don’t know how to measure them. Sometimes this manifests as “We don’t know how much engineer-time this will take.” Sometimes it is even more nebulous: how do you measure the engineering cost of a poorly designed API? Or the societal impact of a product choice? 1.4 TL;DR “Software engineering” differs from “programming” in dimensionality: programming is about producing code. Software engineering extends that to include the maintenance of that code for its useful life span. There is a factor of at least 100,000 times between the life spans of short-lived code and long-lived code. It is silly to assume that the same best practices apply universally on both ends of that spectrum. Software is sustainable when, for the expected life span of the code, we are capable of responding to changes in dependencies, technology, or product requirements. We may choose to not change things, but we need to be capable. Hyrum’s Law: with a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody. Every task your organization has to do repeatedly should be scalable (linear or better) in terms of human input. Policies are a wonderful tool for making process scalable. Process inefficiencies and other software-development tasks tend to scale up slowly. Be careful about boiled-frog problems. Expertise pays off particularly well when combined with economies of scale. “Because I said so” is a terrible reason to do things. Being data driven is a good start, but in reality, most decisions are based on a mix of data, assumption, precedent, and argument. It’s best when objective data makes up the majority of those inputs, but it can rarely be all of them. Being data driven over time implies the need to change directions when the data changes (or when assumptions are dispelled). Mistakes or revised plans are inevitable. I CULTURE 2. Working Well On Temas 2.1 Help Me Hide My Code Some people are insecure in showing code that is in progress because no one likes to be criticized because of the genius myth 2.2 The Genius Myth Genius deconstructed Linus: Linus did was write just the beginnings of a proof-of-concept Unix-like kernel and show it to an email list. That was no small accomplishment, and it was definitely an impressive achievement, but it was just the tip of the iceberg. Linux is hundreds of times bigger than that initial kernel and was developed by thousands of smart people. Linus’ real achievement was to lead these people and coordinate their work; Linux is the shining result not of his original idea, but of the collective labor of the communit Michael Jordan: had Phil Jackson and strong set of teammates 2.3 Hiding Considered Harmful If you spend all of your time working alone, you’re increasing the risk of unnecessary failure and cheating your potential for growth If you keep your great idea hidden from the world and refuse to show anyone anything until the implementation is polished, you’re taking a huge gamble. Early sharing isn’t just about preventing personal missteps and getting your ideas vetted. So, what “hiding” boils down to is this: working alone is inherently riskier than working with others. Even though you might be afraid of someone stealing your idea or thinking you’re not intelligent, you should be much more concerned about wasting huge swaths of your time toiling away on the wrong thing. 2.4 It’s All About the Team Software engineering is a team endeavor 3 Pillars of Social Interaction ==Humility== You are not the center of the universe (nor is your code!). You’re neither omniscient nor infallible. You’re open to self-improvement. ==Respect== You genuinely care about others you work with. You treat them kindly and appreciate their abilities and accomplishments. ==Trust== You believe others are competent and will do the right thing, and you’re OK with letting them drive when appropriate The 3 Pillars in Practice Lose the ego Nobody wants to work with someone who consistently behaves like they’re the most important person in the room. Even if you know you’re the wisest person in the discussion, don’t wave it in people’s face Doesn’t mean you need to be a doormat; there’s nothing wrong with self-confidence. Just don’t come off like a know-it-all. Even better, think about going for a “collective” ego, instead; rather than worrying about whether you’re personally awesome, try to build a sense of team accomplishment and group pride Learn to give AND take criticism Giving criticism Be aware if people may be insecure Strive to make the feedback constructive factual and not personal over-arching statements Practice curiousity and understanding tone Taking criticism You are not the code you write Be humble about your skills and trust that the other person has your best interests at heart Blameless Post Mortem Being Googley Thrives in ambiguity Can deal with conflicting messages or directions, build consensus, and make progress against a problem, even when the environment is constantly shifting. Values feedback Has humility to both receive and give feedback gracefully and understands how valuable feedback is for personal (and team) development. Challenges status quo Is able to set ambitious goals and pursue them even when there might be resistance or inertia from others. Puts the user first Has empathy and respect for users of Google’s products and pursues actions that are in their best interests. Cares about the team Has empathy and respect for coworkers and actively works to help them without being asked, improving team cohesion. Does the right thing Has a strong sense of ethics about everything they do; willing to make difficult or inconvenient decisions to protect the integrity of the team and product. 3 Knowledge Sharing 3.1 Challenges to Learning Lack of psychological safety An environment in which people are afraid to take risks or make mistakes in front of others because they fear being punished for it. This often manifests as a culture of fear or a tendency to avoid transparency. Information islands Knowledge fragmentation that occurs in different parts of an organization that don’t communicate with one another or use shared resources. In such an environment, each group develops its own way of doing things.1 This often leads to the following: Information fragmentation Each island has an incomplete picture of the bigger whole. Information duplication Each island has reinvented its own way of doing something. Information skew Each island has its own ways of doing the same thing, and these might or might not conflict. Single point of failure (SPOF) A bottleneck that occurs when critical information is available from only a single person. This is related to bus factor, which is discussed in more detail in Chapter 2. SPOFs can arise out of good intentions: it can be easy to fall into a habit of “Let me take care of that for you.” But this approach optimizes for short-term efficiency (“It’s faster for me to do it”) at the cost of poor long-term scalability (the team never learns how to do whatever it is that needs to be done). This mindset also tends to lead to all-or-nothing expertise. All-or-nothing expertise A group of people that is split between people who know “everything” and novices, with little middle ground. This problem often reinforces itself if experts always do everything themselves and don’t take the time to develop new experts through mentoring or documentation. In this scenario, knowledge and responsibilities continue to accumulate on those who already have expertise, and new team members or novices are left to fend for themselves and ramp up more slowly. Parroting Mimicry without understanding. This is typically characterized by mindlessly copying patterns or code without understanding their purpose, often under the assumption that said code is needed for unknown reasons. Haunted graveyards Places, often in code, that people avoid touching or changing because they are afraid that something might go wrong. Unlike the aforementioned parroting, haunted graveyards are characterized by people avoiding action because of fear and superstition. 3.2 Setting the Stage: Psychological Safety Software engineering can be defined as the multiperson development of multiversion programs code is an important output but only a small part of building a product code does not emerge spontaneously out of nothing, and neither does expertise. an organization’s success depends on growing and investing in its people. Personalized 1-1 advice doers not scale Documented knowledge scales better, like team wiki but has tradeoffs it might be more generalized and less applicable to individual learners’ situations, and it comes with the added maintenance cost required to keep information relevant and up to date over time You need both documentation and personal 1-1 knowledge Tribal and written knowledge complement each other. Even a perfectly expert team with perfect documentation needs to communicate with one another, coordinate with other teams, and adapt their strategies over time. No single knowledge-sharing approach is the correct solution for all types of learning, and the particulars of a good mix will likely vary based on your organization 3.3 Growing Your Knowledge Ask Questions The more you know, the more you know you don’t know. Openly asking questions4 or expressing gaps in knowledge reinforces that it’s OK for others to do the same Understand the Context Learning is not just about understanding new things; it also includes developing an understanding of the decisions behind the design and implementation of existing things Chesterton’s fence wiki the principle that reforms should not be made until the reasoning behind the existing state of affairs is understood. Google style guide includes rationale for each style guide 3.4 Scaling Your Questions: Ask the Community 3.5 Scaling Your Knowledge: You Always Have Something To Teach Tech Talks and Classes Documentation: update, create, promote documentation 3.6 Scaling Your Organization’s Knowledge Cultivate a Knowledge Sharing Culture Respect Incentives and Recognition encourages engineers to share knowledge by noting these expectations explicitly. part of promotion Ex of senior leadership Growing future leaders by serving as mentors to junior staff, helping them develop both technically and in their Google role Sustaining and developing the software community at Google via code and design reviews, engineering education and development, and expert guidance to others in the field Establish Canonical Source of Information Developer Guides Google has a broad and deep set of official guidance for engineers, including style guides, official software engineering best practices,guides for code review (Chapter 9), and testing (Chap 11, 12, 14) and Tips of the Week ==CodeLabs== some of google’s codelab github Static Analysis (Chap 20 of book) Powerful way to share best practices that can be checked programatically God way to scale team efficiently Staying in the Loop Type of info Knowing how to do a typical development workflow (impt) Update on popular producitivty tools (less impt) Newsletters Communities 3.7 Readability: Standardized Mentorship Through Code Reviews Change list requires readbility approvals 1-2% of google engineers are readability reviewer Readability reviewers are held to the highest standards because they are expected not just to have deep language expertise, but also an aptitude for teaching through code review. Benefits good to cross pollinate standard code across islands Cost but still worth it in my opinion These benefits come with some costs: readability is a heavyweight process compared to other mediums like documentation and classes because it is mandatory and enforced by Google tooling Increased friction for teams that do not have any team members with readability, because they need to find reviewers from outside their team to give readability approval on CLs. Potential for additional rounds of code review for authors who need readability review. Scaling disadvantages of being a human-driven process. Limited to scaling linearly to organization growth because it depends on human reviewers doing specialized code reviews. 4 Engineering for Equity 5 How To Lead a Team 5.1 Managers, TL, and Tech Lead Manager Engineering manager is responsible for the performance, productivity, and happiness of every person on their team The needs of the business and the needs of individual team members don’t always align, this can often place a manager in a difficult position. Tech Lead is responsible for technology decisions and choices, architecture, priorities, velocity, and general project management TLM a single person who can handle both the people and technical needs of their team The job of TLM is a tricky one and often requires the TLM to learn how to balance individual work, delegation, and people management 5.2 Moving from IC to Leadership Role Many ICs ==don’t== want to be a manager not much to point what they have accomplished at the end of day. Quantifying managment work is touch. people rise to the level of their incompetence; so an IC has likely a “crappy” manager A good reason for IC to become a manager is because they want to scale themselves Servant leadership servant leadership,” which is a nice way of saying the most important thing you can do as a leader is to serve your team, much like a butler or majordomo tends to the health and well-being of a household. removing bureaucratic obstacles that a team member can’t remove by themselves, helping a team achieve consensus, or even buying dinner for the team But how is this good for me ? Isn’t this one of the reason why I didn’t want to be a TLM at Ebay? 5.3 The Engineering Manager Dating back to the factories, manager has used the carrot and stick This may not work well; if managers act like parents and treat the employees as chidlren, employees will act like children. Is it better to trust the employees, and the employee fees positive pressure to live up to that trust? Traditional managers worry about HOW to get things done, whereas great managers worry about WHAT things get done (and trust their team to figure out how to do it). 5.4 Anti-patterns HIre pushovers Strive to hire people who are smarter than you and can replace you. This can be difficult because these very same people will challenge you on a regular basis (in addition to letting you know when you make a mistake). These very same people will also consistently impress you and make great things happen. They’ll be able to direct themselves to a much greater extent, and some will be eager to lead the team, as well. Ignore Lower Performers “Hope is not a strategy” Coach the low performer Make the goals small, incremental, and measurable so that there’s an opportunity for lots of small successes Ignore Human Issues Show empathy to collegues who are going through their various challenges. Be Everyone’s Friend Don’t confuse friendship with leading with a soft touch: when you hold power over someone’s career, they might feel pressure to artificially reciprocate gestures of friendship. Remember that you can lead a team and build consensus without being a close friend of your team or a hard ass. Compromise the Hiring Bar Treat Your Children Like Children if you treat them like children or prisoners, don’t be surprised when that’s how they behave 5.5 Positive Patterns Lose the Ego Be a Zen Master This is not to say that you should be naively optimistic at every turn, but you would do well to be less vocally skeptical while still letting your team know you’re aware of the intricacies and obstacles involved in your work Cogs: Be aware that a little movement at the higher cog( vp) makes the lower cogs move alot. Your visible attitude about absolutely everything—no matter how trivial—is unconsciously noticed and spreads infectiously to your team The key is to ==ask questions with humility, respect, and trust==. Be a Catalyst Remove Roadblocks Be a Teacher and Mentor Set Cear Goals Be Honest I won’t lie to you, but I will tell you when I can’t tell you something or if I just don’t know. Track Happiness 5.6 The Unexpected Questions 5.7 Other Tips and Tricks Delegate, but get your hands dirty Seek to replace yourself ?? Under all circumstance ?? Know when to make waves; ie to get your team what they need Shield your team from chaos Let the team know when they’re doing well Give your team air cover Ze’ev was really good at this it’s just as important that you defend them from a lot of the uncertainty and frivolous demands that can be imposed upon you from outside your team. Share as much information as you can with your team, but don’t distract them with organizational craziness that is extremely unlikely to ever actually affect them It’s easy to say “yes” to something that’s easy to undo 5.8 People are Like Plants Different people need different things you need to motivate the ones who are in a rut and provide stronger direction to those who are distracted or uncertain of what to do. Of course, there are those who are “adrift” and need both motivation and direction. So, with this combination of motivation and direction, you can make your team happy and productive. And you don’t want to give them too much of either—because if they don’t need motivation or direction and you try giving it to them, you’re just going to annoy them. Intrinsic vs Extrinsic Motivation With autonomous employees you might give them the general direction in which they need to take the product but leave it up to them to decide how to get there Give ample opportunities for engineers to learn new things and master their craft so as to keep them sharp, efficient, and effective 5.9 Conclusion 6 Leading At Scale 6.1 Always Be Deciding Identify the Blinders Being new to a domain can be an advantage. You can question the old assumptions, ask questions, and then consider new strategies. Identify the Key Trade Offs Ex: Latency, Capacity, Quality Decide and Iterate 6.2 Always Be Leaving Your Mission is to Build a Self Driving Team Dividing Up the Problem Space Challenging problems are usually composed of difficult subproblems. If you’re leading a team of teams, an obvious choice is to put a team in charge of each subproblem. Caveat: subproblems changes over time, so we need a team structure that can evolve. Maybe that’s why a matrix org 6.3 Always Be Scaling The Cycles of Success Stages Analysis Struggle Traction Reward The reward may be a whole new problem to solve. (Remember the heart of the machine book?) graph LR; StruggleIth --\u003e FakeIt; FakeIt --\u003e Traction; Traction --\u003e Reward; Reward --\u003e Compression; Compression --\u003e StruggleIth; Compression Definition: solve the old partially-solved problem and new problem The act of compressing a problem isn’t just about figuring out how to maximize your team’s efficiency, but also about learning to scale your own time and attention to match the new breadth of responsibility. Important Vs Urgent Learn to Drop Balls If dropping some number of balls is inevitable, isn’t it better to drop certain balls ==deliberately== rather than accidentally? Like Mary Kondo, Divide your pile of balls into three groups: the bottom 20% are probably neither urgent nor important and very easy to delete or ignore. There’s a middle 60%, which might contain some bits of urgency or importance, but it’s a mixed bag. At the top, there’s 20% of things that are absolutely, critically important. ==Tackle the top 20% of the tasks== Protecting Your Energy 6.4 Conclusion 7 Measuring Engineering 7.1 What’s Goal? A goal should be written in terms of a desired property 5 Aspects of Productivity (and its trade offs): QuAnts ==Qu==ality of Code What is the quality of the code produced? Are the test cases good enough to prevent regressions? How good is an architecture at mitigating risk and changes? ==A==ttention of Engineer How frequently do engineers reach a state of flow? How much are they distracted by notifications? Does a tool encourage engineers to context switch? I==n==tellecutal complexity How much cognitive load is required to complete a task? What is the inherent complexity of the problem being solved? Do engineers need to deal with unnecessary complexity? ==T==empo and Velocity How quickly can engineers accomplish their tasks? How fast can they push their releases out? How many tasks do they complete in a given timeframe? ==S==atisfaction How happy are engineers with their tools? How well does a tool meet engineers’ needs? How satisfied are they with their work and their end product? Are engineers feeling burned out? 7.2 Signals A signal is the way in which we will know we’ve achieved our goal If the goal is to write high quality code as result of readability process, 2 possible signals are Engineers who have been granted readability judge their code to be of higher quality than engineers who have not been granted readability. The readability process has a positive impact on code quality. 7.3 Metrics Metrics are where we finally determine how we will measure the signal. Metrics are not the signal themselves Ex: a survey II PROCESSES 8 Style Guides and Rules 9 Code Review Perspectives Code is a maintenance task to someone somewhere down the line Make sure you have done research on previous library utlility classes Communicate a new design to the proper group before any code is written Code review should be separated from design review design discussion should be done in design document As much as a code review of entirely new code should not come out of the blue, the code review process itself should also not be viewed as an opportunity to revisit previous decisions. Code Review Benefits Code correctness Code comprehension Code consistency Psychologic and cultural benefits Code review, when it works best, provides not only a challenge to an engineer’s assumptions, but also does so in a prescribed, neutral manner, acting to temper any criticism which might otherwise be directed to the author if provided in an unsolicited manner. Validation: Even the most capable engineers can suffer from imposter syndrome and be too self-critical. A process like code review acts as validation and recognition for one’s work. Knowledge sharing 3 Types of Approvals at Google Correctness and comprehnesion check that the cod eis appropriate and does what the author claims it does Code is appropriate for this part of the code base Readability Code Review Best Practices Be polite and profefssional Changes can be submitted after those changes are made, without any additional rounds of review Reviewers should defer to authors on particular approaches and only point out alternatives if the author’s approach is ==deficient==. If an author can demonstrate that several approaches are equally valid, the reviewer should accept the preference of the author Reviewers should be careful about jumping to conclusions based on a code author’s particular approach. It’s better to ask questions on why something was done the way it was before assuming that approach is wrong. Reviewers should avoid responding to the code review in piecemeal fashion. Few things annoy an author more than getting feedback from a review, addressing it, and then continuing to get unrelated further feedback in the review process. Commiter treat each reviewer comment within a code review as a TODO item; a particular comment might not need to be accepted without question, but it should at least be addressed. If you disagree with a reviewer’s comment, let them know, and let them know why and don’t mark a comment as resolved until each side has had a chance to offer alternatives. One common way to keep such debates civil if an author doesn’t agree with a reviewer is to offer an alternative and ask the reviewer to PTAL (please take another look). Remember that code review is a learning opportunity for both the reviewer and the author. That insight often helps to mitigate any chances for disagreement. Write Small Changes Write Good Change Descriptions A change description should indicate its type of change on the first line, as a summary. The first line is prime real estate and is used to provide summaries within the code review tool itself Keep Reviewers # to a Minimum Types of Code Reviews Greenfield project not very common code is a liability; ensure it’s really ncessary and solves a real problem rather provide just another alternative requires extensive design review API matches agreed design Imperative to ensure code will stand test of time Test fully with unit test code has proper owners Sufficiently commented and supplemental documentation Possible CICD integration Bug Fixes and Rollbacks A bug fix should focus solely on fixing the indicated bug and (usually) updating associated tests to catch the error that occurred in the first place. Refactoring and Large Scale Changes Conclusion Code review has many benefits, including ensuring code correctness, comprehension, and consistency across a codebase. Always check your assumptions through someone else; optimize for the reader. Provide the opportunity for critical feedback while remaining professional. Code review is important for knowledge sharing throughout an organization. Automation is critical for scaling the process. The code review itself provides a historical record. Google uses Gerrit code review 10 Documentation Answers questions such as Why were these design decisions made? Why did we implement this code in this manner? Why did I implement this code in this manner, if you’re looking at your own code two years later? Why are there more documentations? the benefits aren’t immediate, especially to the writer Engineers often view writing as a separate skill than that of programming. (We’ll try to illustrate that this isn’t quite the case, and even where it is, it isn’t necessarily a separate skill from that of software engineering.) Some engineers don’t feel like they are capable writers. But you don’t need a robust command of English2 to produce workable documentation. You just need to step outside yourself a bit and see things from the audience’s perspective. Writing documentation is often more difficult because of limited tools support or integration into the developer workflow. Documentation is viewed as an extra burden—something else to maintain—rather than something that will make maintenance of their existing code easier There are different types of documentations; each with its customer Reference documentation, including code comments Design documents Tutorials Conceptual documentation Landing pages 11 Testing Overview Conclusion Automated testing is foundational to enabling software to change. For tests to scale, they must be automated. A balanced test suite is necessary for maintaining healthy test coverage. “If you liked it, you should have put a test on it.” Changing the testing culture in organizations takes time. 12 Unit Testing 12.1 Importance of Maintainability While tests can help code maintenance and quality, ==ineffective== tests can drain productivity and developer satisfaction. 12.2 Preventing Brittle Tests Brittle tests false negative: fails in the face of unrelated changes; where there is no real bugs. Strive for Unchanging Tests We should strive not to change any existing tests after it is written. Types of commits: pure refactoring new features bug fixes behavior changes ==Action: Test Via Public APIs== Use the apis that customer will use Given APIs public void processTransaction(Transaction transaction) { if (isValid(transaction)) { saveToDatabase(transaction); } } private boolean isValid(Transaction t) { return t.getAmount() \u003c t.getSender().getBalance(); } private void saveToDatabase(Transaction t) { String s = t.getSender() + \",\" + t.getRecipient() + \",\" + t.getAmount(); database.put(t.getId(), s); } Naive Ineffective Tests @Test public void emptyAccountShouldNotBeValid() { assertThat(processor.isValid(newTransaction().setSender(EMPTY_ACCOUNT))) .isFalse(); } @Test public void shouldSaveSerializedData() { processor.saveToDatabase(newTransaction() .setId(123) .setSender(\"me\") .setRecipient(\"you\") .setAmount(100)); assertThat(database.get(123)).isEqualTo(\"me,you,100\"); } Better Tests @Test public void shouldTransferFunds() { processor.setAccountBalance(\"me\", 150); processor.setAccountBalance(\"you\", 20); processor.processTransaction(newTransaction() .setSender(\"me\") .setRecipient(\"you\") .setAmount(100)); assertThat(processor.getAccountBalance(\"me\")).isEqualTo(50); assertThat(processor.getAccountBalance(\"you\")).isEqualTo(120); } ==Action: Test State, Not Interactions== In general, there are two ways to verify that a system under test behaves as expected. With state testing, you observe the system itself to see what it looks like after invoking with it. With interaction testing, you instead check that the system took an expected sequence of actions on its collaborators in response to invoking it. Ineffective The test verifies that a specific call was made against a database API, but there are a couple different ways it could go wrong: If a bug in the system under test causes the record to be deleted from the database shortly after it was written, the test will pass even though we would have wanted it to fail. If the system under test is refactored to call a slightly different API to write an equivalent record, the test will fail even though we would have wanted it to pass. @Test public void shouldWriteToDatabase() { accounts.createUser(\"foobar\"); verify(database).put(\"foobar\"); } Effective Here, we are testing the state of the system under test after interacting with it. @Test public void shouldCreateUsers() { accounts.createUser(\"foobar\"); assertThat(accounts.getUser(\"foobar\")).isNotNull(); } 12.3 Writing Clear Tests Make Your Test Complete and Concise A test is complete when its body contains all of the information a reader needs in order to understand how it arrives at its result. A test is concise when it contains no other distracting or irrelevant information Ineffective @Test public void shouldPerformAddition() { Calculator calculator = new Calculator(new RoundingStrategy(), “unused”, ENABLE_COSINE_FEATURE, 0.01, calculusEngine, false); int result = calculator.calculate(newTestCalculation()); assertThat(result).isEqualTo(5);//Where did this number come from? } - _Effective_ ```java @Test public void shouldPerformAddition() { Calculator calculator = newCalculator(); int result = calculator.calculate(newCalculation(2, Operation.PLUS, 3)); assertThat(result).isEqualTo(5); } Test Behaviors; Not Methods Rationale Ineffective: every production method has ==one== test. As the method under test becomes more complex and implements more functionality, its unit test will become increasingly convoluted and grow more and more difficult to work with Better: each production has N tests, one for each of the method’s behavior we want to verify. Example Method To Test public void displayTransactionResults(User user, Transaction transaction) { ui.showMessage(“You bought a \" + transaction.getItemName()); if (user.getBalance() \u003c LOW_BALANCE_THRESHOLD) { ui.showMessage(“Warning: your balance is low!”); } } - _Ineffecive_ ```java @Test public void testDisplayTransactionResults() { transactionProcessor.displayTransactionResults( newUserWithBalance( LOW_BALANCE_THRESHOLD.plus(dollars(2))), new Transaction(\"Some Item\", dollars(3))); // Ineffective because these makes this test non-concise; each new behavior requires us to update this method. We strive for unchanged tests. assertThat(ui.getText()).contains(\"You bought a Some Item\"); assertThat(ui.getText()).contains(\"your balance is low\"); } Effective @Test public void displayTransactionResults_showsItemName() { transactionProcessor.displayTransactionResults( new User(), new Transaction(\"Some Item\")); assertThat(ui.getText()).contains(\"You bought a Some Item\"); } @Test public void displayTransactionResults_showsLowBalanceWarning() { transactionProcessor.displayTransactionResults( newUserWithBalance( LOW_BALANCE_THRESHOLD.plus(dollars(2))), new Transaction(\"Some Item\", dollars(3))); assertThat(ui.getText()).contains(\"your balance is low\"); } Writing tests as behaviours influences our test strcuture Every behavior has three parts: a “given” component that defines how the system is set up, a “when” component that defines the action to be taken on the system, and a “then” component that validates the result.6 Tests are clearest when this structure is explicit. Doesn’t one of the testing framework follow this flow? Example @Test public void transferFundsShouldMoveMoneyBetweenAccounts() { // Given two accounts with initial balances of $150 and $20 Account account1 = newAccountWithBalance(usd(150)); Account account2 = newAccountWithBalance(usd(20)); // When transferring $100 from the first to the second account bank.transferFunds(account1, account2, usd(100)); // Then the new account balances should reflect the transfer assertThat(account1.getBalance()).isEqualTo(usd(50)); assertThat(account2.getBalance()).isEqualTo(usd(120)); } Don’t Put Logic In Tests Write Clear Failure Messages Ineffective “Test failed: account is closed” Effective Expected an account in state CLOSED, but got account: \u003c{name: “my-account”, state: “OPEN”} 12.4 Test and Code Sharing: DAMP, not DRY Rationale In production code, we strive to be DRY: Don’t Repeat Yourself. So we have common libraries. The downside to such consolidation is that it can make code unclear, requiring readers to follow chains of references to understand what the code is doing. (MC-Kernel) Test code should often strive to be DAMP—that is, to promote “Descriptive And Meaningful Phrases.” A little bit of duplication is OK in tests so long as that duplication makes the test simpler and clearer Ineffective @Test public void shouldAllowMultipleUsers() { List\u003cUser\u003e users = createUsers(false, false); Forum forum = createForumAndRegisterUsers(users); validateForumAndUsers(forum, users); } @Test public void shouldNotAllowBannedUsers() { List\u003cUser\u003e users = createUsers(true); Forum forum = createForumAndRegisterUsers(users); validateForumAndUsers(forum, users); } // Lots more tests... private static List\u003cUser\u003e createUsers(boolean... banned) { List\u003cUser\u003e users = new ArrayList\u003c\u003e(); for (boolean isBanned : banned) { users.add(newUser() .setState(isBanned ? State.BANNED : State.NORMAL) .build()); } return users; } Effective @Test public void shouldAllowMultipleUsers() { User user1 = newUser().setState(State.NORMAL).build(); User user2 = newUser().setState(State.NORMAL).build(); Forum forum = new Forum(); forum.register(user1); forum.register(user2); assertThat(forum.hasRegisteredUser(user1)).isTrue(); assertThat(forum.hasRegisteredUser(user2)).isTrue(); } Example: Shared Values Many tests are structured by defining a set of shared values to be used by tests and then by defining the tests that cover various cases for how these values interact. Example: Ineffective private static final Account ACCOUNT_2 = Account.newBuilder() .setState(AccountState.CLOSED).setBalance(0).build(); private static final Item ITEM = Item.newBuilder() .setName(\"Cheeseburger\").setPrice(100).build(); // Hundreds of lines of other tests... @Test public void canBuyItem_returnsFalseForClosedAccounts() { assertThat(store.canBuyItem(ITEM, ACCOUNT_1)).isFalse(); } This strategy can make tests very concise, but it causes problems as the test suite grows. For one, it can be difficult to understand why a particular value was chosen for a test A better way to accomplish this goal is to construct data using helper methods (see Example 12-22) that require the test author to specify only values they care about, and setting reasonable defaults Example-Better # A helper method wraps a constructor by defining arbitrary defaults for # each of its parameters. def newContact( firstName=\"Grace\", lastName=\"Hopper\", phoneNumber=\"555-123-4567\"): return Contact(firstName, lastName, phoneNumber) # Tests call the helper, specifying values for only the parameters that they # care about. def test_fullNameShouldCombineFirstAndLastNames(self): def contact = newContact(firstName=\"Ada\", lastName=\"Lovelace\") self.assertEqual(contact.fullName(), \"Ada Lovelace\") Example: Shared Setup Used appropriately, these methods can make tests clearer and more concise by obviating the repetition of tedious and irrelevant initialization logic. One risk in using setup methods is that they can lead to unclear tests if those tests begin to depend on the particular values used in setup Exampe: Ineffective: Where does Donald Knuth come from? @Before public void setUp() { nameService = new NameService(); nameService.set(\"user1\", \"Donald Knuth\"); userStore = new UserStore(nameService); } // [... hundreds of lines of tests ...] @Test public void shouldReturnNameFromService() { UserDetails user = userStore.get(\"user1\"); assertThat(user.getName()).isEqualTo(\"Donald Knuth\"); } It is better when a test defines/over-rides all the variable it requires Example: Better private NameService nameService; private UserStore userStore; @Before public void setUp() { nameService = new NameService(); nameService.set(\"user1\", \"Donald Knuth\"); userStore = new UserStore(nameService); } @Test public void shouldReturnNameFromService() { nameService.set(\"user1\", \"Margaret Hamilton\"); UserDetails user = userStore.get(\"user1\"); assertThat(user.getName()).isEqualTo(\"Margaret Hamilton\"); } Example: Shared Helpers and Validation Shared helper method should have a specific purpose Ex: Validation helper method asserts a single conceptual fact about their inputs private void assertUserHasAccessToAccount(User user, Account account) { for (long userId : account.getUsersWithAccess()) { if (user.getId() == userId) { return; } } fail(user.getName() + \" cannot access \" + account.getName()); } 12.5 Conclusion Strive for unchanging tests. Test via public APIs. Test state, not interactions. Make your tests complete and concise. Test behaviors, not methods. Structure tests to emphasize behaviors. Name tests after the behavior being tested. Don’t put logic in tests. 13 Testing Doubles 14 Larger Testing 15 Deprecation III TOOLS 16 Version Control and Branch Management 17 Code Search 18 Build System and Build Philosophy 18.1 Purpose of Build Systems 18.2 What happens if no build System why not ust use a compiler (like javac or gcc)? When we need to scale across our project directories mulitiple compilation units across programming lagnagues Why not just a shell script? Does not scale with complexity: lacks abstraction hard to debug It’s slow build 18.3 Moden Build Systems Modern build systems is all about managing the dependencies tasks (documentation befor e release; external dependencies) Tasks Based Build Systems (Mvn, Ant) the fundamental unit of work is the task. Each task is a script of some sort that can execute any sort of logic, and tasks specify other tasks as dependencies that must run before them. Cons hard to parallelize build steps difficulat to perform incremental build difficult to maintain and debug scripts Artifact Based Build Systems (Bazel) Declare the artifacts you are building; it’s ==declarative== like k8s users specify a set of targets to build (the “what”), and Blaze is responsible for configuring, running, and scheduling the compilation steps (the “how”) java_binary( name = \"MyBinary\", srcs = [\"MyBinary.java\"], deps = [ \":mylib\", ], ) java_library( name = \"mylib\", srcs = [\"MyLibrary.java\", \"MyHelper.java\"], visibility = [\"//java/com/example/myproduct:__subpackages__\"], deps = [ \"//java/com/example/common\", \"//java/com/example/myproduct/otherlib\", \"@com_google_common_guava_guava//jar\", ], ) Distributed Builds Time, Scale, and Tradeoffs 19: Critique: Google Code Review Tools 20 Static Analysis 21 Dependency Management 21.1 Why Dependency Management Is So Difficult Boils down to the Diamond Dependency problem what happens when two nodes in the dependency network have conflicting requirements, and your organization depends on them both graph TD; libUser --\u003e libA; libUser --\u003e libB; libA --\u003e libBase; libB --\u003e libBase; only easy answer is to skip forward or backward in versions for those dependencies to find something compatible. When that isn’t possible, we must resort to locally patching the dependencies in question, which is particularly challenging because the cause of the incompatibility in both provider and consumer is probably not known to the engineer that first discovers the incompatibility 21.2 Importing Dependencies Reuse is healthy, especially compared to the cost of redeveloping quality software from scratch. So long as you aren’t downloading trojaned software, if your external dependency satisfies the requirements for your programming task, you should use it Compatibility Promises for different langagues C++: nearly indefinite backward compatibility. Wow. Downside is once something is published, it is hard to undo Go: You ==cannot== build a library in Go with one version of the language and link that library into a Go program built with a different version of the language. Considerations When Importing Does the project have tests that you can run? Do those tests pass? Who is providing that dependency? Even among “No warranty implied” OSS projects, there is a significant range of experience and skill set—it’s a very different thing to depend on compatibility from the C++ standard library or Java’s Guava library than it is to select a random project from GitHub or npm. Reputation isn’t everything, but it is worth investigating. What sort of compatibility is the project aspiring to? Does the project detail what sort of usage is expected to be supported? How popular is the project? How long will we be depending on this project? How often does the project make breaking changes? How Google Handles Importing Dependencies The overwhelming majority of dependencies in any given Google project are internally developed. 21.3 Dependency Management, In Theory Nothing Changes; don’t change the dependenceis Semantic Versioning SemVer is the nearly ubiquitous practice of representing a version number for some dependency IE: 2.4.71 major.minor.patch major # change represent signifcant incompatibility We can specificy library \u003e 1.5 Bundled Distribution Model aka Spark’s Uber Jar Live At Head 21.4 Limitations of SemVer 21.5 Dependency with Infinit Resources 21.6 Conclusoin Prefer source control problems to dependency management problems: if you can get more code from your organization to have better transparency and coordination, those are important simplifications. Adding a dependency isn’t free for a software engineering project, and the complexity in establishing an “ongoing” trust relationship is challenging. Importing dependencies into your organization needs to be done carefully, with an understanding of the ongoing support costs. A dependency is a contract: there is a give and take, and both providers and consumers have some rights and responsibilities in that contract. Providers should be clear about what they are trying to promise over time. SemVer is a lossy-compression shorthand estimate for “How risky does a human think this change is?” SemVer with a SAT-solver in a package manager takes those estimates and escalates them to function as absolutes. This can result in either overconstraint (dependency hell) or underconstraint (versions that should work together that don’t). By comparison, testing and CI provide actual evidence of whether a new set of versions work together. Minimum-version update strategies in SemVer/package management are higher fidelity. This still relies on humans being able to assess incremental version risk accurately, but distinctly improves the chance that the link between API provider and consumer has been tested by an expert. Unit testing, CI, and (cheap) compute resources have the potential to change our understanding and approach to dependency management. That phase-change requires a fundamental change in how the industry considers the problem of dependency management, and the responsibilities of providers and consumers both. Providing a dependency isn’t free: “throw it over the wall and forget” can cost you reputation and become a challenge for compatibility. Supporting it with stability can limit your choices and pessimize internal usage. Supporting without stability can cost goodwill or expose you to risk of important external groups depending on something via Hyrum’s Law and messing up your “no stability” plan. 22 Large Scale Changes 23 Continuous Integration Conclusion A CI system decides what tests to use, and when. CI systems become progressively more necessary as your codebase ages and grows in scale. CI should optimize quicker, more reliable tests on presubmit and slower, less deterministic tests on post-submit. Accessible, actionable feedback allows a CI system to become more efficient. 24 Continuous Delivery Conclusion Velocity is a team sport: The optimal workflow for a large team that develops code collaboratively requires modularity of architecture and near-continuous integration. Evaluate changes in isolation: Flag guard any features to be able to isolate problems early. Make reality your benchmark: Use a staged rollout to address device diversity and the breadth of the userbase. Release qualification in a synthetic environment that isn’t similar to the production environment can lead to late surprises. Ship only what gets used: Monitor the cost and value of any feature in the wild to know whether it’s still relevant and delivering sufficient user value. Shift left: Enable faster, more data-driven decision making earlier on all changes through CI and continuous deployment. Faster is safer: Ship early and often and in small batches to reduce the risk of each release and to minimize time to market. 25 Compute As a Service (CaaS) 25.1 Taming the Compute Environment In the old days, Jeff Dean needs to do this to run a compute data processing task [Running the task] is a logistical, time-consuming nightmare. It currently requires getting a list of 50+ machines, starting up a process on each of these 50+ machines, and monitoring its progress on each of the 50+ machines. There is no support for automatically migrating the computation to another machine if one of the machines dies, and monitoring the progress of the jobs is done in an ad hoc manner [...] Furthermore, since processes can interfere with each other, there is a complicated, human-implemented “sign up” file to throttle the use of machines, which results in less-than-optimal scheduling, and increased contention for the scarce machine resources Approach 1: simple automations (think Chuck at NPD or Apple) Approach 2: Containerization and Multi-tenancy addresses the 1 program 1 machine paradigm; better usage Improvment: right sizing with replica size and auto-scaling As org and product scales, we need to look at these axes: Number of different applications to be managed Number of copies of an application that needs to run The size of the largest application 25.2 Writing Software for Managed Compute Architecting for Failure K8s service abstraction over k8s pods; pods can fail, but we can restart automatically Batch Versus Serving Serving: program that runs indefinitely and serves incoming requests Batch: program that completes a task and runs to completion Batch jobs are primarily interested in throughput of processing. Serving jobs care about latency of serving a single request. Batch jobs are short lived (minutes, or at most hours). Serving jobs are typically long lived (by default only restarted with new releases). Because they’re long lived, serving jobs are more likely to have longer startup times. Managing State Easiest: make it stateless and extract all storage to an external storage system But how is external storage system implemented with failover (ie with replicatoin) Another type of state is cache if cache goes down, and service has enough capacity, then we just have a latency hit. Connecting to a Service other applications refer to your application by some identifier that is durable across restarts of the specific “backend” instances This is called service discovery (ie Zookeeper) 25.3 CaaS Over Time and Scale 25.4 Choosing a Compute Service Centralization vs Customization Level of Abstraction: Serverless What is serverless? The key defining trait of a framework is the inversion of control—so, the user will only be responsible for writing an “Action” or “Handler” of some sort—a function in the chosen language that takes the request parameters and returns the response. ==The executed code has to be stateless== Pro Don’t need to provision cluster can scale cost to 0 if traffic is 0 adaptable scaling Con Code has to be stateless; request-scope Public vs Private Private (ie Ebay) all the managment of the cluster/infra will be on to you. Public Cloud An organization using a public cloud is effectively outsourcing (a part of) the management overhead to a public cloud provider. Not bad. But how not to be lock in? use open source (ie K8s) to run on EC2. Hybrid cloud: AWS + Google CLoud + private hybrid cloud strategies require the multiple environments to be connected well, through direct network connectivity between machines in different environments and common APIs that are available in both 25.5 Conclusion Scale requires a common infrastructure for running workloads in production. A compute solution can provide a standardized, stable abstraction and environment for software. Software needs to be adapted to a distributed, managed compute environment. The compute solution for an organization should be chosen thoughtfully to provide appropriate levels of abstraction.","title":"Software Engineer At Google"},{"categories":["layouts","paige"],"date":"0001-01-01T00:00:00Z","description":"Demonstration of the search layout.","keywords":null,"link":"/layouts/search/","tags":["search"],"text":"The paige/search layout provides site search. Example config.yaml: outputs: home: [\"atom\", \"html\", \"paige-search\", \"rss\"] Example content/layouts/search.md: --- layout: \"paige/search\" title: \"Search\" --- Result:","title":"Search"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"A brief description of Hugo Shortcodes.","keywords":null,"link":"/content/rich-content/","tags":["privacy","shortcodes"],"text":" Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds. YouTube Privacy Enhanced Shortcode Twitter Simple Shortcode “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.” — Jan Tschichold pic.twitter.com/gcv7SrhvJb — Design Reviewed | Graphic Design History (@DesignReviewed) January 17, 2019 Vimeo Simple Shortcode","title":"Rich Content"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"Lorem Ipsum Dolor Si Amet.","keywords":null,"link":"/content/placeholder-text/","tags":["markdown","text"],"text":" Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum. Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis. Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon Mane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis. Iubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et. Eurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.","title":"Placeholder Text"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"A brief guide to setup KaTeX.","keywords":null,"link":"/content/math-typesetting/","tags":["katex","math","typesetting"],"text":" Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries. In this example we will be using KaTeX Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: {{ if or .Params.math .Site.Params.math }} {{ partial \"math.html\" . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project’s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions Examples Inline math: \\(\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…\\) Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","title":"Math Typesetting"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"Sample article showcasing basic Markdown syntax and formatting for HTML elements.","keywords":null,"link":"/content/markdown-syntax/","tags":["css","html","markdown"],"text":" This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme. Headings The following HTML \u003ch2\u003e—\u003ch6\u003e elements represent five levels of section headings. \u003ch2\u003e is the highest section level while \u003ch6\u003e is the lowest. H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat. Itatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat. Blockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations. Blockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote. Blockquote with attribution Don’t communicate by sharing memory, share memory by communicating. — Rob Pike1 Tables Tables aren’t part of the core Markdown spec, but Hugo supports supports them out-of-the-box. Name Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block indented with four spaces \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block with Hugo’s internal highlight shortcode \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format. H2O Xn + Yn = Zn Press CTRL+ALT+Delete to end the session. Most salamanders are nocturnal, and hunt for insects, worms, and other small creatures. The above quote is excerpted from Rob Pike’s talk during Gopherfest, November 18, 2015. ↩︎","title":"Markdown Syntax Guide"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"Case Study of Various Machine Learning Systems","keywords":null,"link":"/books/machine-learning/machine-learning-system-design-interview/","tags":null,"text":" 1 Clarify Requirement Chapter 2: Visual Search system Chapter 3: Google Street View Blurring system Chapter 4: Youtube Video Search Chapter 5: Harmful Content Detection Chapter 6: Video Recommendation Service Chapter 7: Event Recommendation Service Chapter 8: Ad Click Prediction Chapter 9: Similar Listings on Vacation Rental Chapter 10: Personalized News Feed Chapter 11: People You May Know 2 Frame as ML Defining the ML Objective Maximize # user clicks Maximize # completed videos Maximize watch time Specify Input and Output Late vs Early Fusion (Ch5) ML Category High Level Supervised Classification Single Class (aka Binary) Granularity matters: in detecting harmful content, if we have a binary class (harmful/safe), it will be hard to (1) explain why post is taken down (2) improve model because which we don’t know which class performs less well Multi-class if the classes are mutually exclusive Multi-label if the classes are not mutually exclusive Multi-task (Ch 5: Detecting Harmful Content) multiple models learn multiple task simultaneously; each task has a loss function we are learning Classification head : is a linear layer which projects X dimension to Y dimension to produce logits of size K, where K is the number of classes hf blog. This is then fed to a softmax for multiclass single label sigmoid for binary or (multi class multilabel) (book Deep Learning with Python p114) graph TD; SharedLayers--\u003eFusedFeatures; ViolenceClassHead--\u003eSharedLayers; NudityClassHead--\u003eSharedLayers; HateClassHead--\u003eSharedLayers; Regression Note: Most examples in book are supervised classification Unsupervised Clustering Dimension Reduction Association Reinforcement Learning: AI agent learns to take actions in an environment in order to maximize a particular reward Generative Generates new data, rather than making predictions based on existing data Ranking Statistical (Similarity Measurement) ref euclidean distance cosine similarity Ochiai coefficient taniomoto metric Jaccard index Pearson coefficient Spearman’s rank correlation coefficient Hamming distance (Ch2: Representation Learning) Visual Search (Ch 4) Representation Learning Video Encoder Text Encoder Text Search (Ch 4) Inverted Index Learning To Rank (Ch7) Pointwise (Ch7,8,10) Pairwise Listwise Session Based Recommendation Fusing Layers Classification Single binary classifier (Ch5) One binary classifier per class (Ch5) Multi-label classifier (Ch5) Multi-task classifier (Ch5) Shared layer Task Specific Layer Non Personalized Rule based Personalization (Ch6) Content based filtering Collaborative Filtering Hybrid filtering Rule Based (Ch6, 7) Embedding Based (Ch7) 3 Data Preparation 3.1 Data Engineering Data Sources Is it clean? Can it be trusted? Are there privacy issues? Data Storage SQL: Relational (Postgres) NoSQL: KeyValue(Redis, DynamoDB), Column(Cassandra, HBase), Graph(Neo4J), Document(MongoDB, DynamoDB) Data Types Structured Ex: Numerical \u0026 Categorical Resides in Relational, NoSQL, DataWarehouse Models: Decision Tree, Linear Models, SVM, KNN, Naives Bayse, and Deep Learning Unstructured Ex: Audio, Video, Image, Text Resides in NoSQL DB, Data Lakes Models: Deep Learning Entities Images Users Posts Events Friendships Ads Listings EntityX-EntitY Interaction Annotated dataset 3.2 Feature Engineering Feature Engineering Operations Handle Missing Values Replace with: default values mean, media, or mode (the most common value) Feature Scaling Rationale: Some ML models learn better when features values are in similar ranges Normalization: (aka min-max scaling)$$ z = \\frac{x-x_{min} }{x_{max}-x_{min}} $$ changes distribution Z-score normalization: $$ z = \\frac{x-\\mu }{\\sigma} $$ log scaling: good for long tail distribution Discretization (Bucketing) Encoding Categorical Features (1 hot encoding, ) via embeddings: good for high cardinality features Preparing Text Data (Ch4) Text Normalization lower case punctuation removal trim white spaces Lemmatization Stemming Tokenization Word Subword Character Inverted Index Token To Ids Lookup table hashing Text Encoder (training this can be considered as modeling) Statistical: BOW, TFIDF ML Based: (Ch5) Embedding Layer: f(tokenId) =\u003e embedding Word2Vec: use shallow neural network leverage co-occurrence of words in ==local context== Continuous Bag of Word (cBOW) Skipgram Transformer: Bert, GPT3, Bloom Preparing Video Data (Ch2, 4) Decoding,Sample Resize, scale z-score normalization Correct Color Mode User reaction to post (Ch5) Author Features (Ch5) Contextual Information (Ch5) time of day device Video Features (Ch6) Video ID Duration Language Titles and tags User Features (Ch6) Demographics Contextual Historical interactions Search history Liked Videos Watched Videos Impressions Location Features (Ch7) walk score country user to location affinity Time Related Features (Ch7) time remaining till event Social Related Features (Ch7) number of people attending Event Related Features (Ch7) Ad Features Category/Subcategory Engagement numbers Image/Videos 4 Model Development 4.1 Model Selection Factors to consider in selecting models Amount of data the model needs Training speed Hyper-parameters to choose Possibility of continuous/lifelong learning learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks Each data is trained only once; Available to a flavor of NN, where the typical back propagation is approximate as instantaneous ref Compute requirements/cost Model interpretability ==Interview: Understand the algorithm’s pro/con with respect to requirements: ie business -\u003e ml objective constraints: ie on mobile device== Different types of Models TDS: Top10 Algo CNN based architecture (Ch2) Text Encoder (Ch4) Statistical BoW TF-IDF ML Based Embedding Layer Word2Vec Transformer Based Video Encoder (Ch4) Video Level Models Frame level models Multi-task NN (Ch5) Matrix Factorization (Ch6) Feedback Matrix Training Loss Squared distance over observed pairs Squared distance over all pairs weighted combination of observed/un pairs Optimization Algo SGD WALS Two Tower Network (Ch6) Logistic Regression (LR) (Ch7,8) LR + Feature Crossing (Ch 8) Decision Tree (Ch7) Bagging Boosting Decision Tree GBT XGBoost: has L1 and L2 regularization Neural Network (Ch7) Deep \u0026 Cross Network (Ch 8) Factorization Machines (Ch 8) Deep Factorization Machines 4.1 Model Training 4.1.1 Construct Training Data Collect the raw data Ex: \u003cquery, item1:similar, item2:nonSimilar, item3:nonSimilar\u003e Identify Features and Labels labels: human judgments natural implicit: user clicks as proxy for similarity (Ch2) explicit: ratings Select sampling strategy ref : select a subset of the population so we get a good representation stratified sampling: for each population class, select a subPopulation A that is proportional A’s population in the total population cluster sampling: cluster the population, and select a portion from Split the Data in training, validation, and test ref validation is for tuning hyper-parameters k-fold cross-validation procedure can help if don’t have enough data for complex computational models (large NN) This method guarantees that the score of our model does not depend on the way we picked the train and test set. Address class imbalance (if any) Upsample the minority class; under-sample the majority class Modify the loss function class-balanced loss paper focal loss ref1 paper The focal loss is designed to address the class imbalance by down-weighting the easy examples such that their contribution to the total loss is small even if their number is large $$CrossEntropy = \\sum_{i=0}^{i=classN} y_i \\log(p_i)$$ Focal loss adds a $\\gamma$ scale factor (\u003c1) to down play the importance of the class we want to pay less attention to$$FocalLoss = \\sum_{i=0}^{i=classN} y_i^\\gamma \\log(p_i)$$ Book Hand label (Ch5) Adding Booked Listing as Global Context (Ch 9) 4.1.2 Choosing the Loss function (==Training Metric==) IMOW: How does MLE/MAP fit into loss functions and optimization algorithm? ==MLE and MAP provides the theoretical framework to derive the loss function, by which optimization algorithm can be applied upon.== Maximum Likelihood Estimation (MLE) Vs Maximum Posterior(MAP) Vs Expectation-Maximization Algorithm Vs Least Squared Error and Cross Entropy Terminologies Likelihood : L($\\mu\\sigma$; data) the likelihood of parameters $\\mu\\sigma$ given we observed the data Probability: P(data; $\\mu\\sigma$) probability density of observing the data with model parameters $\\mu\\sigma$ Both MLE and MP describe how we learn the parameters of our model to learn the likelihood function L($\\mu\\sigma$; data) MLE and MAP provide a mechanism to ==derive== the loss function. With alot of math, we see parameters $\\theta$ which maximize the log likelihood is to minimize the mean squared error tds We then apply gradient descent on the mean squared error Goal of MLE is to infer parameter $\\theta$ to infer the most likely likelihood function for our given probability distribution p (Bernouli, Normal, etc..). $$\\theta_{MLE} = {\\operatorname{argmax}} P(X|\\theta)$$ $$= {\\operatorname{argmax}} \\prod p(x_i|\\theta)$$ $$= {\\operatorname{argmax}} \\log( \\prod p(x_i|\\theta) )$$ $$= {\\operatorname{argmax}} \\sum_{i} \\log p(x_i|\\theta) )$$ Similarly, Maximum Posterior learns the likelihood L($\\mu\\sigma$; data), but approach from Bayes Theorem $$p(\\theta|X) = \\frac{p(X|\\theta) p(\\theta)}{p(X)}$$ $$ p(X|\\theta) p(\\theta)$$ $$= {\\operatorname{argmax}} \\sum_{i} \\log p(x_i|\\theta) + p(\\theta)) )$$ MAP is similar to MLE but has a prior term p($\\theta$) In practice, while the log makes the function easier to differentiate to find the minimum, the function can be still intractable. Expectation maximization algorithm is an ==iterative== algorithm to estimate parameters. Least Squared Error vs Cross Entropy Loss Least Squared error and Cross entropy loss are ways to quantify the difference between between prediction and actual for scalars and probability distribution, respectively Cross Entropy $$H(P,Q) = \\sum_{c=1}P(x) \\log Q(x)$$ Q = probability of prediction P = probability of tree class (true class) c is for a class Entropy vs Cross Entropy vs KL Divergence TDS Entropy How much information is gained; how much uncertainty was reduced (more bits more uncertainty reduced) $$H(P,P) = -\\sum_{c=1}P(x) \\log P(x)$$ Example If every day, there is a 35% sunshine of rain and 1% of rain , what is the entropy for 3 days? (how many bits of information)? $$Entropy = 3 (-0.35\\log(0.35) - 0.01\\log(0.01))$$ $$Entropy = 2.33 bits$$ Cross Entropy: takes into account actual vs predicted probability$$H(P,Q) = -\\sum_{c=1}P(x) \\log Q(x)$$ KL Divergence (aka relative entropy) $$KLDivergence = Entropy - CrossEntropy$$ SIDE: Training Metric vs Evaluation Metric Training metric is used to learn the parameters (ie cross entropy, square loss) often has good mathematical computation behavior computationally efficient (log ==\u003e multiplication becomes addition) numerically stable (underflow and overflow) Evaluation metric (ROC) closer to the ML objective ==Summary of Problem Type, Last Layer, and Loss Function== Problem Type Last Layer Activation Loss Function Classification: Binary Sigmoid Binary Cross Entropy Classification: Multi-class, single label Softmax Categorical Cross Entropy Classification: Multi-class, Multi-label Sigmoid Binary Cross Entropy Regression To Arbitrary Values None MSE Regression To Values Between 0 - 1 Sigmoid MSE or Binary Cross Entropy Cross entropy (Ch5) $$BinaryCrossEntropy = y_i \\log(p(i=1)) + (1-y_i)\\log(p(i=0))$$ $$CrossEntropy = \\sum_{i=0}^{i=classN} y_i \\log(p_i)$$ Contrastive/Triplet Loss Loss lilian blog (Ch2 Visual Representation Learning) Contrastive loss takes a pair of inputs $(x_i, x_j)$ and learns a function $f_\\theta(.)$ that encodes $x_i$ into an embedding vector such that minimizes the embedding distance when they are from the same class but maximizes the distance otherwise. $y_i \\in {(1,…, L)}$ is a label among L class $$L_{contrastive}(x_i, x_j, \\theta)= (y_i=y_j)(f_\\theta(x_i)-f_{\\theta}(x_j))^2 + (y_i!=y_j)\\max(0, \\epsilon-(f_\\theta(x_i)-f_{\\theta}(x_j))^2)$$ Triplet loss takes $(anchor, x^+, x^-)$, and learns a function to minimize the distance between anchor and $x^+$ and maximize the distance between anchor and $x^-$ $$L_{tripliet}=\\sum_{x\\in X} \\max(0, [f(x_{anchor})-f(x^+)]^2 - [f(x_{anchor})-f(x^-)]^2 + \\epsilon)$$ Huber Loss Residual Squared, MSE, MAE $$ residual_{squared} = (y_{predict} - y_{actual})^2$$ $$MAE= \\sum_{i=0}^{i=totalPopulation} (y_{actual} - y_{predict}) $$$$MSE= \\sum_{i=0}^{i=totalPopulation} (y_{actual} - y_{predict})^2 $$ 4.1.3 Regularization Context: Bias Variance Tradeoff Bias quantifies how well/close a model’s prediction captures the ==true== relationship between system’s input and output Models with high bias are simple and fail to capture the complexity of the data. Hence, such models lead to a higher training and testing error. Low bias corresponds to a good fit to the training dataset. Generally, more flexible models result in lower bias. Variance quantifies how well the model ==generalizes== to data set beyond the training dataset It is the amount by which the estimate of the true relationship would change on using a different training dataset. Perspective: Math: Using regression as an example 𝔼|MSE| = 𝔼|$\\sum_{i=0}^{i=totalPopulation} (y_{actual} - y_{predict})^2$| 𝔼 is expected value; in this case, it the mean of MSE of the data set The expected value can be broken down into 𝔼|MSE| = $bias(y_{predict})^2 + var(y_{predict}) + \\sigma^2$ $\\sigma$ is the irreducible error ==This is the bias variance tradeoff mathematically!!== Choose the right model complexity to minimize the expected error, by choosing the right parameter to minimal bias and variance ref: TDS Perspective: Bulls Eye TDS low bias; low variance: idea low bias; hi variance: shots centered on bulls eye, but shots are spread apart hi bias; low variance: shots are close to each other, but far from bulls eye hi bias, high variance: shots are from each other and far from the bulls eye Perspective: Model Complexity Complex models (ie large weights, complex architecture NN) can capture the relationship between input and output during training time, but does not generalize well too future unseen data. larger model (num param, large values) can overfit Overfit –\u003e start learning the noise, and won’t generalize well. Regularization penalizes complex model so we can improve generalization L1 L2 ref1 ref2 L1 (aka Lasso Regression): penalize weights in proportion to the sum of ==absolute== value of weight. L1 drive the weights of irrelevant features to 0. $$J = \\sum_{i=0}^{numSamples}(y_i-\\sum_{j=0}^{j=numParams}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=0}^{j=numParams}|\\beta_j|$$ $$J = Cost Function$$ $$\\beta = Feature Coefficient $$ $$\\lambda = Regularization Parameter$$ L2 (aka Ridge Regression): penalize weights in proportion to the sum of value of weight. L2 drives the ==outlier== weights to smaller value. $$J = \\sum_{i=0}^{numSamples}(y_i-\\sum_{j=0}^{j=numParams}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=0}^{j=numParams}\\beta^2_j$$ L1 vs L2 ==L1 (Lasso) works as feature selection because it shrinks the less important feature’s coefficient to 0; yielding a more sparse model== why? TDS L2 ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by β1² + β2² ≤ s_ ; constraint is a circle L1 lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by |β1|+|β2|≤ s; constraint is a diamond Visualize: for 2 parameter; axises are the coefficient values L1’s constraint of diamond has edges on the axis where the coefficient_j is 0 –\u003e remove features the lasso and ridge regression coefficient estimates are given by the ==first point== at which an ellipse contacts the constraint region L2 (Ridge) is better for model interpretation because it will reduce the coefficient magnitude, but not 0. We will be able to use the coefficient (aka predictor) to understand what factors are not important. Both reduces overfit increase underfit because larger cofficient value or more parameter will give a bigger cost function. Entropy Regularization paper Dropout 4.1.4 Optimization (TODO) Once we have framed the loss function, how do we differentiate and update the parameters? Common Optimization Methods deep overview ref SGD AdaGrad Momentum RMSProp 4.1.5 Other Talking Points Training from scratch vs fine tuning Distributed training Negative Sampling (Ch 9, ==p222==) 5 Evaluation Offline Metrics Mean Reciprocal Rank (MRR) $$MRR = \\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{rank_i}$$ m = # of lists rank is the position of the ==first== relevant item Con: only consider the first relevant item Recall@k $$Recall = \\frac{numReleveantItemsInTopkPositions}{totalRelevantItems}$$ Con: for some systems, it is hard to know all the relevant items? Precision@k$$Precision = \\frac{numReleveantItemsInTopkPositions}{k}$$ Con: Relevance might need more granularity (ie level of relevance) Applies to only 1 list Mean Average Precision: mAP Averages the precision over ==multiple== lists $$AP = \\frac{\\sum_{i=1}^{k}Precision@iIfItemIisRelevant}{numRelevItemInList}$$ nDCG DCG: cumulative gain of item $$DCG_p=\\sum_{i=1}^{p}\\frac{rel_i}{log_2(i+1)}$$ p = position rel_i is more granular, ie (1, 2, 3, 4, 5) Normalized DCG (nDCG) $$nDCP_p = \\frac{DCG_p}{IDCG_p}$$ IDCG_P is when the output of a ==perfect== ranking system Confusion matrix Used in classification can be extended to multiclass; 3 classes becomes 3x3 table ref Pos Prediction Neg Prediction Metric Pos Class TP FN Recall TPR Neg Class FP TN FPR Metric Precision $$Precision= \\frac{TP}{TP + FP}$$ $$Recall=TPR=\\frac{TP}{TP + FN}$$ $$FPR = \\frac{FP}{FP + TN}$$ - IMOW: - Precision: Of the positive predictions, how many are truly really true? - Recall \u0026 TPR: Of the positive class, what fraction are we prediction true? - FPR: Of the negative class, what fraction are we predicting true? - note: positive class is referred as minority class F1 Score Combines recall and precision into 1 number $$F1 = \\frac{2}{\\frac{1}{recall}+\\frac{1}{precision}}$$ ROC curve \u0026 ROC-AUC (Ch5) Summarize performance of binary classifier for ==positive== class Plot as the fraction of correct predictions for the positive class (y-axis) versus the fraction of errors for the negative class (x-axis). x-axis: FPR y-axis: TPR Ideal: (x=0, y=1) TPR = 1; we get all the positive class right FPR=0; we have no false positives ==ROC analysis does not have any bias toward models that perform well on the minority class at the expense of the majority class== One point on ROC curve corresponds to 1 threshold. ROC-AUC averages the ROC across ==all classifier== thresholds, giving us ==one== number we can compare across multiple classifiers. PR Curve and PR-AUC (Ch5) Plots Recall(x) vs Precision(y) Perfect skill is upper right (x-axis=recall=1, y-axis=precision=1) Precision recall is recommended for high skewed/imbalanced datasets. Using ROC on balanced dataset can yield over-optimistic performance. ROC vs PR AUC ref Diversity Bleu metrics [wiki] : (bilingual evaluation understudy) Typically used in machine translation ROGUE [] The Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scoring algorithm [1] calculates the similarity between a candidate document and a collection of reference documents. Online Metrics CTR Average Time Spent On X Video completion Rate Total watch time Explicit User Feedback Conversion Rate Revenue Lift Bookmark Rate 6 Serving Cloud vs On Device Deployment Model Compression paper Knowledge distillation - Teacher Student model Model pruning - find the least useful parameter and set them to 0. Leads to sparer models General: 5 Types Random:* Simply prune random parameters. Magnitude:* Prune the parameters with the least weight (e.g. their L2 norm). Gradient: Prune parameters based on the accumulated gradient (requires a backward pass and therefore data). Information: Leverage other information such as high-order curvature information for pruning. Learned: Of course, we can also train our network to prune itself (very expensive, requires training)! Pytorch: blog for pytorch api supports random and magnitude Types (local –\u003e per layer) vs (global –\u003e multiple layers) Structured vs Unstructured Unstructured Pruning refers to pruning individual atoms of parameters. E.g. individual weights in linear layers, individual filter pixels in convolution layers, some scaling floats in your custom layer, etc. The point is you prune parameters without their respective structure Structured pruning removes entire structures of parameters. This does not mean that it has to be an entire parameter, but you go beyond removing individual atoms e.g. in linear weights you’d drop entire rows or columns, or, in convolution layers entire filters Structured Unstructured Global – magnitude(==only== L-1 or custom) Local random, magniteude(==any== L-norm or custom) random, magnitude(==only== L-1 or custom) Quantization - store the parameter with less bits. Can be done during or post training. Testing in Production Shadow deployment: 0% of user see new system A/B testing Prediction Pipeline Batch pro: good for processing large amount of data cons: we need to know in advance what is to be generated results will not be near real time Online Prediction Pipeline Embedding Service (Ch2) Nearest Neighbor Service (Ch2) Re-ranking Service (Ch2,4) Visual Search (Ch4) Text Search (Ch4) Candidate Generation (Ch 8) Ranking (Ch 8) Re-Ranking (Ch 8) Triton Indexing Pipeline Indexing Service Nearest Neighbor Performance Exact match Approximate Tree based LSH based Clustering based 7 Monitoring Why system fails in Production Data distribution changes from production What to Monitor Operation-related (Graphana) Latency Throughput CPU BW ML monitor input drifts model accuracy model version Alarms alarms when metric breaches a threshold? OTHER CONSIDERATIONS Cost DynamoDB on AWS has 2 price scheme: on demand or throttled. latter is an order cheaper. Infra needed Integration Points How will model integrate with upstream/downstream data/service/model Data Privacy Ex: Sending PII data to ChatGPT?","title":"Machine Learning Interview"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"A front matter link.","keywords":null,"link":"/content/link/","tags":["link"],"text":"It takes you to the home page.","title":"Link"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/machine-learning/hands-on-large-language-model/","tags":null,"text":"","title":"Hands On Large Language Model"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"A solid mathematics grasp helps one appreciate the beauty of ML.","keywords":null,"link":"/books/machine-learning/essential-math-for-data-science/","tags":null,"text":"1 Basic Math And Calculus Review Exponential Logarithms $$ log_2 8 = x $$ base = 2 x = 3 - Benefits - Logorithms simplifies operations, which helps computation training - Multiplication: log(a x b) = log(a) + log(b) - Division: log(a/b) = log(a) - log(b) - Expoentiation log(a ^ n) = n * log(a) - Inverse: log(x^-1) == log(1/x) = -log(x) - Log is also good for feature engineering, transforming heavy tail distribution to a gaussian distribution, which is ideal for linear regression models - Code from math import log # 2 raised to what power gives me 8? x = log(8, 2) # x = 3.0 Euler’s Number $e$ and Natural Logarithm $e$ = 2.71828 = euler number; e is a constant like $\\pi$ $e=(1 + 1/n)^n$ as n –\u003e $\\infty$, e approaches 2.71828 Application To describe normal distribtuion model logistic regression Given equation to calculate compound interest, we can simplify it by using $e$ $$A = P *(1 + r/n)^{nt} $$ $$A=P * e^{rt}$$ Natural logarithm when use use e as the ==base== of our logarithm; $e$ is the default base for logarithm $$log_e(10) = ln(10)$$ from math import log, e # e raised to what power gives 10? x = log(10) x_detailed = log(10, e) #x = 2.3025 Limit Limit express a value $\\infty$ that is forever being approached, but never reached $$lim_{x \\to \\infty}1/x = 0$$ Derivatives A derivative tells the slope of a function, and it is useful to measure the rate of change at any point in a function. $$ f(x) = x ^2 $$ $$ \\frac{\\partial }{\\partial x}f(x) = \\frac{\\partial}{\\partial x}x^2 = 2x $$ def derivative_x(f, x, step_size): m = (f(x + step_size) - f(x)) / ((x + step_size) - x) return m def my_function(x): return x**2 derivative_x(my_function, 2, .00001) #4.0000100 partial derivatives Partial derivative enables use to find a slope with respect to multiple variables in several directions Suppose u depends on 3 variables x, y, z. To find the slope of u, $$\\frac{\\partial u}{\\partial t} = h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} \\frac{\\partial^2 u}{\\partial y^2} \\frac{\\partial^2 u}{\\partial z^2} \\right)$$ Chain Rule Suppose we have 2 equations that are related like so $$ y = x^2 + 1$$ $$z = y^3 - 2$$ -To find $\\frac{\\partial}{\\partial y}z$ $$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z} {\\partial y} \\frac{\\partial y}{\\partial x} $$ Why do we care? In neural network layers, chain rule enables se to “untangle” the derivative from each layer Integrals integrations find the area under a curve 2 Probability 2.1 Understanding Probability Statistics vs Probability Statistics is the application of probability to data Ex: data distribtuion will impact what probability tool/approach we take 2.2 Probability Math Joint probability what is the probability event_A ==and== event_B happen ==together== ? P(A and B) = P(A) x P(B) Union probability what is the probability event_A ==or== event_B happen? P(A or B) = P(A) + P(B) Conditional Probability and Baye’s Theorem P (A | B) = P(B | A) * P(A)/ P(B) Bayes theorem can be used to chain several conditional probabilities together to keep updating our beliefs based on new information Ex: Given P(Cofee| Cancer) = 0.85, what is P(cancer | coffee?) P(cancer|coffee) = P(coffee | cancer)* P(coffee) / P(cancer) p_coffee_drinker = .65 p_cancer = .005 p_coffee_drinker_given_cancer = .85 p_cancer_given_coffee_drinker = p_coffee_drinker_given_cancer * p_cancer / p_coffee_drinker Joint and Union Conditional Probability P(A and B) = P(A) x P(A|B) P(A or B) = P(A) + P(B) - P(A|B) * P(B) 2.3 Binomial Probability Distribution Binomial distribution measures how likely ==k== successes can happen out of ==n== trials given we expect a ==p== probability Graph: x-axis = num successes = k y-axis = expected p probabilty Example We run 10 tests, which either pass or fail (ie binomial) and has a success rate of 90%. What is the probabibility of having 8 successes? n=10; p=0.9; k=8 from scipy.stats import binom n = 10 p = 0.9 for k in range(n + 1): probability = binom.pmf(k, n, p) print(\"{0} - {1}\".format(k, probability)) # OUTPUT:\t# 0 - 9.99999999999996e-11 # 1 - 8.999999999999996e-09 # 2 - 3.644999999999996e-07 # 3 - 8.748000000000003e-06 # 4 - 0.0001377809999999999 # 5 - 0.0014880347999999988 # 6 - 0.011160260999999996 # 7 - 0.05739562800000001 # 8 - 0.19371024449999993 # 9 - 0.38742048900000037 # 10 - 0.34867844010000004 2.4 Beta Probability Distribution Theta distribtion allows us to calculate the probability of a probability (yikes!) In previous example, what is the probability that p=90%? Beta distribution can help anser that question. Like all probability distribution, the area under the curve adds up to 1. graph Given 8 successes and 2 failures x-axis = p y-axis = likelihood of p 3 Descriptive and Inferential Statistics 3.1 What is Data ==Data does not capture context or explanations== data provides clues, and not truths data may lead us to truth or mislead into erronous conclusions ==I need to be curious how the data was created, who create it, and what the data is not capturing== only 13% of ML projects succeeds 3.2 Descriptive vs Inferential Statistics Descriptive statistics summarizes the data Ex: median, mean, , variance, standard deviation Inferential statistics attemps to uncover attributes of the ==larger== population based on ==a few samples==. Ex: p-values, confidence interval, central limit theorem 3.3 Populations, Samples, and Bias A population is a particular group of interest we want to study A sample is a subset of the population that is ideally random and unbiased biases different types confirmation bias: capturing data that supports your belief survival bias: caputre only the living subjects 3.4 Descriptive Statistics mean vs weighted mean mean over sample n: $$\\overline{x}=\\sum_{i=0}^{n}\\frac{x_i}{n}$$ over entire population N: $$\\mu=\\sum_{i=0}^{N}\\frac{x_i}{N}$$ $$\\mu = \\frac{x_1 + x_2 + .. + x_N}{N} $$ weighted mean $$ weightedmean =\\frac{x_1w_1 + x_2w_2 + .. + x_n*w_n}{w_1 + w_2 + .. + w_n} $$ Variance vs standard deviation variance : $$\\sigma^2=\\frac{\\sum_{i=0}^{N}(x_i - \\mu)^2}{N}$$ standard deviation = $$s^2=\\frac{\\sum_{i=0}^{n}(x_i - \\overline{x})^2}{n-1} $$ Normal distribution Why normal distribution is useful It’s symmetrical; both sides are identically mirrored at the mean, which is the center. Most mass is at the center around the mean. It has a spread (being narrow or wide) that is specified by standard deviation. The “tails” are the least likely outcomes and approach zero infinitely but never touch zero It resembles a lot of phenomena in nature and daily life, and ==even generalizes nonnormal problems because of the central limit theorem==, which we will talk about shortly. Probabibilty Density Function (PDF) vs Cumulative Density Function (CDF) PDF for normal distribution $$f(x)=\\frac{1}{\\sigma}(2\\pi)^{0.5}e^{-\\frac{1}{2}(\\frac{x-\\mu^2}{\\sigma})} $$ PDF returns ==liklihood== and NOT probabibility Probability is the area under the PDF CDF is the area of the PDF as the x axis value increases; so CDF can be used to return the probability for x \u003c some value. from scipy.stats import norm mean = 64.43 # mean weight of golden retreivers std_dev = 2.99 x = norm.cdf(64.43, mean, std_dev) print(x) # prints 0.5; means 50% of the area, ie - Inverse CDF (ppf) - There will be situations where we need to look up an area on the CDF and then return the corresponding x-value. This is a backward usage of the CDF - CDF: x-axis --\u003e area/probability - Inverse CDF: area/probabibility --\u003e x-axis - We use the ppf - Example: - I want to find the weight that 95% of golden retrievers fall under from scipy.stats import norm x = norm.ppf(.95, loc=64.43, scale=2.99) print(x) # 69.3481123445849 lb - z score - use to rescale the normal distribution so the mean is 0 and the standard devation is 1. This makes it easy to compare the spread of one normal distribution to another normal distribution, even if they have different means and variances. 3.5 Inferential Statistics Central Limit Theorem As the population N size increases, even if that population does not follow a normal distribution, the normal distribution still makes an appearance. WOW. Confidence Interval A confidence interval is a range calculation showing how confidently we believe a sample mean (or other parameter) falls in a range for the population mean. Example: Based on a sample of 31 golden retrievers with a sample mean of 64.408 and a sample standard deviation of 2.05, I am 95% confident that the population mean lies between 63.686 and 65.1296 IMOW: you find the x value which encomposes 95% of the PDF area Code First find the critical z value $z_c$ which captures 95% of the PDF curve area from scipy.stats import norm def critical_z_value(p): norm_dist = norm(loc=0.0, scale=1.0) left_tail_area = (1.0 - p) / 2.0 upper_area = 1.0 - ((1.0 - p) / 2.0) return norm_dist.ppf(left_tail_area), norm_dist.ppf(upper_area) print(critical_z_value(p=.95)) # (-1.959963984540054, 1.959963984540054) 2. Use Central limit theorem to proce margein of error, which is the range around the sample mean that contains the population mean at that level of confidence. $$ E = \\frac{+}{-} z_c * \\frac{s}{n^{0.5}} $$ $$ 95% confdence = \\mu \\frac{+}{} E $$ def confidence_interval(p, sample_mean, sample_std, n): # Sample size must be greater than 30 lower, upper = critical_z_value(p) lower_ci = lower * (sample_std / sqrt(n)) upper_ci = upper * (sample_std / sqrt(n)) return sample_mean + lower_ci, sample_mean + upper_ci print(confidence_interval(p=.95, sample_mean=64.408, sample_std=2.05, n=31)) # (63.68635915701992, 65.12964084298008) Understanding P Values p value is the probability of something occurring by chance rather than because of a hypothesized explanation In the context of an experiment, Given our control variable, ie new model the result of something happening, ie increase in click rate A p value is low, ie \u003c 5%, means we ==discard== the $H_0$ null hypothesis $H_0$ null hypotheseis states our variable has no impact; the result is random choice means we accept the $H_1$ alternative hypothesis, which states the control variable is the reason behind the result. Hypothesis Testing Example: does our pill (the variable) reduce the lenght of a fever? Data fever recovery is gaussian mean = 18 days std = 1.5 Experiment: we give drug to 40 people and it took 16 days to recover. Did drug have impact? One tail test to be statistically significant, p \u003c 0.05. What is x, the number of day? from scipy.stats import norm # Cold has 18 day mean recovery, 1.5 std dev mean = 18 std_dev = 1.5 # What x-value has 5% of area behind it? x = norm.ppf(.05, mean, std_dev) print(x) # 15.53271955957279 - Ans= 15.5, which is less than our experiment mean recovery of 16 days, which means our pill is not statistically significant 3.6 T-Distribution: Dealing with Small Samples 3.7 Big Data Considerations and the Texas Sharpshooter Fallacy 4 Linear Algebra 4.1 What is a Vector? A vector is an arrow in space with a specific direction and length, often representing a piece of data Applications of vectors Solvers like the one in Excel or Python PuLP use linear programming, which uses vectors to ==maximize a solution== while meeting those constraints Computer graphics manim library Representation v = [x, y, z] implicit to the origin code import numpy as np v_nump = np.array([3, 2]) v_python_list = [3, 2] Adding vectors IMOW: the origin of the second vector is the end of the 1st vector from numpy import array v = array([3,2]) w = array([2,-1]) # sum the vectors v_plus_w = v + w # display summed vector print(v_plus_w) # [5, 1] - Scaling vectors - IMOW: scales the vector lengh - Span and Linear Dependence - ==Two vectors in opposite directions (ie one of their cordinate has opposite signs) can be used to create an inifinite spans through multiplication and adding== - Terminologies - span : the whole spae of possible vectors - linear indepdenent: two vectors in different directions (ie one of their coordinate has opposite sign) - linear depedent: two vectors in the same direction (or plane for multi-dim vector) - So what? - Linear ==independent== vectors can create an infinite span - When solving system of equations, linear independent vectors gives us more \"flexibility\", which is important to solve the system of equations. - Conversely, linear dependent vectors are - Later on, we will learn determinant, which is a tool to check for linear dependence ### 4.2 Linear Transformations - Basis Vectors $\\hat{i}$ and $\\hat{j}$ - properties - lenght of 1 - perpendicular to earch other - We can use $\\hat{i}$ and $\\hat{j}$ to create any vector we want by scaling and adding them. - Ex ``` python i j basis = [1 0] [0 1] 4.3 Matrix Vector Multiplication - This transformation of a vector by applying basis vectors is known as _matrix vector multiplication_ i j [x'] = [a b] [x] [y'] = [c d] [y] [x'] = [ax + by] [y'] = [cx + dy] from numpy import array # Declare i-hat and j-hat i_hat = array([2, 0]) j_hat = array([0, 3]) # compose basis matrix using i-hat and j-hat # also need to transpose rows into columns basis = array([i_hat, j_hat]).transpose() # declare vector v v = array([1,1]) # create new vector # by transforming v with dot product new_v = basis.dot(v) print(new_v) # [2, 3] 4.4 Determinants During linar transformation, we can increase,decrease,rotate the original spacea area by X times. The X times is the ==determinant== from numpy.linalg import det from numpy import array i_hat = array([3, 0]) j_hat = array([0, 2]) basis = array([i_hat, j_hat]).transpose() determinant = det(basis) print(determinant) # prints 6.0; we increase area by 6. Other transformations can rotate 4.5 Special Type of Matrices Square matrix: numRows = numCols application: eigendecomposition Identity matrix diagonal are 1’s; everything else is 0 application: when you have an identity matrix, you essentially have undone a transformation and found your starting basis vectors $\\hat{i}$ and $\\hat{j}$. This will play a big role in solving systems of equations in the next section. Inverse matrix application: inverse matrix undoes the transformation of another matrix $A^{-1}$ x $A^{1}$ = identity matrix Diagonal matrix only diagonal has non-zero values application: represents a simple scalar along the vector space Triangular matrix only diagonal and the upper right diagonal have non-zero values application: easier to solve in system of equations; good for decomposition tasks like Lower Upper decompositison Sparse matrix matrix is mostly 0s There is no interesting transformation, but this represents an opportunity to optimize the memory via more efficient representation. 4.6 System of Equations and Inverse Matrices Linear algebra is used to solve a system equation via inverse matrix Given 4x + 2y + 4z = 44 5x + 3y + 7z = 56 9x + 3y + 6z = 71 A = 4 2 4 5 3 7 9 3 6 B = 44 56 72 AX = B X = (A^-1)(B) # !!! WOW A = Matrix([ [4, 2, 4], [5, 3, 7], [9, 3, 6] ]) from numpy import array from numpy.linalg import inv A = array([ [4, 2, 4], [5, 3, 7], [9, 3, 6] ]) B = array([ 44, 56, 72 ]) X = inv(A).dot(B) print(X) # [ 2. 34. -8.] 4.7 Eigenvectors and Eigenvalues Matrix decomposition is breaking up a matrix into its basic components, much like factoring numbers (e.g., 10 can be factored to 2 × 5). There are multiple matrix decomposition techniques Linear regression: QR decomposition (Chap 5) eigendecomposition (ie used by PCA) (this chapter) In eigen decomposition, the original matrix A is decomposed to ==eigenvalue $\\lambda$== and ==eigenvector== $\\upsilon$ $$A\\upsilon =\\lambda\\upsilon$$ Example: $$ A = 1 2 4 5 $$ $$\t\\lambda = eigenvalue = [-0.464, 6.464] $$ $$ \\upsilon = eigenvector = [ [0.0806, 0.0343], [0.59, -0.939]] $$ from numpy import array, diag from numpy.linalg import eig, inv A = array([ [1, 2], [4, 5] ]) eigenvals, eigenvecs = eig(A) print(\"EIGENVALUES\") print(eigenvals) print(\"\\nEIGENVECTORS\") print(eigenvecs) \"\"\" EIGENVALUES [-0.46410162 6.46410162] EIGENVECTORS [[-0.80689822 -0.34372377] [ 0.59069049 -0.9390708 ]] \"\"\" 5 Linear Regression 5.1 Basic Linear Regression import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression # Import points df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\") # Extract input variables (all rows, all columns but last column) X = df.values[:, :-1] # Extract output column (all rows, last column) Y = df.values[:, -1] # Fit a line to the points fit = LinearRegression().fit(X, Y) # m = 1.7867224, b = -16.51923513 m = fit.coef_.flatten() b = fit.intercept_.flatten() print(\"m = {0}\".format(m)) print(\"b = {0}\".format(b)) # show in chart plt.plot(X, Y, 'o') # scatterplot plt.plot(X, m*X+b) # line plt.show() 5.2 Residuals and Squared Errors $$ residual_{squared} = (y_{predict} - y_{actual})^2$$ import pandas as pd # Import points points = pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples() # Test with a given line m = 1.93939 b = 4.73333 sum_of_squares = 0.0 # calculate sum of squares for p in points: y_actual = p.y y_predict = m*p.x + b residual_squared = (y_predict - y_actual)**2 sum_of_squares += residual_squared print(\"sum of squares = {}\".format(sum_of_squares)) # sum of squares = 28.096969704500005 5.3 Finding the Best Fit Line 5.3.1 closed form not computationally efficient $$m = \\frac{n\\sum{xy} - \\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2} $$ $$b = \\frac{\\sum{y}}{n} - m\\frac{\\sum{x}}{n} $$ 5.3.2 Via Linear Algebra (Inverse Matrix Technique) Even though ML uses stochastic descent to solve the system of equations, it is instructional to see how linear algebra can be used to arrive at the close form solution more numerical stable Numerical stability is how well an algorithm keeps errors minimized, rather than amplifying errors in approximations. Remember that computers work only to so many decimal places and have to approximate, so it becomes important our algorithms do not deteriorate with compounding errors in those approximations $$b=(X^TX)^{-1} X^T y $$ import pandas as pd from numpy.linalg import inv import numpy as np # Import points df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\") # Extract input variables (all rows, all columns but last column) X = df.values[:, :-1].flatten() # Add placeholder \"1\" column to generate intercept X_1 = np.vstack([X, np.ones(len(X))]).T # Extract output column (all rows, last column) Y = df.values[:, -1] # Calculate coefficents for slope and intercept b = inv(X_1.transpose() @ X_1) @ (X_1.transpose() @ Y) print(b) # [1.93939394, 4.73333333] # Predict against the y-values y_predict = X_1.dot(b) Enhance with QR Matrix Decomposition As the matrix increase in dimension, we want to decompose our original matrix Z $$X = QR $$ $$ b = R^{-1} Q^T y$$ import pandas as pd from numpy.linalg import qr, inv import numpy as np # Import points df = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\") # Extract input variables (all rows, all columns but last column) X = df.values[:, :-1].flatten() # Add placeholder \"1\" column to generate intercept X_1 = np.vstack([X, np.ones(len(X))]).transpose() # Extract output column (all rows, last column) Y = df.values[:, -1] # calculate coefficents for slope and intercept # using QR decomposition Q, R = qr(X_1) b = inv(R).dot(Q.transpose()).dot(Y) 5.3.3 Gradient Descent Gradient descent is an optimization technique that uses derivatives and iterations to minimize/maximize a set of parameters against an objective so what is the objective? when f(x) is linear equation, it’s the minimization of the sum of squares. ==This residual squared is the graph you are trying to minimize.== $$ residual_{squared} = (y_{predict} - y_{actual})^2$$ Ex: Gradient descet to find min of a parabola import random def f(x): return (x - 3) ** 2 + 4 def dx_f(x): return 2*(x - 3) # The learning rate L = 0.001 # The number of iterations to perform gradient descent iterations = 100_000 # start at a random x x = random.randint(-15,15) for i in range(iterations): # get slope d_x = dx_f(x) # update x by subtracting the (learning rate) * (slope) x -= L * d_x print(x, f(x)) # prints 2.999999999999889 4.0 For linear equation, it’s the minimization of the sum of squares Ex: Gradient desceint for linear regression import pandas as pd # Import points from CSV points = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples()) # Building the model m = 0.0 b = 0.0 # The learning Rate L = .001 # The number of iterations iterations = 100_000 n = float(len(points)) # Number of elements in X # Perform Gradient Descent for i in range(iterations): # TWC: Find the derivatives of our sum of squares function with respect to _m_ and _b_ # slope of squared residual with respect to m D_m = sum(2 * p.x * ((m * p.x + b) - p.y) for p in points) # slope of squared residual with respect to b; D_b = sum(2 * ((m * p.x + b) - p.y) for p in points) # update m and b m -= L * D_m b -= L * D_b print(\"y = {0}x + {1}\".format(m, b)) # y = 1.9393939393939548x + 4.733333333333227 5.4 Overfitting and Variance The big-picture objective is not to minimize the sum of squares but to make accurate predictions on new data If we fit a nonlinear curve to the data (overfitting), it will get 0 error, but probably will not generalize well to real data. Overfitted model are more sensitive to outliers, thereby increase variance in our prediction. 5.5 Stochasitc Gradient Descent 5.6 Correlation Coefficient 5.7 Statistical Significance 5.8 Conefficient of Determination 5.9 Standard Error of the Estimate 5.10 Prediction Intervals 5.11 Train/Test Splits 5.12 Multiple Linear Regressions 6 Logistic Regression and Classification 6.1 Understanding Logistic Regression A regression that predicts the probability of an outcome given 1+ independent variable. Characteristics label = binary, or categorical # (multi-class) features: $x_i \\in X$ output = ==probability== -\u003e can be converted into discrete value fairly resilient to outliers 6.2 Performing Logistic Regression Logistic Function creates the sigmoid curve sigmoid curve transforms an input value x into an output range [0,1] $$p(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}}$$ $\\beta_0 + \\beta_1$ are learned parameters x is the input indpendent variable(s), feature(s) import math def predict_probability(x, b0, b1): p = 1.0 / (1.0 + math.exp(-(b0 + b1 * x))) return p 6.2.1 Finding the Coeffcients Previously, in linear regression, we used lease sqaures to find the linear equation. We can either used close form$$m = \\frac{n\\sum{xy} - \\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2} $$ $$b = \\frac{\\sum{y}}{n} - m\\frac{\\sum{x}}{n} $$ - inverse matrix $$b=(X^TX)^{-1} X^T y$$ - approximation: (gradient descent) In logistic regression we use maximum likelihood estimation (MLE) maximizes the likelihood a given logistic curve would output the observed data (for binary, data label is 0,1) MLE(logistic regression) vs least squared (linear regression) We use gradient descent (or a library) to solve (there’s no closed form) sklearn library import pandas as pd from sklearn.linear_model import LogisticRegression # Load the data df = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\") # Extract input variables (all rows, all columns but last column) X = df.values[:, :-1] # Extract output column (all rows, last column) Y = df.values[:, -1] # Perform logistic regression # Turn off penalty model = LogisticRegression(penalty='none') model.fit(X, Y) # print beta1 print(model.coef_.flatten()) # 0.69267212 # print beta0 print(model.intercept_.flatten()) # -3.17576395 MLE (details) High level: claculate parameters that bring our logistic curve to data points as closely as possible (vs regression: as close to the residual loss squared) IMOW: calculate parameter to maximize the likihood probability to fit observed data to the sigmoid curve For binary $$JointLiklihood = \\prod_{i=1}^{n=totalSamples}(\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{y_i} (\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{1-y_i}$$ 1st term applies when data label is 1 2nd term appleis when data label is 0 Mathematical optmization to prevent floating underflow: use log multiple saller number gets you smaller number; ME235! log transforms multiplication into addition $$JointLiklihood = \\sum_{i=1}^{n=totalSamples}\\log(\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{y_i} (\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{1-y_i})$$ code1: Joint liklihood definition joint_likelihood = Sum(log((1.0 / (1.0 + exp(-(b + m * x(i)))))**y(i) * \\ (1.0 - (1.0 / (1.0 + exp(-(b + m * x(i))))))**(1-y(i))), (i, 0, n)) code2: use gradient descent (simpy) from sympy import * import pandas as pd points = list(pd.read_csv(\"https://tinyurl.com/y2cocoo7\").itertuples()) b1, b0, i, n = symbols('b1 b0 i n') x, y = symbols('x y', cls=Function) joint_likelihood = Sum(log((1.0 / (1.0 + exp(-(b0 + b1 * x(i))))) ** y(i) \\ * (1.0 - (1.0 / (1.0 + exp(-(b0 + b1 * x(i)))))) ** (1 - y(i))), (i, 0, n)) # Partial derivative for m, with points substituted d_b1 = diff(joint_likelihood, b1) \\ .subs(n, len(points) - 1).doit() \\ .replace(x, lambda i: points[i].x) \\ .replace(y, lambda i: points[i].y) # Partial derivative for m, with points substituted d_b0 = diff(joint_likelihood, b0) \\ .subs(n, len(points) - 1).doit() \\ .replace(x, lambda i: points[i].x) \\ .replace(y, lambda i: points[i].y) # compile using lambdify for faster computation d_b1 = lambdify([b1, b0], d_b1) d_b0 = lambdify([b1, b0], d_b0) # Perform Gradient Descent b1 = 0.01 b0 = 0.01 L = .01 for j in range(10_000): b1 += d_b1(b1, b0) * L b0 += d_b0(b1, b0) * L print(b1, b0) # 0.6926693075370812 -3.175751550409821 6.3 Multivariable Logistic Regression I frame the previous 6.2 section as logistic regression Example: Employment retention import pandas as pd from sklearn.linear_model import LogisticRegression employee_data = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\") # grab independent variable columns inputs = employee_data.iloc[:, :-1] # grab dependent \"did_quit\" variable column output = employee_data.iloc[:, -1] # build logistic regression fit = LogisticRegression(penalty='none').fit(inputs, output) # Print coefficients: print(\"COEFFICIENTS: {0}\".format(fit.coef_.flatten())) print(\"INTERCEPT: {0}\".format(fit.intercept_.flatten())) # Interact and test with new employee data def predict_employee_will_stay(sex, age, promotions, years_employed): prediction = fit.predict([[sex, age, promotions, years_employed]]) probabilities = fit.predict_proba([[sex, age, promotions, years_employed]]) if prediction == [[1]]: return \"WILL LEAVE: {0}\".format(probabilities) else: return \"WILL STAY: {0}\".format(probabilities) # Test a prediction while True: n = input(\"Predict employee will stay or leave {sex}, {age},{promotions},{years employed}: \") (sex, age, promotions, years_employed) = n.split(\",\") print(predict_employee_will_stay(int(sex), int(age), int(promotions), int(years_employed))) 6.4 Understanding the Log-Odds Log odds used by the logit function Since 1900s, mathematicians wants transform a linear function (==1 neural network layer==), and scale the output to the range of [0,1], corresponding to a probability $$JointLiklihood = \\sum_{i=1}^{n=totalSamples}\\log(\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{y_i} (\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1+..+\\beta_nx_n)}})^{1-y_i})$$ linear function: $\\beta_0 + \\beta_1x_1+..+\\beta_nx_n$ ==log odds== = $e^{linear function} = e^{\\beta_0 + \\beta_1x_1+..+\\beta_nx_n}$ logit function uses log odds; they are not the same if we take the log of both side, $$log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1x_1+..+\\beta_nx_n$$![[book-math-linear-to-log.png]] how to convert probability to odd? logit function gives us probability p $$odd = \\frac{p}{1-p}$$ So in the employment retention example above, if timeOfEmployment=6 then logit function returns p = 72.7% then $odd = \\frac{0.727}{1-0.727}$ = 2.665 2.66 times more likely to leave More example: see diagram above 6.5 R-Squared -$R^2$ indicates how well a given independent variable explains a dependent variable $$R^2 = \\frac{logLiklihood - logLiklihoodFit}{logLiklihood}$$ $$ logLiklihoodFit = \\sum_{i=0}^n log(f(x_i)) \\times y_i+log(1-f(x_i)) \\times (1-y_i) $$ $$logLiklihood = \\frac{\\sum y_i}{n}\\times y_i + (1-\\frac{\\sum y_i}{n})\\times (1-y_i) $$ import pandas as pd from math import log, exp patient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\") \\ .itertuples()) # Declare fitted logistic regression b0 = -3.17576395 b1 = 0.69267212 def logistic_function(x): p = 1.0 / (1.0 + exp(-(b0 + b1 * x))) return p # calculate the log likelihood of the fit log_likelihood_fit = sum(log(logistic_function(p.x)) * p.y + log(1.0 - logistic_function(p.x)) * (1.0 - p.y) for p in patient_data) # calculate the log likelihood without fit likelihood = sum(p.y for p in patient_data) / len(patient_data) log_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\ for p in patient_data) # calculate R-Square r2 = (log_likelihood - log_likelihood_fit) / log_likelihood print(r2) # 0.306456105756576 6.6 P-Values We need to investigate how likely we would have seen this data by chance rather than because of an actual relationship. This means we need a p-value. Chi Squared distribution $\\chi^2$ Degree of freedom = DOF = (# of parameters in our logistic regression - 1) $\\chi^2$ distribution with DOF = 1 sum each value in a normal distribution and saured $$ pValue = \\chi^2(2(logLiklihoodFit) - logLiklihood)$$ import pandas as pd from math import log, exp from scipy.stats import chi2 patient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples()) # Declare fitted logistic regression b0 = -3.17576395 b1 = 0.69267212 def logistic_function(x): p = 1.0 / (1.0 + exp(-(b0 + b1 * x))) return p # calculate the log likelihood of the fit log_likelihood_fit = sum(log(logistic_function(p.x)) * p.y + log(1.0 - logistic_function(p.x)) * (1.0 - p.y) for p in patient_data) # calculate the log likelihood without fit likelihood = sum(p.y for p in patient_data) / len(patient_data) log_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\ for p in patient_data) # calculate p-value chi2_input = 2 * (log_likelihood_fit - log_likelihood) p_value = chi2.pdf(chi2_input, 1) # 1 degree of freedom (n - 1) print(p_value) # 0.0016604875618753787 6.7 Train/Test Splits Use k-fold validation to reuse all the data for training and test Ex: 3fold cross validation import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import KFold, cross_val_score # Load the data df = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\", delimiter=\",\") X = df.values[:, :-1] Y = df.values[:, -1] # \"random_state\" is the random seed, which we fix to 7 kfold = KFold(n_splits=3, random_state=7, shuffle=True) model = LogisticRegression(penalty='none') results = cross_val_score(model, X, Y, cv=kfold) print(\"Accuracy Mean: %.3f (stdev=%.3f)\" % (results.mean(), results.std())) 6.8 Confusion Matrices Actual:1 Actual:0 Predict: 1 TP FN Sensitivity Predict: 0 FP TN Specificity Precision Accuracy $$Precision=\\frac{TP}{TP+FP}$$ $$Sensitivty=\\frac{TP}{TP+FN}$$ $$Specificity=\\frac{TN}{TN+FP}$$ $$Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$ $$F1Score=\\frac{2\\times Precision\\times Recall}{Precision+ Recall}$$ import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split # Load the data df = pd.read_csv('https://bit.ly/3cManTi', delimiter=\",\") # Extract input variables (all rows, all columns but last column) X = df.values[:, :-1] # Extract output column (all rows, last column)\\ Y = df.values[:, -1] model = LogisticRegression(solver='liblinear') X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33, random_state=10) model.fit(X_train, Y_train) prediction = model.predict(X_test) \"\"\" The confusion matrix evaluates accuracy within each category. [[truepositives falsenegatives] [falsepositives truenegatives]] The diagonal represents correct predictions, so we want those to be higher \"\"\" matrix = confusion_matrix(y_true=Y_test, y_pred=prediction) print(matrix) 6.9 Bayes Theorem and Classification One can use Bayes’ Theorem to bring in outside information to further validate findings on a confusion matrix 6.10 Receiver Operator Characteristics/Area Under Curve graph used tune the probability threshold to balance for the TPR vs FPR y-axis = TPR = Sensitivity x-axis = FPR = (1- Specificty) AUC is used to choose which model has better performance (tree vs linear regression) Code results = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc') print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std())) # AUC: 0.791 (0.051) 6.11 Class Imbalance Class imbalance is still an open problem with no great solution Possibilities collect more data try different models confusion matrix and AUC to track generate synthetic samples of minority data (SMOTE algoritm) duplication minority samples X, Y = ... X_train, X_test, Y_train, Y_test = \\ train_test_split(X, Y, test_size=.33, stratify=Y) 7 Neural Network A neural network is a multilayered ==regression== containing layers of weights, biases, and nonlinear functions that reside between input variables and output variables Each node resembles a linear function before being passed to a nonlinear function (called an activation function) 7.1 When To Use Neural Network and Deep Learning Neural network and DL can be used for classification or regression Linear regression, logistic regression, and GBM trees do well on structured data structure data can be represented as a table Use NN for unstructure data: text: predict the next token images: classify a text 7.2 Simple Neural Network node: a linear equation consiting of $$ X_1W_1 + X_2W_2+B_1 $$ where W are the weights, which are the arrows/path to the node X is the input from the previous layer 1 layer = N nodes M layers So far, it looks similar to linear regression 7.2.1 Activation Function An activation function is a nonlinear function that transforms or compresses the weighted and summed values in a node, helping the neural network separate the data effectively so it can be classified. Types of Activation function Name Typical Layer Description Notes Logistic Output S-shaped sigmoid curve output in [0,1]; used in binary classifcation Softmax Output Ensures all outputs nodes sumes to 1 Useful for multiple classifcations and scale outputs add to 1.0 Tangent Hyperbolic Output tanh, S-shaped sigmoid curve between -1 and 1 Assists in “centering” data by bringing mean close to 0 Relu Hidden Turns negative values to 0 Popular activation faster than sigmoid and tanh, mitigates vanishing gradient problems and computationally cheap Leaky Relu Hidden Multiples neg values by 0.01 Controversial variant of ReLU that marginalizes rather than eliminates negative values 7.2.2 Forward Propagation Mental model with L layers input X –\u003e Hidden: $Relu( W @ X + B){layer-1}$ –\u003e output: $logistic(W@X + B){layer_L}$ import numpy as np import pandas as pd from sklearn.model_selection import train_test_split all_data = pd.read_csv(\"https://tinyurl.com/y2qmhfsr\") # Extract the input columns, scale down by 255 all_inputs = (all_data.iloc[:, 0:3].values / 255.0) all_outputs = all_data.iloc[:, -1].values # Split train and test data sets X_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs, test_size=1/3) n = X_train.shape[0] # number of training records # Build neural network with weights and biases # with random initialization w_hidden = np.random.rand(3, 3) w_output = np.random.rand(1, 3) b_hidden = np.random.rand(3, 1) b_output = np.random.rand(1, 1) # Activation functions relu = lambda x: np.maximum(x, 0) logistic = lambda x: 1 / (1 + np.exp(-x)) # Runs inputs through the neural network to get predicted outputs def forward_prop(X): Z1 = w_hidden @ X + b_hidden A1 = relu(Z1) Z2 = w_output @ A1 + b_output A2 = logistic(Z2) return Z1, A1, Z2, A2 # Calculate accuracy test_predictions = forward_prop(X_test.transpose())[3] # grab only output layer, A2 test_comparisons = np.equal((test_predictions \u003e= .5).flatten().astype(int), Y_test) accuracy = sum(test_comparisons.astype(int) / X_test.shape[0]) print(\"ACCURACY: \", accuracy) 7.3 Backpropagation 7.3.1 Calculate the Weight and Bias Derivatives For each layer, we do a partial derivative of the cost function $C=(A_{layer_i-1}^2-Y_{layer_i-1})$ C is the cost function $A_{layer-1}$ is the output of layer i-1 = ReLU($Z_{layer-1} = (W @ X + B)_{layer-2}$) 7.3.2 Stochastic Gradient Descent Raw import numpy as np import pandas as pd from sklearn.model_selection import train_test_split all_data = pd.read_csv(\"https://tinyurl.com/y2qmhfsr\") # Learning rate controls how slowly we approach a solution # Make it too small, it will take too long to run. # Make it too big, it will likely overshoot and miss the solution. L = 0.05 # Extract the input columns, scale down by 255 all_inputs = (all_data.iloc[:, 0:3].values / 255.0) all_outputs = all_data.iloc[:, -1].values # Split train and test data sets X_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs, test_size=1 / 3) n = X_train.shape[0] # Build neural network with weights and biases # with random initialization w_hidden = np.random.rand(3, 3) w_output = np.random.rand(1, 3) b_hidden = np.random.rand(3, 1) b_output = np.random.rand(1, 1) # Activation functions relu = lambda x: np.maximum(x, 0) logistic = lambda x: 1 / (1 + np.exp(-x)) # Runs inputs through the neural network to get predicted outputs def forward_prop(X): Z1 = w_hidden @ X + b_hidden A1 = relu(Z1) Z2 = w_output @ A1 + b_output A2 = logistic(Z2) return Z1, A1, Z2, A2 # Derivatives of Activation functions d_relu = lambda x: x \u003e 0 d_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2 # returns slopes for weights and biases # using chain rule def backward_prop(Z1, A1, Z2, A2, X, Y): dC_dA2 = 2 * A2 - 2 * Y dA2_dZ2 = d_logistic(Z2) dZ2_dA1 = w_output dZ2_dW2 = A1 dZ2_dB2 = 1 dA1_dZ1 = d_relu(Z1) dZ1_dW1 = X dZ1_dB1 = 1 dC_dW2 = dC_dA2 @ dA2_dZ2 @ dZ2_dW2.T dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2 dC_dA1 = dC_dA2 @ dA2_dZ2 @ dZ2_dA1 dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T dC_dB1 = dC_dA1 @ dA1_dZ1 * dZ1_dB1 return dC_dW1, dC_dB1, dC_dW2, dC_dB2 # Execute gradient descent for i in range(100_000): # randomly select one of the training data idx = np.random.choice(n, 1, replace=False) X_sample = X_train[idx].transpose() Y_sample = Y_train[idx] # run randomly selected training data through neural network Z1, A1, Z2, A2 = forward_prop(X_sample) # distribute error through backpropagation # and return slopes for weights and biases dW1, dB1, dW2, dB2 = backward_prop(Z1, A1, Z2, A2, X_sample, Y_sample) # update weights and biases w_hidden -= L * dW1 b_hidden -= L * dB1 w_output -= L * dW2 b_output -= L * dB2 # Calculate accuracy test_predictions = forward_prop(X_test.transpose())[3] # grab only A2 test_comparisons = np.equal((test_predictions \u003e= .5).flatten().astype(int), Y_test) accuracy = sum(test_comparisons.astype(int) / X_test.shape[0]) print(\"ACCURACY: \", accuracy) Using scikit import pandas as pd # load data from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier df = pd.read_csv('https://bit.ly/3GsNzGt', delimiter=\",\") # Extract input variables (all rows, all columns but last column) # Note we should do some linear scaling here X = (df.values[:, :-1] / 255.0) # Extract output column (all rows, last column) Y = df.values[:, -1] # Separate training and testing data X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3) nn = MLPClassifier(solver='sgd', hidden_layer_sizes=(3, ), activation='relu', max_iter=100_000, learning_rate_init=.05) nn.fit(X_train, Y_train) # Print weights and biases print(nn.coefs_ ) print(nn.intercepts_) print(\"Training set score: %f\" % nn.score(X_train, Y_train)) print(\"Test set score: %f\" % nn.score(X_test, Y_test)) 7.4 Limitation of Neural Networks and Deep Learning NN can overfit layers, nodes, and activation functions makes it flexible fitting to data in a nonlinear manner 7.5 Conclusion 8 Career Advice and Path Forward data science is software engineering with proficiency in statistics, machine learning, and optimization","title":"Essential Math For Data Science"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"Guide to emoji usage in Hugo.","keywords":null,"link":"/content/emoji-support/","tags":["emoji"],"text":" Emoji can be enabled in a Hugo project in a number of ways. The emojify function can be called directly in templates or Inline Shortcodes. To enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g. 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil: The Emoji cheat sheet is a useful reference for emoji shorthand codes. N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g. .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","title":"Emoji Support"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"Great book on how to implement data science from first principles.","keywords":null,"link":"/books/machine-learning/data-science-from-scratch/","tags":null,"text":"1 Introduction 2 Crash Course In Python Tuples List’s immutable cousin my_list = [1, 2] my_tuple = (1, 2) other_tuple = 3, 4 my_list[1] = 3 # my_list is now [1, 3] try: my_tuple[1] = 3 except TypeError: print(\"cannot modify a tuple\") convenient way to return multiple values and multiple assignment def sum_and_product(x, y): return (x + y), (x * y) sp = sum_and_product(2, 3) x, y = 1, 2 # now x is 1, y is 2 x, y = y, x # Pythonic way to swap variables; now x is 2, y is 1 Dict # Literal assignment grades = {\"Joel\": 80, \"Tim\": 95} # dictionary literal # Access non-existent key will throw KeyError exception grade[\"Kate\"] # Better way; # defaults to None grades.get(\"Kate\") # or grades.get(\"Kate\", 0) DefaultDict Handling non-existent keys is cumbersome; defaultdict can help for various types, such as int, list. from collections import defaultdict word_counts = defaultdict(int) # int() produces 0 for word in document: word_counts[word] += 1 dd_list = defaultdict(list) # list() produces an empty list dd_list[2].append(1) # now dd_list contains {2: [1]} dd_dict = defaultdict(dict) # dict() produces an empty dict dd_dict[\"Joel\"][\"City\"] = \"Seattle\" # {\"Joel\" : {\"City\": Seattle\"}} dd_pair = defaultdict(lambda: [0, 0]) dd_pair[2][1] = 1 # now dd_pair contains {2: [0, 1]} Counters Turns a sequence of values into a defauldict(int) from collections import Counter c = Counter([0, 1, 2, 0]) # c is (basically) {0: 2, 1: 1, 2:1} # recall, document is a list of words word_counts = Counter(document) word_counts.most_common(10) # returns top 10 common word Set if you want fast lookup time, use a set and not a tuple primes_below_10 = {2,3,5,7} Truthiness # None indicates nonexistent value x = {}.get(\"Something\") assert x is None # more pythonic than x == None # Empty containers and values are Falsy # [], {}, set, 0, Sorting a List 2 methods sort method sorts in place sorted returns a new list x = [4, 1, 2, 3] y = sorted(x) # y is [1, 2, 3, 4], x is unchanged x.sort() # now x is [1, 2, 3, 4] By default, sort in ascending order. To reverse, we have two options # sort the list by absolute value from largest to smallest x = sorted([-4, 1, -2, 3], key=abs, reverse=True) # is [-4, 3, -2, 1] # sort the words and counts from highest count to lowest wc = sorted(word_counts.items(), key=lambda word_and_count: word_and_count[1], reverse=True) Generators Generators are useful when you need to access 1 element at a time; unlike lists where all the elements are stored in memory def generate_range(n): i = 0 while i \u003c n: yield i # every call to yield produces a value of the generator i += 1 for i in generate_range(10): print(f\"i: {i}\") Iterable signifies it’s lazy evens: Iterable[int] = (x for x in range(10) if x % 2 == 0) Regular Expressions Fairly complex and powerful module for expression matching doc re.match checks whether the beginning of a string matches a regular expression re.search checks whether any part of a string matches a regular expression. Example import re re_examples = [ # All of these are True, because not re.match(\"a\", \"cat\"), # 'cat' doesn't start with 'a' re.search(\"a\", \"cat\"), # 'cat' has an 'a' in it not re.search(\"c\", \"dog\"), # 'dog' doesn't have a 'c' in it. 3 == len(re.split(\"[ab]\", \"carbs\")), # Split on a or b to ['c','r','s']. \"R-D-\" == re.sub(\"[0-9]\", \"-\", \"R2D2\") # Replace digits with dashes. ] assert all(re_examples), \"all the regex examples should be True\" args and kwargs args and kwargs enables one to specify a function that takes arbitrary arguments args is a tuple of its unnamed arguments kwargs is a dict of its named arguments Simple example def magic(*args, **kwargs): print(\"unnamed args:\", args) print(\"keyword args:\", kwargs) magic(1, 2, key=\"word\", key2=\"word2\") # prints # unnamed args: (1, 2) # keyword args: {'key': 'word', 'key2': 'word2'} More complex function with high order function (function that takes a function as its input) def doubler_correct(f): \"\"\"works no matter what kind of inputs f expects\"\"\" def g(*args, **kwargs): \"\"\"whatever arguments g is supplied, pass them through to f\"\"\" return 2 * f(*args, **kwargs) return g g = doubler_correct(f2) assert g(1, 2) == 6, \"doubler should work now\" Type Annotation Python is dynamically typed; types operations are loosely enforced as long as it runs def add(a, b): return a + b assert add(10, 5) == 15, \"+ is valid for numbers\" assert add([1, 2], [3]) == [1, 2, 3], \"+ is valid for lists\" assert add(\"hi \", \"there\") == \"hi there\", \"+ is valid for strings\" try: add(10, \"five\") except TypeError: print(\"cannot add an int to a string\") While python does not enforce strict type, type annotation in python has these benefits types are a form of documentation External tools (ie mypy) leverages types autocompletion Thinking about types may help/highlight need for simpler design # Hmm.. is operation too wide? we are allowing so many different operators def ugly_function(value: int, operation: Union[str, int, float, bool]) -\u003e int: Some Types in typing module Dict, Iterable, Tuple from typing import Dict, Iterable, Tuple # keys are strings, values are ints counts: Dict[str, int] = {'data': 1, 'science': 2} # lists and generators are both iterable if lazy: evens: Iterable[int] = (x for x in range(10) if x % 2 == 0) else: evens = [0, 2, 4, 6, 8] # tuples specify a type for each element triple: Tuple[int, float, int] = (10, 2.3, 5) Optional from typing import Optional best_so_far: Optional[float] = None # allowed to be either a float or None Callable is a type annotation for function from typing import Callable # The type hint says that repeater is a function that takes # two arguments, a string and an int, and returns a string. def twice(repeater: Callable[[str, int], str], s: str) -\u003e str: return repeater(s, 2) def comma_repeater(s: str, n: int) -\u003e str: n_copies = [s for _ in range(n)] return ', '.join(n_copies) assert twice(comma_repeater, \"type hints\") == \"type hints, type hints\" 3 Visualizing Data 3.1 matplotlib matplotlib python module is good for basic visualizations (bar, line, scatter). For more interactive alternatives, see section 5. from matplotlib import pyplot as plt years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # create a line chart, years on x-axis, gdp on y-axis plt.plot(years, gdp, color='green', marker='o', linestyle='solid') # add a title plt.title(\"Nominal GDP\") # add a label to the y-axis plt.ylabel(\"Billions of $\") plt.show() 3.2 Bar Charts Basic Example movies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\", \"West Side Story\"] num_oscars = [5, 11, 3, 8, 10] # plot bars with left x-coordinates [0, 1, 2, 3, 4], heights [num_oscars] plt.bar(range(len(movies)), num_oscars) plt.title(\"My Favorite Movies\") # add a title plt.ylabel(\"# of Academy Awards\") # label the y-axis # label x-axis with movie names at bar centers plt.xticks(range(len(movies)), movies) plt.show() _Histograms: Buckets _ from collections import Counter grades = [83, 95, 91, 87, 70, 0, 85, 82, 100, 67, 73, 77, 0] # Bucket grades by decile, but put 100 in with the 90s histogram = Counter(min(grade // 10 * 10, 90) for grade in grades) plt.bar([x + 5 for x in histogram.keys()], # Shift bars right by 5 histogram.values(), # Give each bar its correct height 10, # Give each bar a width of 10 edgecolor=(0, 0, 0)) # Black edges for each bar plt.axis([-5, 105, 0, 5]) # x-axis from -5 to 105, # y-axis from 0 to 5 plt.xticks([10 * i for i in range(11)]) # x-axis labels at 0, 10, ..., 100 plt.xlabel(\"Decile\") plt.ylabel(\"# of Students\") plt.title(\"Distribution of Exam 1 Grades\") plt.show() 3.3 Line Charts Good for showing trends Example: Bias vs Variance Tradeoff x-axis: model complexity # Each index in the 3 lists below corresponds to a model complexity variance = [1, 2, 4, 8, 16, 32, 64, 128, 256] bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1] total_error = [x + y for x, y in zip(variance, bias_squared)] xs = [i for i, _ in enumerate(variance)] # We can make multiple calls to plt.plot # to show multiple series on the same chart plt.plot(xs, variance, 'g-', label='variance') # green solid line plt.plot(xs, bias_squared, 'r-.', label='bias^2') # red dot-dashed line plt.plot(xs, total_error, 'b:', label='total error') # blue dotted line # Because we've assigned labels to each series, # we can get a legend for free (loc=9 means \"top center\") plt.legend(loc=9) plt.xlabel(\"model complexity\") plt.xticks([]) plt.title(\"The Bias-Variance Tradeoff\") plt.show() 3.4 Scatter Plots Good to visualizing ==relationship== between 2 sets of data friends = [ 70, 65, 72, 63, 71, 64, 60, 64, 67] minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190] labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'] plt.scatter(friends, minutes) # label each point for label, friend_count, minute_count in zip(labels, friends, minutes): plt.annotate(label, xy=(friend_count, minute_count), # Put the label with its point xytext=(5, -5), # but slightly offset textcoords='offset points') plt.title(\"Daily Minutes vs. Number of Friends\") plt.xlabel(\"# of friends\") plt.ylabel(\"daily minutes spent on the site\") plt.show() 3.5 Further Exploration The matplotlib Gallery will give you a good idea of the sorts of things you can do with matplotlib (and how to do them). seaborn is built on top of matplotlib and allows you to easily produce prettier (and more complex) visualizations. Altair is a newer Python library for creating declarative visualizations. D3.js is a JavaScript library for producing sophisticated interactive visualizations for the web. Although it is not in Python, it is widely used, and it is well worth your while to be familiar with it. Bokeh is a library that brings D3-style visualizations into Python. 4 Linear Algebra 4.1 Vectors Vector are objects that can be added with other vectors multiplied by a scalar to form another vector Defining our Vector type alias from typing import List Vector = List[float] Common vector operators # Add 2 vectors def add(v: Vector, w: Vector) -\u003e Vector: \"\"\"Adds corresponding elements\"\"\" assert len(v) == len(w), \"vectors must be the same length\" return [v_i + w_i for v_i, w_i in zip(v, w)]\tassert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9] # Scale a vector by a scalar def scalar_multiply(scalar: float, v: Vector) -\u003e Vector: return [scalar * v_i for v_i in v] assert scalar_multiply(2, [1, 2, 3]) == [2, 4, 6] # Dot product measures the similarity of 2 vectors based on the direction they are pointing to. # dot_exact(a,b) = |a| [b] cos(theta) # dot_approx = sum(a_i * b_i + a_j * b_j ...) # WOW: don't need to know the angle theta def dot(v: Vector, w: Vector) -\u003e float: return sum(v_i * w_i for v_i, w_i in zip(v, v)) assert dot([1, 2, 3], [4, 5, 6]) == 32 # sum of sqaures of 1 vector can be defined in terms of dot function def sum_of_squares(v: Vector) -\u003e float: return dot(v, v) assert sum_of_squares([1, 2, 3]) == 14 # mangnitude of a vector can be defined using sum_of_squares # aka L2 Norm def magnitude(v: Vector) -\u003e float: return math.sqrt(sum_of_sqaures(v)) assert magnitude([3, 4]) == 5 Distance between two vectors (Euclidean) $$ \\sqrt{(v_1-w_1)^2 + .. + (v_n-w_n)^2}$$ def squared_distance(v: Vector, w: Vector) -\u003e float: \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\" return sum_of_squares(subtract(v, w)) def distance(v: Vector, w: Vector) -\u003e float: \"\"\"Computes the distance between v and w\"\"\" return math.sqrt(squared_distance(v, w)) other distance measurements blog L2 norm calculates the distance of ==1== vector from the origin Manhattan distance - shortest path between 2 points on the grid def manhattan_distance(a, b): return sum(abs(e1-e2) for e1, e2 in zip(a,b)) Minkowski distance: generalization of euclidean and manhattan 4.2 Matrices In practice, use numpy for better performance. But we will implement our own for better understanding Type aliases # Another type alias Matrix = List[List[float]] A = [[1, 2, 3], # A has 2 rows and 3 columns [4, 5, 6]] B = [[1, 2], # B has 3 rows and 2 columns [3, 4], [5, 6]] Matrix accessor from typing import Tuple def shape(A: Matrix) -\u003e Tuple[int, int]: \"\"\"Returns (# of rows of A, # of columns of A)\"\"\" num_rows = len(A) num_cols = len(A[0]) if A else 0 # number of elements in first row return num_rows, num_cols def get_row(A: Matrix, i: int) -\u003e Vector: \"\"\"Returns the i-th row of A (as a Vector)\"\"\" return A[i] # A[i] is already the ith row def get_column(A: Matrix, j: int) -\u003e Vector: \"\"\"Returns the j-th column of A (as a Vector)\"\"\" return [A_i[j] # jth element of row A_i for A_i in A] # for each row A_i Matrix creation from typing import Callable def make_matrix(num_rows: int, num_cols: int, entry_fn: Callable[[int, int], float]) -\u003e Matrix: \"\"\" Returns a num_rows x num_cols matrix whose (i,j)-th entry is entry_fn(i, j) \"\"\" return [[entry_fn(i, j) # given i, create a list for j in range(num_cols)] # [entry_fn(i, 0), ... ] for i in range(num_rows)] # create one list for each i def identity_matrix(n: int) -\u003e Matrix: \"\"\"Returns the n x n identity matrix\"\"\" return make_matrix(n, n, lambda i, j: 1 if i == j else 0) 4.3 Further Exploration 5 Statistics 5.1 Describing a Single Set of Data Central Tendencies A common metric is to quantify where our data is centered (mean and median) def mean(xs: List[float]) -\u003e float: return sum(xs)/len(xs) # median # if odd length --\u003e mid element; if even length --\u003e avg of the middel two elements def _median_odd(xs: List[float]) -\u003e float: mid_index = len(xs)//2 return sorted(xs)[mid_index] def _median_even(xs: List[float]) -\u003e float: sorted_xs = sorted(xs) high_index = len(xs)//2 return (sorted_xs[high_index-1] + sorted_xs[high_index]) /2 def median(xs: List[float]) -\u003e float: return _median_even(xs) if len(xs)%2 == 0 else _median_odd(xs) Percentile: generalization of median def quantile(xs: List[float], percentile: float) -\u003e float: index = int(percentile * len(xs)) return sorted(xs)[p_index] Mode : most common values def mode(xs: List[float]) -\u003e List[float]: counts = Counter(x) max_count = max(counts.values()) return [ x_i for x_i, count in counts.items() if count == max_count ] Dispersion Dispersion measures how spread out our data is # CON: does not use the entire data set def data_range(xs: List[float]) -\u003e float: return max(xs) - min(xs) def variance(xs: List[float]) -\u003e float: xs_mean = mean(xs) deviations = [x - xs_mean for x in xs] return dot(deviations, deviations) / (len(xs) - 1) def standard_deviation(xs: List[float]) -\u003e float: return math.sqrt(variance(xs)) 5.2 Correlation Correlation measures the relationship between two arrays Covariance Covariance vs variance Variance measures how variable deviate from the mean Covariance measures how 2 variables vary in tandem from their respective means def covariance(xs: List[float], ys: List[Float]) -\u003e float: \"\"\" +1 -\u003e pos relationship; 0 no relationship \"\"\" assert len(xs) == len(ys), \"xs and ys must be same length\" def mean_diff(ls: List[float]) -\u003e List[float]: mean_ls = sum(ls)/len(ls) return [i-mean_ls for i in ls] return dot(mean_diff(xs), mean_diff(ys)) / (len(xs) -1) Correlation Covariance is hard to interpret because each input is not normalized to standard deviation of each inputs; def correlation(xs: List[float], ys: List[float]) -\u003e float: \"\"\"Measures how much xs and ys vary in tandem about their means\"\"\" stdev_x = standard_deviation(xs) stdev_y = standard_deviation(ys) if stdev_x \u003e 0 and stdev_y \u003e 0: return covariance(xs, ys) / stdev_x / stdev_y else: return 0 # if no variation, correlation is zero 5.3 Simpson’s Paradox Correlation may be misleading Correlation assume ==all else being equal== But factors like how we sample/construct the dataset can affect the correlation result significantly 5.4 Other Correlation Caveats 5.5 Correlation and Causation Correlation is not causation 5.6 Further Exploration 6 Probability 6.1 Dependence and Independence 2 events E and F are dependent if knowing something about E gives us information whether F happens; otherwise they are independent. If events E and F are independent, then probability of both event E and F happening $$ P(E,F) = P(E) P(F)$$ Examples probability of rolling 2 dices is 1/2 * 1/2 6.2 Conditional Probability If two events are dependent $$P(E | F) = P(E,F)/P(F)$$ 6.3 Baye’s Theorem Think of it as a way to reverse conditional probabilities $$P(E | F) = P(E,F)/P(F)$$ $$P(E|F) = F(F|E) P(E) / P(F)$$ 6.4 Random Variables Random variable have values which has a probability distribution. Expected value: the average of a random’s variable weighted by its probability Ex: Coin toss where head=0 and tail=1 $$Expected Value = value_{head} * prob_{head} + value_{tail} * prob_{tail} $$ $$Expected Value = 0 * 0.5 + 1 * 0.5 $$ 6.5 Continuous Distribution Random variables value can follow a discrete distribution: dices rolls: {1,2,3,4,5,6} continuous distribution - values falls within a continuous spectrum calculus integration: if value follows a density function f then probability of seeing a value between x and x+h is h* f(x) for small h Cumulative distribution function (CDF) gives probability variable is less than or equal to a value 6.6 Normal Distribution Normal distribution is a particular continuous distribution described by its mean $\\mu$ and standard deviation $\\sigma$ $$f(x|\\sigma, \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma}}exp(-\\frac{(x-\\mu^2)}{2\\sigma^2})$$ import math SQRT_TWO_PI = math.sqrt(2 * math.pi) def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -\u003e float: return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma)) Probability Distribution Function PDF import matplotlib.pyplot as plt xs = [x / 10.0 for x in range(-50, 50)] plt.plot(xs,[normal_pdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1') plt.plot(xs,[normal_pdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2') plt.plot(xs,[normal_pdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5') plt.plot(xs,[normal_pdf(x,mu=-1) for x in xs],'-.',label='mu=-1,sigma=1') plt.legend() plt.title(\"Various Normal pdfs\") plt.show() CDFs for Normal Distribution using the erf function def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -\u003e float: return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2 # PLOTS for normal CDF for various values for mean and deviation xs = [x / 10.0 for x in range(-50, 50)] plt.plot(xs,[normal_cdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1') plt.plot(xs,[normal_cdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2') plt.plot(xs,[normal_cdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5') plt.plot(xs,[normal_cdf(x,mu=-1) for x in xs],'-.',label='mu=-1,sigma=1') plt.legend(loc=4) # bottom right plt.title(\"Various Normal cdfs\") plt.show() Use CDF to find value ($\\mu \\sigma$?) corresponding to a certain probability def inverse_normal_cdf(p: float, mu: float = 0, sigma: float = 1, tolerance: float = 0.00001) -\u003e float: \"\"\"Find approximate inverse using binary search\"\"\" # if not standard, compute standard and rescale if mu != 0 or sigma != 1: return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance) low_z = -10.0 # normal_cdf(-10) is (very close to) 0 hi_z = 10.0 # normal_cdf(10) is (very close to) 1 while hi_z - low_z \u003e tolerance: mid_z = (low_z + hi_z) / 2 # Consider the midpoint mid_p = normal_cdf(mid_z) # and the CDF's value there if mid_p \u003c p: low_z = mid_z # Midpoint too low, search above it else: hi_z = mid_z # Midpoint too high, search below it return mid_z 6.7 Central Limit Theorem a random variable defined as the average of a ==large number of independent and identically distributed random variables== is itself approximately normally distributed. Ex: Bernouli distribution with large enough sample size follows a normal distribution Bernouli: mean of Bernouli is p standard deviation is $\\sqrt{p(1-p)}$ Large sample –\u003e follows normal distribution mean = np standard deviation = $\\sqrt{np(1-p)}$ def bernoulli_trial(p: float) -\u003e int: \"\"\"Returns 1 with probability p and 0 with probability 1-p\"\"\" return 1 if random.random() \u003c p else 0 def binomial(n: int, p: float) -\u003e int: \"\"\"Returns the sum of n bernoulli(p) trials\"\"\" return sum(bernoulli_trial(p) for _ in range(n)) from collections import Counter def binomial_histogram(p: float, n: int, num_points: int) -\u003e None: \"\"\"Picks points from a Binomial(n, p) and plots their histogram\"\"\" data = [binomial(n, p) for _ in range(num_points)] # use a bar chart to show the actual binomial samples histogram = Counter(data) plt.bar([x - 0.4 for x in histogram.keys()], [v / num_points for v in histogram.values()], 0.8, color='0.75') mu = p * n sigma = math.sqrt(n * p * (1 - p)) # use a line chart to show the normal approximation xs = range(min(data), max(data) + 1) ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma) for i in xs] plt.plot(xs,ys) plt.title(\"Binomial Distribution vs. Normal Approximation\") plt.show() 6.8 Further Exploration 7 Hypothesis and Inference 7.1 Statistical Hypothesis Testing Terminologies $H_0$ : aka null hypothesis, represent the default position Type 1 error (false positive) : wrongly reject $H_0$ even though it is true Significance : how willing we are to make a type 1 error (usually 5% or 1%) Type 2 error (false negative) : fail to reject $H_0$ even though it’s false power of test: $H_1$ : alternative hypothesis We use statistics to decide whether we can reject $H_0$ The different approaches below differ on the what the probability is on, ie parameters like normal distribution $\\mu, \\sigma$, value, etc.. Approach 1: Choose bounds based on some probability cutoff using the CDF See 7.2 below 7.2 Example: Flipping a Coin Overview Our default hypothesis: probability of head is 0.5 What is the range of head counts we need to observe to satisfy our requirements on significance: type 1 error power: type 2 error? Model probability of head in coin toss as Bernoulli Central limit theorem –\u003e Bernoulli distribution becomes normal distribution Use CDF to calculate various count_head needs to be observed Null hypothesis : probability of head is 0.5 (p_head = 0.5) Model random variable’s value as binomial(n=#numSamples, p=probability_head) If large enough sample, binomial distribution can be approximated as normal distribution, and characterized by $\\mu, \\sigma$ from typing import Tuple import math def normal_approximation_to_binomial(n: int, p: float) -\u003e Tuple[float, float]: \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\" mu = p * n sigma = math.sqrt(p * (1 - p) * n) return mu, sigma Since we assume random variable follows normal distribution, we can use norml_cdf to quantify the probability that the value (p_head or ($\\mu, \\sigma)$?) lies within or outside a particular interval from scratch.probability import normal_cdf # The normal cdf _is_ the probability the variable is below a threshold normal_probability_below = normal_cdf # It's above the threshold if it's not below the threshold def normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\"The probability that an N(mu, sigma) is greater than lo.\"\"\" return 1 - normal_cdf(lo, mu, sigma) # It's between if it's less than hi, but not less than lo def normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\"The probability that an N(mu, sigma) is between lo and hi.\"\"\" return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma) # It's outside if it's not between def normal_probability_outside(lo: float, hi: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\"The probability that an N(mu, sigma) is not between lo and hi.\"\"\" return 1 - normal_probability_between(lo, hi, mu, sigma) Similarly, we can find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood Ex: we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%) from scratch.probability import inverse_normal_cdf def normal_upper_bound(probability: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\"Returns the z for which P(Z \u003c= z) = probability\"\"\" return inverse_normal_cdf(probability, mu, sigma) def normal_lower_bound(probability: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\"Returns the z for which P(Z \u003e= z) = probability\"\"\" return inverse_normal_cdf(1 - probability, mu, sigma) def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -\u003e Tuple[float, float]: \"\"\" Returns the symmetric (about the mean) bounds that contain the specified probability \"\"\" tail_probability = (1 - probability) / 2 # upper bound should have tail_probability above it upper_bound = normal_lower_bound(tail_probability, mu, sigma) # lower bound should have tail_probability below it lower_bound = normal_upper_bound(tail_probability, mu, sigma) return lower_bound, upper_bound If our hypothesis is correct, then $\\mu=500, $\\sigma=15.8$ mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5) Signficance: What’s our appetite for false positive (Type 1 error)? Let’s choose significance (ie willingness to make a false positive) to be 5% –\u003e p=0.95 # (469, 531) lower_bound, upper_bound = normal_two_sided_bounds(0.95, mu_0, sigma_0) In order for us to reject H0 with 5% chance of false positive, we need to see between 469 and 531 heads Power of Test : What’s our appetite for false negative (Type 2 error) this quantifies our probability of a type 2 error # 95% bounds based on assumption p is 0.5 #(469, 531) lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0) # actual mu and sigma based on p = 0.55 mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55) # a type 2 error means we fail to reject the null hypothesis, # which will happen when X is still in our original interval type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1) power = 1 - type_2_probability # 0.887 ``` - - We reject H_0 when we see ### 7.3 Approach 2: p-Values - We compute the probability, assuming $H_0$ is true, that we see a value as extreme as the one we actually observed. ```python def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -\u003e float: \"\"\" How likely are we to see a value at least as extreme as x (in either direction) if our values are from an N(mu, sigma)? \"\"\" if x \u003e= mu: # x is greater than the mean, so the tail is everything greater than x return 2 * normal_probability_above(x, mu, sigma) else: # x is less than the mean, so the tail is everything less than x return 2 * normal_probability_below(x, mu, sigma) If we were to see 530 heads, our p value is 0.062; so our $H_0$ hypothesis is valid 7.4 Approach 3: Confidence Intervals Quantify the confidence interval around the observed value of the parameter, ie probability of head in a coin toss. Ex: If we observe 525 heads out of 1000 flips, –\u003e p_head = 0.525. What is our confidence level that p_head is actually 0.525? # We use our observation, and assign p_hat = 0.525 p_hat = 0.525 sigma = math.sqrt(p_hat * (1-p_hat) #0.0158 # probability threadhold where we are 95% confident is the true p p_lower, p_higher = normal_two_sided_bounds(0.95, mu, sigma) # [0.4940, 0.5560] # Since H_0 has p=0.5, then H_0 falls within our 95% confidence interval 7.5 p-Hacking 7.6 Example: Running on A/B Test Given 2 Ads A and B where $n_A$ = number of users we clicked on Ad A $N_A$= total number of ad A shown $n_B$ = number of users we clicked on Ad B $N_B$= total number of ad B shown Our null hypothesis $H_0$ is the original ad A is better We want to know if we should roll out A or B Click is a Bernouli distribution, but with enough sample, it follows a normal distribution which can be characterized by $\\mu , \\sigma$ def estimated_parameters(N: int, n: int) -\u003e Tuple[float, float]: p = n / N sigma = math.sqrt(p * (1 - p) / N) return p, sigma ``` - Calculate the difference of $\\mu , \\sigma$ - The 2 normal distribution A and B are independent - $p_{diff} = p_a - p_b$ - $\\sigma_{diff} = \\sqrt{\\sigma_a^2 + \\sigma_b^2}$ - Our Null hypothesis is $p_A$ and $p_B$ are the same ```python # returns the z value def a_b_test_statistic(N_A: int, n_A: int, N_B: int, n_B: int) -\u003e float: p_A, sigma_A = estimated_parameters(N_A, n_A) p_B, sigma_B = estimated_parameters(N_B, n_B) return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2) Ex 1: Insufficient to disprove null hypothesis z = a_b_test_statistic(1000, 200, 1000, 180) # -1.14 two_sided_p_value(z) # 0.254 Ex 2: Sufficient to disprove null hypothesis z = a_b_test_statistic(1000, 200, 1000, 150) # -2.94 two_sided_p_value(z) # 0.003 7.7 Approach 4: Bayesian Inference Rather than making probability judgments about the tests, you make probability judgments about the parameters. Assume a prior distribution; collect data ; update the posterior 7.8 Further Exploration 8 Gradient Descent In ML, training a model often means finding the best parameters. minimization problem: minimize the error of its prediction maximization problem: maximize the likelihood of the data Gradient gives the input direction which the function increases most quickly maximization -\u003e gradient minimization -\u003e - gradient 8.1 Estimating Gradient Defining Scalar Gradient from typing import Callable # returns gradient def gradient_scalar(f: Callable[[float], float], x: float, h: float) -\u003e float: return (f(x + h) - f(x)) / h # Funtion we are trying to calculate the derivative def square(x: float) -\u003e float: return x * x # Test xs = range(-10, 11) estimates = [gradient_scalar(square, x, h=0.001) for x in xs] # plot to show they're basically the same --\u003e shows a line with a slope of 2 import matplotlib.pyplot as plt plt.title(\"Actual Derivatives vs. Estimates\") plt.plot(xs, estimates, 'b+', label='Estimate') # blue + plt.legend(loc=9) plt.show() When the function f has many variables (ie input is a vector), each element has a gradient; it is a partial derivative def gradient_vector(f: Callable[[Vector], float], v: Vector, i: int, h: float) -\u003e float: \"\"\"Returns the i-th partial difference quotient of f at v\"\"\" w = [v_j + (h if j == i else 0) # add h to just the ith element of v for j, v_j in enumerate(v)] return (f(w) - f(v)) / h def estimate_gradient(f: Callable[[Vector], float], v: Vector, h: float = 0.0001): return [gradient_vector(f, v, i, h) for i in range(len(v))] Using the Gradient import random from scratch.linear_algebra import distance, add, scalar_multiply def gradient_step(v: Vector, gradient: Vector, step_size: float) -\u003e Vector: \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\" assert len(v) == len(gradient) step = scalar_multiply(step_size, gradient) return add(v, step) # pick a random starting point v = [random.uniform(-10, 10) for i in range(3)] # let f be the function we are trying to optimize for epoch in range(1000): # compute the gradient at v grad = estimate_gradient(lambda x: x * x , v, step_size=0.001) v = gradient_step(v, grad, -0.01) # take a negative gradient step print(epoch, v) Choosing the Right Step Size Some options are: Use a fixed step size Gradually shrink the step size over time At each step, choose the step size that minimizes the value of the objective function Using Gradient Descent to Fit Models Let’s assume we have a linear model where we are trying to learn the parameter theta, (ie slope and intercept) # Method clculates the error between prediction and actual def linear_gradient(x: float, y: float, theta: Vector) -\u003e Vector: slope, intercept = theta. # theta is the parameter vector we are learning predicted = slope * x + intercept # The prediction of the model. error = (predicted - y) # error is (predicted - actual). squared_error = error ** 2 # We'll minimize squared error grad = [2 * error * x, 2 * error] # using its gradient. return grad Code steps through Start with a random value for theta. Compute the mean of the gradients. Adjust theta in that direction. Repeat. from scratch.linear_algebra import vector_mean # Start with random values for slope and intercept theta = [random.uniform(-1, 1), random.uniform(-1, 1)] learning_rate = 0.001 for epoch in range(5000): # Compute the mean of the gradients grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs]) # Take a step in that direction theta = gradient_step(theta, grad, -learning_rate) print(epoch, theta) Mini-batch and Stochastic Gradient Descent mini-batch gradient descent: calculate gradient descent using only a subset of larger dataset: stochastic gradient descent: calculate gradient descent using only ==ONE== sample: 9 Getting Data 9.1 stdin and stdout We can use unix pipe with python files to create interesting functionality pipes takes the output from the left cmd as input to the next output command Example 1: Pipe with greps cat SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py # egrep.py import sys, re # sys.argv is the list of command-line arguments # sys.argv[0] is the name of the program itself # sys.argv[1] will be the regex specified at the command line regex = sys.argv[1] # for every line passed into the script for line in sys.stdin: # if it matches the regex, write it to stdout if re.search(regex, line): sys.stdout.write(line) # line_cout.py import sys count = 0 for line in sys.stdin: count += 1 # print goes to sys.stdout print(count) Ex 2: Most common word of files cat the_bible.txt `|` python most_common_words.py 10 # most_common_words.py import sys from collections import Counter # pass in number of words as first argument try: num_words = int(sys.argv[1]) except: print(\"usage: most_common_words.py num_words\") sys.exit(1) # nonzero exit code indicates error counter = Counter(word.lower() # lowercase words for line in sys.stdin for word in line.strip().split() # split on spaces if word) # skip empty 'words' for word, count in counter.most_common(num_words): sys.stdout.write(str(count)) sys.stdout.write(\"\\t\") sys.stdout.write(word) sys.stdout.write(\"\\n\") ``` ### 9.2 Reading Files - Basic: Read all the domains of a file ```python def get_domain(email_address: str) -\u003e str: \"\"\"Split on '@' and return the last piece\"\"\" return email_address.lower().split(\"@\")[-1] # a couple of tests assert get_domain('joelgrus@gmail.com') == 'gmail.com' assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com' from collections import Counter with open('email_addresses.txt', 'r') as f: domain_counts = Counter(get_domain(line.strip()) for line in f if \"@\" in line) ``` ### 9.3 Scraping the Web - Tools - Use Beautiful Soup Library to build a tree out of the web page - Use Request library to make HTTP request - Parser: html5lib - python -m pip install beautifulsoup4 requests html5lib - Ex: Scrape house of representative _[web](https://www.house.gov/representatives)_ who mentioned data in their press link _HTML: view source_ ```html \u003ctd\u003e \u003ca href=\"https://jayapal.house.gov\"\u003eJayapal, Pramila\u003c/a\u003e \u003c/td\u003e ``` _Code 1: Find the Valid URLS of representative_ ```python from bs4 import BeautifulSoup import requests url = \"https://www.house.gov/representatives\" text = requests.get(url).text soup = BeautifulSoup(text, \"html5lib\") # Must start with http:// or https:// # Must end with .house.gov or .house.gov/ regex = r\"^https?://.*\\.house\\.gov/?$\" # Let's write some tests! assert re.match(regex, \"http://joel.house.gov\") assert re.match(regex, \"https://joel.house.gov\") all_urls = [a['href'] for a in soup('a') if a.has_attr('href')] # And now apply good_urls = [url for url in all_urls if re.match(regex, url)] print(len(set(all_urls))) # 965 for me, way too many Code 2: Find the Press Link Test code: each press link is a relative link, which means we need to remember the originating site html = requests.get('https://jayapal.house.gov').text soup = BeautifulSoup(html, 'html5lib') # Use a set because the links might appear multiple times. links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()} print(links) # {'/media/press-releases'} Actual Code: dict of representative to links from typing import Dict, Set press_releases: Dict[str, Set[str]] = {} for house_url in good_urls: html = requests.get(house_url).text soup = BeautifulSoup(html, 'html5lib') pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()} print(f\"{house_url}: {pr_links}\") press_releases[house_url] = pr_links Code 3: Did press link mention data? for house_url, pr_links in press_releases.items(): for pr_link in pr_links: url = f\"{house_url}/{pr_link}\" text = requests.get(url).text if paragraph_mentions(text, 'data'): print(f\"{house_url}\") break # done with this house_url 9.4 Using the APIs 9.5 Using Twitter APIs Logic Go to https://developer.twitter.com/. If you are not signed in, click “Sign in” and enter your Twitter username and password. Click Apply to apply for a developer account. Request access for your own personal use. Fill out the application. It requires 300 words (really) on why you need access, so to get over the limit you could tell them about this book and how much you’re enjoying it. Wait some indefinite amount of time. If you know someone who works at Twitter, email them and ask them if they can expedite your application. Otherwise, keep waiting. Once you get approved, go back to developer.twitter.com, find the “Apps” section, and click “Create an app.” Fill out all the required fields (again, if you need extra characters for the description, you could talk about this book and how edifying you’re finding it). Click CREATE. _Code1 _: Create twython Instance import os # Feel free to plug your key and secret in directly CONSUMER_KEY = os.environ.get(\"TWITTER_CONSUMER_KEY\") CONSUMER_SECRET = os.environ.get(\"TWITTER_CONSUMER_SECRET\") import webbrowser from twython import Twython # Get a temporary client to retrieve an authentication URL temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET) temp_creds = temp_client.get_authentication_tokens() url = temp_creds['auth_url'] # Now visit that URL to authorize the application and get a PIN print(f\"go visit {url} and get the PIN code and paste it below\") webbrowser.open(url) PIN_CODE = input(\"please enter the PIN code: \") # Now we use that PIN_CODE to get the actual tokens auth_client = Twython(CONSUMER_KEY, CONSUMER_SECRET, temp_creds['oauth_token'], temp_creds['oauth_token_secret']) final_step = auth_client.get_authorized_tokens(PIN_CODE) ACCESS_TOKEN = final_step['oauth_token'] ACCESS_TOKEN_SECRET = final_step['oauth_token_secret'] # And get a new Twython instance using them. twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET) _Code2a _: Search whether tweet contains the word data # Search for tweets containing the phrase \"data science\" for status in twitter.search(q='\"data science\"')[\"statuses\"]: user = status[\"user\"][\"screen_name\"] text = status[\"text\"] print(f\"{user}: {text}\\n\") Code2b: Using Streaming API to collect all data continuously define on_success and on_error from twython import TwythonStreamer # Appending data to a global variable is pretty poor form # but it makes the example much simpler tweets = [] class MyStreamer(TwythonStreamer): def on_success(self, data): \"\"\" What do we do when Twitter sends us data? Here data will be a Python dict representing a tweet. \"\"\" # We only want to collect English-language tweets if data.get('lang') == 'en': tweets.append(data) print(f\"received tweet #{len(tweets)}\") # Stop when we've collected enough if len(tweets) \u003e= 100: self.disconnect() def on_error(self, status_code, data): print(status_code, data) self.disconnect() stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET) # starts consuming public statuses that contain the keyword 'data' stream.statuses.filter(track='data') # if instead we wanted to start consuming a sample of *all* public statuses # stream.statuses.sample() top_hashtags = Counter(hashtag['text'].lower() for tweet in tweets for hashtag in tweet[\"entities\"][\"hashtags\"]) print(top_hashtags.most_common(5)) 10 Working With Data 10.1 Exploring Your data One Dimension: just one axis histogram a distribution from typing import List, Dict from collections import Counter import math import matplotlib.pyplot as plt def bucketize(point: float, bucket_size: float) -\u003e float: \"\"\"Floor the point to the next lower multiple of bucket_size\"\"\" return bucket_size * math.floor(point / bucket_size) def make_histogram(points: List[float], bucket_size: float) -\u003e Dict[float, int]: \"\"\"Buckets the points and counts how many in each bucket\"\"\" return Counter(bucketize(point, bucket_size) for point in points) def plot_histogram(points: List[float], bucket_size: float, title: str = \"\"): histogram = make_histogram(points, bucket_size) plt.bar(histogram.keys(), histogram.values(), width=bucket_size) plt.title(title) # normal distribution with mean 0, standard deviation 57 import random # uniform between -100 and 100 uniform = [200 * random.random() - 100 for _ in range(10000)] plot_histogram(uniform, 10, \"Uniform Histogram\") Two Dimensions: Compare 2 Variables Via Scatter Plot # Define 2 axis input def random_normal() -\u003e float: \"\"\"Returns a random draw from a standard normal distribution\"\"\" return inverse_normal_cdf(random.random()) xs = [random_normal() for _ in range(1000)] ys1 = [ x + random_normal() / 2 for x in xs] ys2 = [-x + random_normal() / 2 for x in xs] plt.scatter(xs, ys1, marker='.', color='black', label='ys1') plt.scatter(xs, ys2, marker='.', color='gray', label='ys2') plt.xlabel('xs') plt.ylabel('ys') plt.legend(loc=9) plt.title(\"Very Different Joint Distributions\") plt.show() Many Dimensions use correlation matrix from scratch.linear_algebra import Matrix, Vector, make_matrix def correlation_matrix(data: List[Vector]) -\u003e Matrix: \"\"\" Returns the len(data) x len(data) matrix whose (i, j)-th entry is the correlation between data[i] and data[j] \"\"\" def correlation_ij(i: int, j: int) -\u003e float: return correlation(data[i], data[j]) return make_matrix(len(data), len(data), correlation_ij) # corr_data is a list of four 100-d vectors num_vectors = len(corr_data) fig, ax = plt.subplots(num_vectors, num_vectors) for i in range(num_vectors): for j in range(num_vectors): # Scatter column_j on the x-axis vs. column_i on the y-axis if i != j: ax[i][j].scatter(corr_data[j], corr_data[i]) # unless i == j, in which case show the series name else: ax[i][j].annotate(\"series \" + str(i), (0.5, 0.5), xycoords='axes fraction', ha=\"center\", va=\"center\") # Then hide axis labels except left and bottom charts if i \u003c num_vectors - 1: ax[i][j].xaxis.set_visible(False) if j \u003e 0: ax[i][j].yaxis.set_visible(False) # Fix the bottom-right and top-left axis labels, which are wrong because # their charts only have text in them ax[-1][-1].set_xlim(ax[0][-1].get_xlim()) ax[0][0].set_ylim(ax[0][1].get_ylim()) plt.show() 10.2 Using Named Tuples To Model Data Rationale: using dict is error prone because of undefined keys. NamedTuple is a tuple, and is better than ordinary tuple because we can access with named slots like tuple, it is also immutable from collections import namedtuple StockPrice = namedtuple('StockPrice', ['symbol', 'date', 'closing_price']) price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03) assert price.symbol == 'MSFT' assert price.closing_price == 106.03 10.3 Dataclasses Mutable data structure Dataclass usage may feel like a namedtuple but it is data classes are implemented as classes is mutable while nametuple is implemented as tuples (which is more compact) Implement with decorator from dataclasses import dataclass @dataclass class StockPrice2: symbol: str date: datetime.date closing_price: float def is_high_tech(self) -\u003e bool: \"\"\"It's a class, so we can add methods too\"\"\" return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL'] price2 = StockPrice2('MSFT', datetime.date(2018, 12, 14), 106.03) assert price2.symbol == 'MSFT' assert price2.closing_price == 106.03 assert price2.is_high_tech() 10.4 Cleaning and Munging What to do with bad data (invalid format, etc..) ? Reject data maybe ok if we have billions of record Fix the source Do nothing and hope (haha..) 10.5 Manipulating data What columns, precision do we want to collect? This is where the business and domain logic first starts to creep in 10.6 Rescaling Refer to ==Book: Feature Scaling For Machine Learning== for more details 10.7 TQDM import tqdm for i in tqdm.tqdm(range(100)): # do something slow _ = [random.random() for _ in range(1000000)] 10.8 Dimension Reduction 11 Machine Learning 11.1 Modeling Models are mathematical (or probabilistic) relationship that exists between different variables. 11.2 What is ML? machine learning to refer to creating and using models that are learned from data 11.3 Overfitting and Underfitting Refer to [[Book - Machine Learning System Design Interview - Detail]] 4.1.3 Regularization 11.4 Bias Variance TradeOffs 11.5 Feature Extraction and Selection IMOW: book is a bit shallow on this Dimensional reduction Regularizationn chang 12 K Nearest Neighbor 12.1 The Model How it works Look at the K nearest points, as measured by a distance function The predicted label is the function of the the neighbor’s label But what about ties? Pick one of the winners at random. Weight the votes by distance and pick the weighted winner. Reduce k until we find a unique winner. def majority_vote(labels: List[str]) -\u003e str: \"\"\"Assumes that labels are ordered from nearest to farthest.\"\"\" vote_counts = Counter(labels) winner, winner_count = vote_counts.most_common(1)[0] num_winners = len([count for count in vote_counts.values() if count == winner_count]) if num_winners == 1: return winner else: # try again without the farthest return majority_vote(labels[:-1]) # Tie, so look at first 4, then 'b' assert majority_vote(['a', 'b', 'c', 'b', 'a']) == 'b' ``` Algorithm from typing import NamedTuple from scratch.linear_algebra import Vector, distance class LabeledPoint(NamedTuple): point: Vector label: str def dot(v: Vector, w: Vector) -\u003e float: return sum(v_i * w_i for v_i, w_i in zip(v, v)) # sum of sqaures of 1 vector can be defined in terms of dot function def sum_of_squares(v: Vector) -\u003e float: return dot(v, v) def squared_distance(v: Vector, w: Vector) -\u003e float: \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\" return sum_of_squares(subtract(v, w)) # Euclidean distance def distance(v: Vector, w: Vector) -\u003e float: \"\"\"Computes the distance between v and w\"\"\" return math.sqrt(squared_distance(v, w)) def knn_classify(k: int, labeled_points: List[LabeledPoint], new_point: Vector) -\u003e str: # Order the labeled points from nearest to farthest. by_distance = sorted(labeled_points, key=lambda lp: distance(lp.point, new_point)) # Find the labels for the k closest k_nearest_labels = [lp.label for lp in by_distance[:k]] # and let them vote. return majority_vote(k_nearest_labels) Vs KMeans Clustering KNN is a supervised machine learning algorithm used for ==classification== KMeans is an unsupervised machine learning algorithm used for clustering. 12.2 Example: Iris Dataset from typing import Tuple # track how many times we see (predicted, actual) confusion_matrix: Dict[Tuple[str, str], int] = defaultdict(int) num_correct = 0 for iris in iris_test: predicted = knn_classify(5, iris_train, iris.point) actual = iris.label if predicted == actual: num_correct += 1 confusion_matrix[(predicted, actual)] += 1 pct_correct = num_correct / len(iris_test) print(pct_correct, confusion_matrix) 12.3 Curse of Dimensionality Limitation of K nearest neighbor is high dimensions space are vast, increasing the chance the points are not close to each other. two points are close only if they’re close in every dimension, and every extra dimension—even if just noise—is another opportunity for each point to be farther away from every other point. When you have a lot of dimensions, it’s likely that the closest points aren’t much closer than average, so two points being close doesn’t mean very much 12.4 Further Exploration 13 Naives Bayes 13.1 Really Dumb Spam Filter Given S = Spam X is a vector of word length n The probability of spam for the sentence is $$ P(S | X) = [P(X | S)\\times P(S)] / P(X) $$ 13.2 More Sophisticated Spam Filter Need to address case Numerical underflow $$P(X_1 = x_1,..X_n=x_n) = P(X_1=x_1|S) \\times … \\times P(X_n=x_n | S)$$ $$ = \\exp[ log(P(X_1=x_1|S)) + log(P(X_n=x_n|S)) ]$$ $$P(X_1=x_1|S) + .. + .. P(X_n=x_n|S) $$ Also need to address : For rare words in spam text messages–\u003e aka smoothing $$P(X_i | Spam) = (k + numSpamsContainingX_i)/(2k + numSpams )$$ 13.3 Implementation Code: Naives Bayes from typing import Set import re def tokenize(text: str) -\u003e Set[str]: text = text.lower() # Convert to lowercase, all_words = re.findall(\"[a-z0-9']+\", text) # extract the words, and return set(all_words) from typing import NamedTuple class Message(NamedTuple): text: str is_spam: bool from typing import List, Tuple, Dict, Iterable import math from collections import defaultdict class NaiveBayesClassifier: def __init__(self, k: float = 0.5) -\u003e None: self.k = k # smoothing factor self.tokens: Set[str] = set() self.token_spam_counts: Dict[str, int] = defaultdict(int) self.token_ham_counts: Dict[str, int] = defaultdict(int) self.spam_messages = self.ham_messages = 0 def train(self, messages: Iterable[Message]) -\u003e None: for message in messages: # Increment message counts if message.is_spam: self.spam_messages += 1 else: self.ham_messages += 1 # Increment word counts for token in tokenize(message.text): self.tokens.add(token) if message.is_spam: self.token_spam_counts[token] += 1 else: self.token_ham_counts[token] += 1 def _probabilities(self, token: str) -\u003e Tuple[float, float]: \"\"\"returns P(token | spam) and P(token | ham)\"\"\" spam = self.token_spam_counts[token] ham = self.token_ham_counts[token] p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k) p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k) return p_token_spam, p_token_ham def predict(self, text: str) -\u003e float: text_tokens = tokenize(text) log_prob_if_spam = log_prob_if_ham = 0.0 # Iterate through each word in our vocabulary for token in self.tokens: prob_if_spam, prob_if_ham = self._probabilities(token) # If *token* appears in the message, # add the log probability of seeing it if token in text_tokens: log_prob_if_spam += math.log(prob_if_spam) log_prob_if_ham += math.log(prob_if_ham) # Otherwise add the log probability of _not_ seeing it, # which is log(1 - probability of seeing it) else: log_prob_if_spam += math.log(1.0 - prob_if_spam) log_prob_if_ham += math.log(1.0 - prob_if_ham) prob_if_spam = math.exp(log_prob_if_spam) prob_if_ham = math.exp(log_prob_if_ham) return prob_if_spam / (prob_if_spam + prob_if_ham) 13.4 Testing messages = [Message(\"spam rules\", is_spam=True), Message(\"ham rules\", is_spam=False), Message(\"hello ham\", is_spam=False)] model = NaiveBayesClassifier(k=0.5) model.train(messages) text = \"hello spam\" probs_if_spam = [ (1 + 0.5) / (1 + 2 * 0.5), # \"spam\" (present) 1 - (0 + 0.5) / (1 + 2 * 0.5), # \"ham\" (not present) 1 - (1 + 0.5) / (1 + 2 * 0.5), # \"rules\" (not present) (0 + 0.5) / (1 + 2 * 0.5) # \"hello\" (present) ] probs_if_ham = [ (0 + 0.5) / (2 + 2 * 0.5), # \"spam\" (present) 1 - (2 + 0.5) / (2 + 2 * 0.5), # \"ham\" (not present) 1 - (1 + 0.5) / (2 + 2 * 0.5), # \"rules\" (not present) (1 + 0.5) / (2 + 2 * 0.5), # \"hello\" (present) ] p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam)) p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham)) # Should be about 0.83 assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham) 13.5 Using The Model from io import BytesIO # So we can treat bytes as a file. import requests # To download the files, which import tarfile # are in .tar.bz format. BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\" FILES = [\"20021010_easy_ham.tar.bz2\", \"20021010_hard_ham.tar.bz2\", \"20021010_spam.tar.bz2\"] # This is where the data will end up, # in /spam, /easy_ham, and /hard_ham subdirectories. # Change this to where you want the data. OUTPUT_DIR = 'spam_data' for filename in FILES: # Use requests to get the file contents at each URL. content = requests.get(f\"{BASE_URL}/{filename}\").content # Wrap the in-memory bytes so we can use them as a \"file.\" fin = BytesIO(content) # And extract all the files to the specified output dir. with tarfile.open(fileobj=fin, mode='r:bz2') as tf: tf.extractall(OUTPUT_DIR) import glob, re # modify the path to wherever you've put the files path = 'spam_data/*/*' data: List[Message] = [] # glob.glob returns every filename that matches the wildcarded path for filename in glob.glob(path): is_spam = \"ham\" not in filename # There are some garbage characters in the emails; the errors='ignore' # skips them instead of raising an exception. with open(filename, errors='ignore') as email_file: for line in email_file: if line.startswith(\"Subject:\"): subject = line.lstrip(\"Subject: \") data.append(Message(subject, is_spam)) break # done with this file import random from scratch.machine_learning import split_data random.seed(0) # just so you get the same answers as me train_messages, test_messages = split_data(data, 0.75) model = NaiveBayesClassifier() model.train(train_messages) from collections import Counter predictions = [(message, model.predict(message.text)) for message in test_messages] # Assume that spam_probability \u003e 0.5 corresponds to spam prediction # and count the combinations of (actual is_spam, predicted is_spam) confusion_matrix = Counter((message.is_spam, spam_probability \u003e 0.5) for message, spam_probability in predictions) print(confusion_matrix) 14 Simple Linear Regression 14.1 The Model Task: Predict time of use as a function of numFriends $$y_i = \\beta x_i + \\alpha $$ such that $y_i$ is the amount of time user i spend on platform $x_i$ is the number of friends user_i has on the platform Code def predict(alpha: float, beta: float, x_i: float) -\u003e float: return beta * x_i + alpha def error(alpha: float, beta: float, x_i: float, y_i: float) -\u003e float: \"\"\" The error from predicting beta * x_i + alpha when the actual value is y_i \"\"\" return predict(alpha, beta, x_i) - y_i from scratch.linear_algebra import Vector def sum_of_sqerrors(alpha: float, beta: float, x: Vector, y: Vector) -\u003e float: return sum(error(alpha, beta, x_i, y_i) ** 2 for x_i, y_i in zip(x, y)) from typing import Tuple from scratch.linear_algebra import Vector from scratch.statistics import correlation, standard_deviation, mean def least_squares_fit(x: Vector, y: Vector) -\u003e Tuple[float, float]: \"\"\" Given two vectors x and y, find the least-squares values of alpha and beta \"\"\" beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x) alpha = mean(y) - beta * mean(x) return alpha, beta Goodness of fit: How well does our parameter fit the data? R-squared (aka coefficient of determination) measures the fraction of total variation in the dependent variable (ie y) is captured by the variable ```python from scratch.statistics import de_mean def total_sum_of_squares(y: Vector) -\u003e float: \"\"\"the total squared variation of y_i's from their mean\"\"\" return sum(v ** 2 for v in de_mean(y)) def r_squared(alpha: float, beta: float, x: Vector, y: Vector) -\u003e float: \"\"\" the fraction of variation in y captured by the model, which equals 1 - the fraction of variation in y not captured by the model \"\"\" return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) / total_sum_of_squares(y)) ``` 14.2 Using Gradient Descent In 14.1, we solved the close form of the linear equation where we solved the parameters $\\alpha \\beta$ In this section, we use gradient descent as a way to approximate the solution theta = $\\alpha , \\beta$ import random import tqdm from scratch.gradient_descent import gradient_step num_epochs = 10000 random.seed(0) guess = [random.random(), random.random()] # choose random value to start learning_rate = 0.00001 with tqdm.trange(num_epochs) as t: for _ in t: alpha, beta = guess # Partial derivative of loss with respect to alpha grad_a = sum(2 * error(alpha, beta, x_i, y_i) for x_i, y_i in zip(num_friends_good, daily_minutes_good)) # Partial derivative of loss with respect to beta grad_b = sum(2 * error(alpha, beta, x_i, y_i) * x_i for x_i, y_i in zip(num_friends_good, daily_minutes_good)) # Compute loss to stick in the tqdm description loss = sum_of_sqerrors(alpha, beta, num_friends_good, daily_minutes_good) t.set_description(f\"loss: {loss:.3f}\") # Finally, update the guess guess = gradient_step(guess, [grad_a, grad_b], -learning_rate) 14.3 Maximum Liklihood Estmation (MLE) In this section, we use MLE to explain why we use least squares fit to derive the closed form for parameters $\\alpha, \\beta$ MLE estimates the parameters $\\theta$ that makes the observed data most probable. Minimizing the sum of squared errors is equivalent to maximizing the likelihood of the observed data. 15 Multiple Regression 15. 1 The Model In Chapter 14, we solve a linear equation with 2 parameters. In this chapter, we increase the number of parameters we will learn. Let’s increase the num parameter we learn (go beyond a linear line) $$y_i = \\alpha + \\beta_1x_{i1} + … + \\beta_kx_{ik}$$ where i = user i k = number of parameters we are fitting model learn parameter as a vector $$\\beta = [\\alpha, \\beta_1, .., \\beta_k]$$ predict code def predict(x: Vector, beta: Vector) -\u003e float: \"\"\"assumes that the first element of x is 1\"\"\" return dot(x, beta) 15. 2 Further Assumptions of Least Squares Model Key assumptions All the features are linearly independent The parameter of input vector x are uncorrelated with the error 15.3 Fitting the Model Code: Helper Functions from typing import List def error(x: Vector, y: float, beta: Vector) -\u003e float: return predict(x, beta) - y def squared_error(x: Vector, y: float, beta: Vector) -\u003e float: return error(x, y, beta) ** 2 def sqerror_gradient(x: Vector, y: float, beta: Vector) -\u003e Vector: err = error(x, y, beta) return [2 * err * x_i for x_i in x] Code: Least Squares Fit import random import tqdm from scratch.linear_algebra import vector_mean from scratch.gradient_descent import gradient_step def least_squares_fit(xs: List[Vector], ys: List[float], learning_rate: float = 0.001, num_steps: int = 1000, batch_size: int = 1) -\u003e Vector: \"\"\" Find the beta that minimizes the sum of squared errors assuming the model y = dot(x, beta). \"\"\" # Start with a random guess guess = [random.random() for _ in xs[0]] for _ in tqdm.trange(num_steps, desc=\"least squares fit\"): for start in range(0, len(xs), batch_size): batch_xs = xs[start:start+batch_size] batch_ys = ys[start:start+batch_size] gradient = vector_mean([sqerror_gradient(x, y, guess) for x, y in zip(batch_xs, batch_ys)]) guess = gradient_step(guess, gradient, -learning_rate) return guess 15.4 Interpreting the Model The value of the learned coefficients represents the importance of that features, assuming ==all else being equal (big caveat)== Ex: all else being equal, having one friend translate to 2 minutes on the app Parameters do not explain how the variables interact with each other. 15.5 Goodness of Fit from scratch.simple_linear_regression import total_sum_of_squares def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -\u003e float: sum_of_squared_errors = sum(error(x, y, beta) ** 2 for x, y in zip(xs, ys)) return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys) 15.6 The Bootstrap Bootstrapping is a method of inferring results for a population from results found on a collection of ==smaller random ==samples== of that population, using replacement during the sampling process. Code: utility function from typing import TypeVar, Callable X = TypeVar('X') # Generic type for data Stat = TypeVar('Stat') # Generic type for \"statistic\" def bootstrap_sample(data: List[X]) -\u003e List[X]: \"\"\"randomly samples len(data) elements with replacement\"\"\" return [random.choice(data) for _ in data] def bootstrap_statistic(data: List[X], stats_fn: Callable[[List[X]], Stat], num_samples: int) -\u003e List[Stat]: \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\" return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)] Code: usage # 101 points all very close to 100 close_to_100 = [99.5 + random.random() for _ in range(101)] medians_close = bootstrap_statistic(close_to_100, median, 100) # 101 points, 50 of them near 0, 50 of them near 200 far_from_100 = ([99.5 + random.random()] + [random.random() for _ in range(50)] + [200 + random.random() for _ in range(50)]) medians_far = bootstrap_statistic(far_from_100, median, 100) 15.7 Standard Errors of Regression Coefficient 15.8 Regularization Regularization is an approach in which we add to the error term a penalty that gets larger as beta gets larger. We then minimize the combined error and penalty. The more importance we place on the penalty term, the more we discourage large coefficients. 2 Types in LInear Regression Ridge (aka L2) shrink the coefficient overall Lasso (aka L1) drive coefficient to 0 to learn more sparse models Code: Ridge (L2) # alpha is a *hyperparameter* controlling how harsh the penalty is. # Sometimes it's called \"lambda\" but that already means something in Python. def ridge_penalty(beta: Vector, alpha: float) -\u003e float: return alpha * dot(beta[1:], beta[1:]) def squared_error_ridge(x: Vector, y: float, beta: Vector, alpha: float) -\u003e float: \"\"\"estimate error plus ridge penalty on beta\"\"\" return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha) from scratch.linear_algebra import add def ridge_penalty_gradient(beta: Vector, alpha: float) -\u003e Vector: \"\"\"gradient of just the ridge penalty\"\"\" return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]] def sqerror_ridge_gradient(x: Vector, y: float, beta: Vector, alpha: float) -\u003e Vector: \"\"\" the gradient corresponding to the ith squared error term including the ridge penalty \"\"\" return add(sqerror_gradient(x, y, beta), ridge_penalty_gradient(beta, alpha))\t# TEST CODE random.seed(0) beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0, # alpha learning_rate, 5000, 25) Code: Lasso def lasso_penalty(beta, alpha): return alpha * sum(abs(beta_i) for beta_i in beta[1:]) 16 Logistic Regression 16.1 The Problem Potential issues with linear regression models Output model range does not match our end use case of 0 or 1 Linear regression assumes Linear relationship between X and the mean of Y Not true if R coefficient is positive Variance of residual is the same for any value of X Observations are independent of each other Logistic function $$y = \\frac{1}{1 + e^{-x}}$$ $$x = f(x_i\\beta)$$ 16.2 The Logistic Function 16.3 Applying the Model 16.4 Goodness of Fit 16.5 Support Vector Machines 17 Decision Tree 17.1 What is a Decision Tree A decision tree uses a tree structure to represent a number of possible decision paths and an outcome for each path. 17.2 Entropy Entropy measure how much information/randomness there is Applied to ML, high entropy means there are multiple classes label in the set low entropy means there are few (or 1) class label in the set Mathematics $$H(S) = -p_1\\log_2p_1 - … -p_n\\log_2p_n $$ S = Partition Set $p_n$ is the proportion of class | label in the set S ==Entropy is lowest when the $p_n$ is 0 or 1 == Code from typing import List, Any import math from collections import Counter def entropy(class_probabilities: List[float]) -\u003e float: \"\"\"Given a list of class probabilities, compute the entropy\"\"\" return sum(-p * math.log(p, 2) for p in class_probabilities if p \u003e 0) # ignore zero probabilities def class_probabilities(labels: List[Any]) -\u003e List[float]: total_count = len(labels) return [count / total_count for count in Counter(labels).values()] def data_entropy(labels: List[Any]) -\u003e float: return entropy(class_probabilities(labels)) 17.3 Entropy of a Partition Lowest entropy of a partition is when all the member in a set belongs to a label class Mathematics to calculate the entropy of a partition ==If we partition the data by a feature (ie gender) and its value(ie female), we will create multiple sets, each set will have data with labels from class 1 to m== $$H_{partition} = q_1H(S_1) + … + q_mH(S_m)$$ where $q_m$ is the proportion of data with class label m $S_m$ is the resulting set where all the data has gender=male (as an example), but the remaining features (age, race) will each be split until we reach the leaf node. Code def partition_entropy(subsets: List[List[Any]]) -\u003e float: \"\"\"Returns the entropy from this partition of data into subsets\"\"\" # subsets is S1, S2, .., S_m in our equation above total_count = sum(len(subset) for subset in subsets) return sum(data_entropy(subset) * len(subset) / total_count for subset in subsets) 17.4 Creating a Decision Tree Algorithm-Simplified If the data all have the same label, create a leaf node that predicts that label and then stop. If the list of attributes is empty (i.e., there are no more possible questions to ask), create a leaf node that predicts the most common label and then stop. Otherwise, try partitioning the data by each of the attributes (aka feature). Choose the partition with the ==lowest partition entropy==. Add a decision node based on the chosen attribute. Recur on each partitioned subset using the remaining attributes. 17.4.1 Example: Decision tree to see whether to interview someone based on the features level, language, tweets, and phd Mock Data from typing import NamedTuple, Optional class Candidate(NamedTuple): level: str lang: str tweets: bool phd: bool did_well: Optional[bool] = None # allow unlabeled data # level lang tweets phd did_well inputs = [Candidate('Senior', 'Java', False, False, False), Candidate('Senior', 'Java', False, True, False), Candidate('Mid', 'Python', False, False, True), Candidate('Junior', 'Python', False, False, True), Candidate('Junior', 'R', True, False, True), Candidate('Junior', 'R', True, True, False), Candidate('Mid', 'R', True, True, True), Candidate('Senior', 'Python', False, False, False)] Partition The Set of Candidate Based on a attribute/feature from typing import Dict, TypeVar from collections import defaultdict T = TypeVar('T') # generic type for inputs def partition_by(inputs: List[T], attribute: str) -\u003e Dict[Any, List[T]]: \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\" partitions: Dict[Any, List[T]] = defaultdict(list) for input in inputs: key = getattr(input, attribute) # value of the specified attribute partitions[key].append(input) # add input to the correct partition return partitions def partition_entropy_by(inputs: List[Any], attribute: str, label_attribute: str) -\u003e float: \"\"\"Compute the entropy corresponding to the given partition\"\"\" # partitions consist of our inputs partitions = partition_by(inputs, attribute) # but partition_entropy needs just the class labels labels = [[getattr(input, label_attribute) for input in partition] for partition in partitions.values()] return partition_entropy(labels) Test Code for key in ['level','lang','tweets','phd']: print(key, partition_entropy_by(inputs, key, 'did_well')) assert 0.69 \u003c partition_entropy_by(inputs, 'level', 'did_well') \u003c 0.70 assert 0.86 \u003c partition_entropy_by(inputs, 'lang', 'did_well') \u003c 0.87 assert 0.78 \u003c partition_entropy_by(inputs, 'tweets', 'did_well') \u003c 0.79 assert 0.89 \u003c partition_entropy_by(inputs, 'phd', 'did_well') \u003c 0.90 17.5 Putting It All Together Code: Inference Logic from typing import NamedTuple, Union, Any class Leaf(NamedTuple): value: Any class Split(NamedTuple): attribute: str subtrees: dict default_value: Any = None DecisionTree = Union[Leaf, Split] # This is the result of our training the tree hiring_tree = Split('level', { # first, consider \"level\" 'Junior': Split('phd', { # if level is \"Junior\", next look at \"phd\" False: Leaf(True), # if \"phd\" is False, predict True True: Leaf(False) # if \"phd\" is True, predict False }), 'Mid': Leaf(True), # if level is \"Mid\", just predict True 'Senior': Split('tweets', { # if level is \"Senior\", look at \"tweets\" False: Leaf(False), # if \"tweets\" is False, predict False True: Leaf(True) # if \"tweets\" is True, predict True }) }) def classify(tree: DecisionTree, input: Any) -\u003e Any: \"\"\"classify the input using the given decision tree\"\"\" # If this is a leaf node, return its value if isinstance(tree, Leaf): return tree.value # Otherwise this tree consists of an attribute to split on # and a dictionary whose keys are values of that attribute # and whose values are subtrees to consider next subtree_key = getattr(input, tree.attribute) if subtree_key not in tree.subtrees: # If no subtree for key, return tree.default_value # return the default value. subtree = tree.subtrees[subtree_key] # Choose the appropriate subtree return classify(subtree, input) # and use it to classify the input. Code: Build The Tree def build_tree_id3(inputs: List[Any], split_attributes: List[str], target_attribute: str) -\u003e DecisionTree: # Count target labels label_counts = Counter(getattr(input, target_attribute) for input in inputs) most_common_label = label_counts.most_common(1)[0][0] # If there's a unique label, predict it if len(label_counts) == 1: return Leaf(most_common_label) # If no split attributes left, return the majority label if not split_attributes: return Leaf(most_common_label) # Otherwise split by the best attribute def split_entropy(attribute: str) -\u003e float: \"\"\"Helper function for finding the best attribute\"\"\" return partition_entropy_by(inputs, attribute, target_attribute) best_attribute = min(split_attributes, key=split_entropy) partitions = partition_by(inputs, best_attribute) new_attributes = [a for a in split_attributes if a != best_attribute] # Recursively build the subtrees subtrees = {attribute_value : build_tree_id3(subset, new_attributes, target_attribute) for attribute_value, subset in partitions.items()} return Split(best_attribute, subtrees, default_value=most_common_label) 17.6 Random Forests Reduce decision tree’s tendency to overfit by bootsgtapping, where bootstrap aggregation (aka bagging): each tree is selected with different sampled subset of data randomly select subset of attributes 18 Neural Network 18.1 Perceptrons 18.2 Feed Forward Neural Network 18.3 Back propagation 18.4 Example: Fiz Fuzz 19 Deep Learning 19.1 The Tensor 19.2 The Layer Abstraction 19.3 Linear Layer 19.4 NN As a Sequence of Layers 19.5 Loss and Optimization 19.6 Example: XOR Revisited 19.7 Other Activation Functions 19.8 FizzBuzz Revisited 19.9 Softmaxes and Cross Entropy 19.10 Dropout 19.11 Example: MNIST 19.12 Saving and Loading Models 20 Clustering 20.1 The Idea (K-Means) Partition the inputs into sets $S_1, .., S_k$ in a way to minimize the squared distance of each point from the $cluster_k$ mean. This is a top down approach, starting from the mean of k clusters. Compared to the bottom up hierarchical clustering in section 20.6 20.2 The Model Algorithm Start with a set of k-means, which are points in d-dimensional space. Assign each point to the mean to which it is closest. If no point’s assignment has changed, stop and keep the clusters. If some point’s assignment has changed, recompute the means and return to step 2. Code: How many coordinates 2 vectors differ in from scratch.linear_algebra import Vector def num_differences(v1: Vector, v2: Vector) -\u003e int: assert len(v1) == len(v2) return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2]) assert num_differences([1, 2, 3], [2, 1, 3]) == 2 assert num_differences([1, 2], [1, 2]) == 0 Code: Given some vectors and their assignments to clusters, compute the mean of the clusters from typing import List from scratch.linear_algebra import vector_mean def cluster_means(k: int, inputs: List[Vector], assignments: List[int]) -\u003e List[Vector]: # clusters[i] contains the inputs whose assignment is i clusters = [[] for i in range(k)] for input, assignment in zip(inputs, assignments): clusters[assignment].append(input) # if a cluster is empty, just use a random point return [vector_mean(cluster) if cluster else random.choice(inputs) for cluster in clusters] Code: End to End import itertools import random import tqdm from scratch.linear_algebra import squared_distance class KMeans: def __init__(self, k: int) -\u003e None: self.k = k # number of clusters self.means = None def classify(self, input: Vector) -\u003e int: \"\"\"return the index of the cluster closest to the input\"\"\" return min(range(self.k), key=lambda i: squared_distance(input, self.means[i])) def train(self, inputs: List[Vector]) -\u003e None: # Start with random assignments assignments = [random.randrange(self.k) for _ in inputs] with tqdm.tqdm(itertools.count()) as t: for _ in t: # Compute means and find new assignments self.means = cluster_means(self.k, inputs, assignments) new_assignments = [self.classify(input) for input in inputs] # Check how many assignments changed and if we're done num_changed = num_differences(assignments, new_assignments) if num_changed == 0: return # Otherwise keep the new assignments, and compute new means assignments = new_assignments self.means = cluster_means(self.k, inputs, assignments) t.set_description(f\"changed: {num_changed} / {len(inputs)}\") 20.3 Example: Meetups 20.4 Choosing K various ways to choose k Plot squared distance error as a function of k; choose where the graph bends Code from matplotlib import pyplot as plt def squared_clustering_errors(inputs: List[Vector], k: int) -\u003e float: \"\"\"finds the total squared error from k-means clustering the inputs\"\"\" clusterer = KMeans(k) clusterer.train(inputs) means = clusterer.means assignments = [clusterer.classify(input) for input in inputs] return sum(squared_distance(input, means[cluster]) for input, cluster in zip(inputs, assignments)) # now plot from 1 up to len(inputs) clusters ks = range(1, len(inputs) + 1) errors = [squared_clustering_errors(inputs, k) for k in ks] plt.plot(ks, errors) plt.xticks(ks) plt.xlabel(\"k\") plt.ylabel(\"total squared error\") plt.title(\"Total Error vs. # of Clusters\") plt.show() 20.5 Example: Clustering Colors 20.6 Bottom Up Hierarchical Clustering Algorithm Make each input its own cluster of one. As long as there are multiple clusters remaining, find the two closest clusters and merge them. Code: Modeling from typing import NamedTuple, Union class Leaf(NamedTuple): value: Vector class Merged(NamedTuple): children: tuple order: int # the sequence of the merge leaf1 = Leaf([10, 20]) leaf2 = Leaf([30, -15]) merged = Merged((leaf1, leaf2), order=1) Cluster = Union[Leaf, Merged] Code: Helper Function # Return all values contained in cluster recursively def get_values(cluster: Cluster) -\u003e List[Vector]: if isinstance(cluster, Leaf): return [cluster.value] else: return [value for child in cluster.children for value in get_values(child)] assert get_values(merged) == [[10, 20], [30, -15]] # Metric for distance from typing import Callable from scratch.linear_algebra import distance def cluster_distance(cluster1: Cluster, cluster2: Cluster, distance_agg: Callable = min) -\u003e float: \"\"\" compute all the pairwise distances between cluster1 and cluster2 and apply the aggregation function _distance_agg_ to the resulting list \"\"\" return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)]) # Pts will be merged from bottom up; keep track of the order def get_merge_order(cluster: Cluster) -\u003e float: if isinstance(cluster, Leaf): return float('inf') # was never merged else: return cluster.order def get_children(cluster: Cluster): if isinstance(cluster, Leaf): raise TypeError(\"Leaf has no children\") else: return cluster.children Code: Bottom Up def bottom_up_cluster(inputs: List[Vector], distance_agg: Callable = min) -\u003e Cluster: # Start with all leaves clusters: List[Cluster] = [Leaf(input) for input in inputs] def pair_distance(pair: Tuple[Cluster, Cluster]) -\u003e float: return cluster_distance(pair[0], pair[1], distance_agg) # as long as we have more than one cluster left... while len(clusters) \u003e 1: # find the two closest clusters c1, c2 = min(((cluster1, cluster2) for i, cluster1 in enumerate(clusters) for cluster2 in clusters[:i]), key=pair_distance) # remove them from the list of clusters clusters = [c for c in clusters if c != c1 and c != c2] # merge them, using merge_order = # of clusters left merged_cluster = Merged((c1, c2), order=len(clusters)) # and add their merge clusters.append(merged_cluster) # when there's only one cluster left, return it return clusters[0] 21 Natural Language Processing 21.1 Word Clouds Given (Word, jobListing cnt, resume cnt) data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50), (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60), (\"data science\", 60, 70), (\"analytics\", 90, 3), (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0), (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10), (\"self-starter\", 30, 50), (\"customer focus\", 65, 15), (\"thought leadership\", 35, 35)] Code to Plot Word Cloud from matplotlib import pyplot as plt def text_size(total: int) -\u003e float: \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\" return 8 + total / 200 * 20 for word, job_popularity, resume_popularity in data: plt.text(job_popularity, resume_popularity, word, ha='center', va='center', size=text_size(job_popularity + resume_popularity)) plt.xlabel(\"Popularity on Job Postings\") plt.ylabel(\"Popularity on Resumes\") plt.axis([0, 100, 0, 100]) plt.xticks([]) plt.yticks([]) plt.show() 21.2 n-Gram Language Models 21.3 Grammers 21.4 Gibbs Sampling 21.5 ==Topics Modeling== 21.6 ==Word Vectors== Background Word vectors have the nice property that similar words appear near each other Code: Similarity Measurement Via Cosine Similarity from scratch.linear_algebra import dot, Vector import math def cosine_similarity(v1: Vector, v2: Vector) -\u003e float: return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2)) Code: Create Training Sentences colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"] nouns = [\"bed\", \"car\", \"boat\", \"cat\"] verbs = [\"is\", \"was\", \"seems\"] adverbs = [\"very\", \"quite\", \"extremely\", \"\"] adjectives = [\"slow\", \"fast\", \"soft\", \"hard\"] def make_sentence() -\u003e str: return \" \".join([ \"The\", random.choice(colors), random.choice(nouns), random.choice(verbs), random.choice(adverbs), random.choice(adjectives), \".\" ]) NUM_SENTENCES = 50 random.seed(0) sentences = [make_sentence() for _ in range(NUM_SENTENCES)] Code: Vocabulary Converts Word to ID from scratch.deep_learning import Tensor class Vocabulary: def __init__(self, words: List[str] = None) -\u003e None: self.w2i: Dict[str, int] = {} # mapping word -\u003e word_id self.i2w: Dict[int, str] = {} # mapping word_id -\u003e word for word in (words or []): # If words were provided, self.add(word) # add them. @property def size(self) -\u003e int: \"\"\"how many words are in the vocabulary\"\"\" return len(self.w2i) def add(self, word: str) -\u003e None: if word not in self.w2i: # If the word is new to us: word_id = len(self.w2i) # Find the next id. self.w2i[word] = word_id # Add to the word -\u003e word_id map. self.i2w[word_id] = word # Add to the word_id -\u003e word map. def get_id(self, word: str) -\u003e int: \"\"\"return the id of the word (or None)\"\"\" return self.w2i.get(word) def get_word(self, word_id: int) -\u003e str: \"\"\"return the word with the given id (or None)\"\"\" return self.i2w.get(word_id) def one_hot_encode(self, word: str) -\u003e Tensor: word_id = self.get_id(word) assert word_id is not None, f\"unknown word {word}\" return [1.0 if i == word_id else 0.0 for i in range(self.size)] Code: Save and Load Vobcabulary import json def save_vocab(vocab: Vocabulary, filename: str) -\u003e None: with open(filename, 'w') as f: json.dump(vocab.w2i, f) # Only need to save w2i def load_vocab(filename: str) -\u003e Vocabulary: vocab = Vocabulary() with open(filename) as f: # Load w2i and generate i2w from it vocab.w2i = json.load(f) vocab.i2w = {id: word for word, id in vocab.w2i.items()} return vocab Code: Train Embedding Via SkipGram Concepts Skip-gram takes as input a word and generates probabilities for what words are likely to be seen near it. We will feed it training pairs (word, nearby_word) and try to minimize the SoftmaxCrossEntropy loss Embedding layer takes a word-id input and returns a vector. We will implement it as a lookup table {word_id -\u003e vector_i } word_id –\u003e linear layer –\u003e softmax dimension of word_id and linear layer = size of vocabulary _Code: Embedding _ from typing import Iterable from scratch.deep_learning import Layer, Tensor, random_tensor, zeros_like class Embedding(Layer): def __init__(self, num_embeddings: int, embedding_dim: int) -\u003e None: self.num_embeddings = num_embeddings self.embedding_dim = embedding_dim # One vector of size embedding_dim for each desired embedding self.embeddings = random_tensor(num_embeddings, embedding_dim) self.grad = zeros_like(self.embeddings) # Save last input id self.last_input_id = None def forward(self, input_id: int) -\u003e Tensor: \"\"\"Just select the embedding vector corresponding to the input id\"\"\" self.input_id = input_id # remember for use in backpropagation return self.embeddings[input_id] def backward(self, gradient: Tensor) -\u003e None: # Zero out the gradient corresponding to the last input. # This is way cheaper than creating a new all-zero tensor each time. if self.last_input_id is not None: zero_row = [0 for _ in range(self.embedding_dim)] self.grad[self.last_input_id] = zero_row self.last_input_id = self.input_id self.grad[self.input_id] = gradient def params(self) -\u003e Iterable[Tensor]: return [self.embeddings] def grads(self) -\u003e Iterable[Tensor]: return [self.grad] Code: Text Embedding class TextEmbedding(Embedding): def __init__(self, vocab: Vocabulary, embedding_dim: int) -\u003e None: # Call the superclass constructor super().__init__(vocab.size, embedding_dim) # And hang onto the vocab self.vocab = vocab def __getitem__(self, word: str) -\u003e Tensor: word_id = self.vocab.get_id(word) if word_id is not None: return self.embeddings[word_id] else: return None def closest(self, word: str, n: int = 5) -\u003e List[Tuple[float, str]]: \"\"\"Returns the n closest words based on cosine similarity\"\"\" vector = self[word] # Compute pairs (similarity, other_word), and sort most similar first scores = [(cosine_similarity(vector, self.embeddings[i]), other_word) for other_word, i in self.vocab.w2i.items()] scores.sort(reverse=True) return scores[:n] Code: Create Training Data import re # This is not a great regex, but it works on our data. tokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower()) for sentence in sentences] # Create a vocabulary (that is, a mapping word -\u003e word_id) based on our text. vocab = Vocabulary(word for sentence_words in tokenized_sentences for word in sentence_words) from scratch.deep_learning import Tensor, one_hot_encode inputs: List[int] = [] targets: List[Tensor] = [] for sentence in tokenized_sentences: for i, word in enumerate(sentence): # For each word for j in [i - 2, i - 1, i + 1, i + 2]: # take the nearby locations if 0 \u003c= j \u003c len(sentence): # that aren't out of bounds nearby_word = sentence[j] # and get those words. # Add an input that's the original word_id inputs.append(vocab.get_id(word)) # Add a target that's the one-hot-encoded nearby word targets.append(vocab.one_hot_encode(nearby_word)) Code: Create Model from scratch.deep_learning import Sequential, Linear random.seed(0) EMBEDDING_DIM = 5 # seems like a good size # Define the embedding layer separately, so we can reference it. embedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM) model = Sequential([ # Given a word (as a vector of word_ids), look up its embedding. embedding, # And use a linear layer to compute scores for \"nearby words.\" Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size) ]) Code: Training Loop from scratch.deep_learning import SoftmaxCrossEntropy, Momentum, GradientDescent loss = SoftmaxCrossEntropy() optimizer = GradientDescent(learning_rate=0.01) for epoch in range(100): epoch_loss = 0.0 for input, target in zip(inputs, targets): predicted = model.forward(input) epoch_loss += loss.loss(predicted, target) gradient = loss.gradient(predicted, target) model.backward(gradient) optimizer.step(model) print(epoch, epoch_loss) # Print the loss print(embedding.closest(\"black\")) # and also a few nearest words print(embedding.closest(\"slow\")) # so we can see what's being print(embedding.closest(\"car\")) # learned. Code: Analysis pairs = [(cosine_similarity(embedding[w1], embedding[w2]), w1, w2) for w1 in vocab.w2i for w2 in vocab.w2i if w1 \u003c w2] pairs.sort(reverse=True) # PLOT from scratch.working_with_data import pca, transform import matplotlib.pyplot as plt # Extract the first two principal components and transform the word vectors components = pca(embedding.embeddings, 2) transformed = transform(embedding.embeddings, components) # Scatter the points (and make them white so they're \"invisible\") fig, ax = plt.subplots() ax.scatter(*zip(*transformed), marker='.', color='w') # Add annotations for each word at its transformed location for word, idx in vocab.w2i.items(): ax.annotate(word, transformed[idx]) # And hide the axes ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() 21.7 RNN 21.8 Example: Using a Character Level RNN 22 Network Analysis (Graph) 22.1 Betweenness Centrality 22.2 Eigenvector Centrality 22.3 Directed Graphs and Page Ranks 23 Recommender System 23.1 Manual Curations users_interests = [ [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"], [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"], [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"], [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"], [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"], [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"], [\"statistics\", \"probability\", \"mathematics\", \"theory\"], [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"], [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"], [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"], [\"statistics\", \"R\", \"statsmodels\"], [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"], [\"pandas\", \"R\", \"Python\"], [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"], [\"libsvm\", \"regression\", \"support vector machines\"] ] 23.2 Recommending What’s Popular # Flatten nested list from collections import Counter popular_interests = Counter(interest for user_interests in users_interests for interest in user_interests) # Suggest the most popular interests the user currently does not have # CON: new users will get everything; not very personalized from typing import List, Tuple def most_popular_new_interests( user_interests: List[str], max_results: int = 5) -\u003e List[Tuple[str, int]]: suggestions = [(interest, frequency) for interest, frequency in popular_interests.most_common() if interest not in user_interests] return suggestions[:max_results] 23.3 User Based Collaborative Filtering Concept Recommend other similar users who has similar interests Create a user vector where each position represent a interest Cosine similarity to find close users CODE: unique_interests = sorted({interest for user_interests in users_interests for interest in user_interests}) # ------------------------------------------------------- # Create user vector # ------------------------------------------------------- def make_user_interest_vector(user_interests: List[str]) -\u003e List[int]: \"\"\" Given a list of interests, produce a vector whose ith element is 1 if unique_interests[i] is in the list, 0 otherwise \"\"\" return [1 if interest in user_interests else 0 for interest in unique_interests] user_interest_vectors = [make_user_interest_vector(user_interests) for user_interests in users_interests] # ------------------------------------------------------- # Find other simlar users # ------------------------------------------------------- from scratch.nlp import cosine_similarity user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j) for interest_vector_j in user_interest_vectors] for interest_vector_i in user_interest_vectors] def most_similar_users_to(user_id: int) -\u003e List[Tuple[int, float]]: pairs = [(other_user_id, similarity) # Find other for other_user_id, similarity in # users with enumerate(user_similarities[user_id]) # nonzero if user_id != other_user_id and similarity \u003e 0] # similarity. return sorted(pairs, # Sort them key=lambda pair: pair[-1], # most similar reverse=True) # first. # ------------------------------------------------------- # Suggest other users: # - for each S similar users # - add each S similar user's simlarity score to interest score dict # - return top k score interest # ------------------------------------------------------- from collections import defaultdict def user_based_suggestions(user_id: int, include_current_interests: bool = False): # Sum up the similarities suggestions: Dict[str, float] = defaultdict(float) for other_user_id, similarity in most_similar_users_to(user_id): for interest in users_interests[other_user_id]: suggestions[interest] += similarity # Convert them to a sorted list suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], # weight reverse=True) # And (maybe) exclude already interests if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] 23.4 Item Based Collaborative Filtering Compute similarities between items (aka interest directly) suggest interests similar to the user’s current interest CODE # Transpose our preivous user-to-interest vector to interest-to-user vector interest_user_matrix = [[user_interest_vector[j] for user_interest_vector in user_interest_vectors] for j, _ in enumerate(unique_interests)] # `unique_interests[0]` is Big Data, and so #`interest_user_matrix[0]` is: [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0] interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) for user_vector_j in interest_user_matrix] for user_vector_i in interest_user_matrix] def most_similar_interests_to(interest_id: int): similarities = interest_similarities[interest_id] pairs = [(unique_interests[other_interest_id], similarity) for other_interest_id, similarity in enumerate(similarities) if interest_id != other_interest_id and similarity \u003e 0] return sorted(pairs, key=lambda pair: pair[-1], reverse=True) def item_based_suggestions(user_id: int, include_current_interests: bool = False): # Add up the similar interests suggestions = defaultdict(float) user_interest_vector = user_interest_vectors[user_id] for interest_id, is_interested in enumerate(user_interest_vector): if is_interested == 1: similar_interests = most_similar_interests_to(interest_id) for interest, similarity in similar_interests: suggestions[interest] += similarity # Sort them by weight suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], reverse=True) if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] 23.5 Matrix Factorization Theory Users and items are represented has a latent type (ie embedding) User matrix = numUsers by vector dim matrix Model learn the representation such that similarity_score(embedding_user, embedding_item) ~= label(ie rating) Example - MoveLens Ratings: data. Code: Ingest Data # Class models from typing import NamedTuple class Rating(NamedTuple): user_id: str movie_id: str rating: float # Import Movie ratings import csv # We specify this encoding to avoid a UnicodeDecodeError. # See: https://stackoverflow.com/a/53136168/1076346. with open(MOVIES, encoding=\"iso-8859-1\") as f: reader = csv.reader(f, delimiter=\"|\") movies = {movie_id: title for movie_id, title, *_ in reader} # Create a list of [Rating] with open(RATINGS, encoding=\"iso-8859-1\") as f: reader = csv.reader(f, delimiter=\"\\t\") ratings = [Rating(user_id, movie_id, float(rating)) for user_id, movie_id, rating, _ in reader] # Split data import random random.seed(0) random.shuffle(ratings) split1 = int(len(ratings) * 0.7) split2 = int(len(ratings) * 0.85) train = ratings[:split1] # 70% of the data validation = ratings[split1:split2] # 15% of the data test = ratings[split2:] # 15% of the data Explore Data import re # Data structure for accumulating ratings by movie_id star_wars_ratings = {movie_id: [] for movie_id, title in movies.items() if re.search(\"Star Wars|Empire Strikes|Jedi\", title)} # Iterate over ratings, accumulating the Star Wars ones for rating in ratings: if rating.movie_id in star_wars_ratings: star_wars_ratings[rating.movie_id].append(rating.rating) # Compute the average rating for each movie avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id) for movie_id, title_ratings in star_wars_ratings.items()] # And then print them in order for avg_rating, movie_id in sorted(avg_ratings, reverse=True): print(f\"{avg_rating:.2f} {movies[movie_id]}\") Code: Baseline Mode predicts the mean avg_rating = sum(rating.rating for rating in train) / len(train) baseline_error = sum((rating.rating - avg_rating) ** 2 for rating in test) / len(test) # This is what we hope to do better than assert 1.26 \u003c baseline_error \u003c 1.27 _Code: Create User and Item Embedding (Random Initially) _ from scratch.deep_learning import random_tensor EMBEDDING_DIM = 2 # Find unique ids user_ids = {rating.user_id for rating in ratings} movie_ids = {rating.movie_id for rating in ratings} # Then create a random vector per id user_vectors = {user_id: random_tensor(EMBEDDING_DIM) for user_id in user_ids} movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM) for movie_id in movie_ids} Code: Training Loop from typing import List import tqdm from scratch.linear_algebra import dot def loop(dataset: List[Rating], learning_rate: float = None) -\u003e None: with tqdm.tqdm(dataset) as t: loss = 0.0 for i, rating in enumerate(t): movie_vector = movie_vectors[rating.movie_id] user_vector = user_vectors[rating.user_id] predicted = dot(user_vector, movie_vector) error = predicted - rating.rating loss += error ** 2 if learning_rate is not None: # predicted = m_0 * u_0 + ... + m_k * u_k # So each u_j enters output with coefficent m_j # and each m_j enters output with coefficient u_j user_gradient = [error * m_j for m_j in movie_vector] movie_gradient = [error * u_j for u_j in user_vector] # Take gradient steps for j in range(EMBEDDING_DIM): user_vector[j] -= learning_rate * user_gradient[j] movie_vector[j] -= learning_rate * movie_gradient[j] t.set_description(f\"avg loss: {loss / (i + 1)}\") Code: HyperParameter Tuning Decrease learning rate learning_rate = 0.05 for epoch in range(20): learning_rate *= 0.9 print(epoch, learning_rate) loop(train, learning_rate=learning_rate) loop(validation) loop(test) 24 Database and SQL 24.1 Create and Insert 24.2 Update 24.3 Delete 24.4 Select 24.5 Group By 24.6 Order By 24.7 Join 24.8 Query Optimization 24.9 NoSQL 25 MapReduce 25.1 Example: Word Count 25.2 Why MapReduce 25.3 MapReduce More Generally 25.4 Example: Analyzing Status Update 25.5 Example: Matrix Multiplication 25.6 Combiners","title":"DataScience From Scratch"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/layouts/cloud/cantaloupe/","tags":null,"text":"","title":"Cantaloupe"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/layouts/cloud/banana/","tags":null,"text":"","title":"Banana"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/layouts/cloud/apple/","tags":null,"text":"","title":"Apple"},{"categories":["content","paige"],"date":"0001-01-01T00:00:00Z","description":"An alert.","keywords":null,"link":"/content/alert/","tags":["alerts"],"text":"This page has the following parameters: paige: alert: message: \"Get more information \u003ca href=\\\"#\\\" class=\\\"alert-link\\\"\u003ehere\u003c/a\u003e.\" type: \"primary\"","title":"Alert"},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/scaling-ml-spark/","tags":null,"text":"","title":""},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/learning-go/","tags":null,"text":"","title":""},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/learning-algorithms/","tags":null,"text":"","title":""},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/kubernetes-up-and-running/","tags":null,"text":"","title":""},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/grpc-up-and-running/","tags":null,"text":"","title":""},{"categories":null,"date":"0001-01-01T00:00:00Z","description":"","keywords":null,"link":"/books/software-system/getting-started-with-bazel/","tags":null,"text":"","title":""}]