<!doctype html><html data-paige="Paige theme from https://github.com/willfaught/paige" lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta content="A solid mathematics grasp helps one appreciate the beauty of ML." name=description><meta content="#0d6efd" name=msapplication-TileColor><meta content="/browserconfig.xml" name=msapplication-config><meta content="https://github.com/willfaught/paige" name=theme><meta content="#0d6efd" name=theme-color><meta content="width=device-width,initial-scale=1" name=viewport><meta property="og:url" content="http://localhost:1313/books/machine-learning/essential-math-for-data-science/"><meta property="og:title" content="Essential Math For Data Science"><meta property="og:description" content="A solid mathematics grasp helps one appreciate the beauty of ML."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="books"><meta name=twitter:card content="summary"><meta name=twitter:title content="Essential Math For Data Science"><meta name=twitter:description content="A solid mathematics grasp helps one appreciate the beauty of ML."><title>Essential Math For Data Science · Machine Learning · · Paige</title>
<link href=/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/favicon.ico rel="shortcut icon"><link href=/favicon.png rel=icon type=image/png><link href=/favicon.svg rel=icon type=image/svg+xml><link href=/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link color=#0d6efd href=/safari-pinned-tab.svg rel=mask-icon><link href=/site.webmanifest rel=manifest><link crossorigin=anonymous href=/css/paige/bootstrap/6186ad141d391e4c0c10d2e9cb4eaeaadf75e809-paige.min.5a695f13e40d00e33dc2ef3b080efa570bbcc6991647824214ff6d7f4b20527e.css integrity="sha256-WmlfE+QNAOM9wu87CA76Vwu8xpkWR4JCFP9tf0sgUn4=" referrerpolicy=no-referrer rel=stylesheet><link crossorigin=anonymous href=/css/paige/bootstrap-icons/bootstrap-icons.min.540116b7267d4b2410c74086ae6497566928c6d5715900be7ce7279ad45addcd.css integrity="sha256-VAEWtyZ9SyQQx0CGrmSXVmkoxtVxWQC+fOcnmtRa3c0=" referrerpolicy=no-referrer rel=stylesheet><link crossorigin=anonymous href=/css/paige/katex/katex.min.min.c12d9cca9aa0af743d947ef8c8f5fabaad13e5b4c9fed061fbdf7c9d3c387581.css integrity="sha256-wS2cypqgr3Q9lH74yPX6uq0T5bTJ/tBh+998nTw4dYE=" referrerpolicy=no-referrer rel=stylesheet><style>.paige-figure-numbered{counter-increment:paige-figure-numbered}.paige-figure-numbered>div>figure>figcaption::before{content:"Figure " counter(paige-figure-numbered)": "}.paige-figure-numbered>div>figure>figcaption:empty::before{content:"Figure " counter(paige-figure-numbered)}.paige-header-link{opacity:0;margin-left:.5rem;position:absolute;transition:color .15s ease-in-out,opacity .15s ease-in-out}.paige-header-link::after{content:"#"}.paige-quote .blockquote-footer{margin-top:0}.paige-quote blockquote{border-left:0;border-right:0;margin-bottom:0;padding:0}#paige-content>*{margin-bottom:1rem}blockquote{padding:.5rem 1rem;border-left:.25rem solid var(--bs-border-color);border-right:.25rem solid var(--bs-body-bg)}td,th{padding:.25rem}.highlight .chroma .hl,.highlight .chroma .lnt{display:flex}.paige-figure .paige-quote,.paige-figure .paige-video,.paige-figure .highlight pre.chroma,.paige-figure .highlight .chroma pre,.paige-figure .paige-quote blockquote,.paige-figure figure>div>:last-child,.paige-gallery .paige-figure,.paige-gallery .paige-image,blockquote>p:last-of-type{margin-bottom:0}.paige-figure,.paige-gallery,.paige-image,.paige-quote,.paige-video,table{margin-bottom:1rem}.paige-header-link:focus,.paige-header-link:hover,:hover>.paige-header-link,:target>.paige-header-link{opacity:1}@media(prefers-color-scheme:dark){.bg{color:#c9d1d9;background-color:var(--bs-body-bg)}.chroma{color:#c9d1d9;background-color:var(--bs-body-bg)}.chroma .x{}.chroma .err{color:#f85149}.chroma .cl{}.chroma .lnlinks{outline:none;text-decoration:none;color:inherit}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0}.chroma .hl{background-color:#4f4f4d}.chroma .lnt{white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#64686c}.chroma .ln{white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#6e7681}.chroma .line{display:flex}.chroma .k{color:#ff7b72}.chroma .kc{color:#79c0ff}.chroma .kd{color:#ff7b72}.chroma .kn{color:#ff7b72}.chroma .kp{color:#79c0ff}.chroma .kr{color:#ff7b72}.chroma .kt{color:#ff7b72}.chroma .n{}.chroma .na{}.chroma .nb{}.chroma .bp{}.chroma .nc{color:#f0883e;font-weight:700}.chroma .no{color:#79c0ff;font-weight:700}.chroma .nd{color:#d2a8ff;font-weight:700}.chroma .ni{color:#ffa657}.chroma .ne{color:#f0883e;font-weight:700}.chroma .nf{color:#d2a8ff;font-weight:700}.chroma .fm{}.chroma .nl{color:#79c0ff;font-weight:700}.chroma .nn{color:#ff7b72}.chroma .nx{}.chroma .py{color:#79c0ff}.chroma .nt{color:#7ee787}.chroma .nv{color:#79c0ff}.chroma .vc{}.chroma .vg{}.chroma .vi{}.chroma .vm{}.chroma .l{color:#a5d6ff}.chroma .ld{color:#79c0ff}.chroma .s{color:#a5d6ff}.chroma .sa{color:#79c0ff}.chroma .sb{color:#a5d6ff}.chroma .sc{color:#a5d6ff}.chroma .dl{color:#79c0ff}.chroma .sd{color:#a5d6ff}.chroma .s2{color:#a5d6ff}.chroma .se{color:#79c0ff}.chroma .sh{color:#79c0ff}.chroma .si{color:#a5d6ff}.chroma .sx{color:#a5d6ff}.chroma .sr{color:#79c0ff}.chroma .s1{color:#a5d6ff}.chroma .ss{color:#a5d6ff}.chroma .m{color:#a5d6ff}.chroma .mb{color:#a5d6ff}.chroma .mf{color:#a5d6ff}.chroma .mh{color:#a5d6ff}.chroma .mi{color:#a5d6ff}.chroma .il{color:#a5d6ff}.chroma .mo{color:#a5d6ff}.chroma .o{color:#ff7b72;font-weight:700}.chroma .ow{color:#ff7b72;font-weight:700}.chroma .p{}.chroma .c{color:#8b949e;font-style:italic}.chroma .ch{color:#8b949e;font-style:italic}.chroma .cm{color:#8b949e;font-style:italic}.chroma .c1{color:#8b949e;font-style:italic}.chroma .cs{color:#8b949e;font-weight:700;font-style:italic}.chroma .cp{color:#8b949e;font-weight:700;font-style:italic}.chroma .cpf{color:#8b949e;font-weight:700;font-style:italic}.chroma .g{}.chroma .gd{color:#ffa198;background-color:#490202}.chroma .ge{font-style:italic}.chroma .gr{color:#ffa198}.chroma .gh{color:#79c0ff;font-weight:700}.chroma .gi{color:#56d364;background-color:#0f5323}.chroma .go{color:#8b949e}.chroma .gp{color:#8b949e}.chroma .gs{font-weight:700}.chroma .gu{color:#79c0ff}.chroma .gt{color:#ff7b72}.chroma .gl{text-decoration:underline}.chroma .w{color:#6e7681}}@media(prefers-color-scheme:light){.bg{background-color:#fff}.chroma{background-color:#fff}.chroma .x{}.chroma .err{color:#a61717;background-color:#e3d2d2}.chroma .cl{}.chroma .lnlinks{outline:none;text-decoration:none;color:inherit}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0}.chroma .hl{background-color:#ffc}.chroma .lnt{white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .line{display:flex}.chroma .k{color:#000;font-weight:700}.chroma .kc{color:#000;font-weight:700}.chroma .kd{color:#000;font-weight:700}.chroma .kn{color:#000;font-weight:700}.chroma .kp{color:#000;font-weight:700}.chroma .kr{color:#000;font-weight:700}.chroma .kt{color:#458;font-weight:700}.chroma .n{}.chroma .na{color:teal}.chroma .nb{color:#0086b3}.chroma .bp{color:#999}.chroma .nc{color:#458;font-weight:700}.chroma .no{color:teal}.chroma .nd{color:#3c5d5d;font-weight:700}.chroma .ni{color:purple}.chroma .ne{color:#900;font-weight:700}.chroma .nf{color:#900;font-weight:700}.chroma .fm{}.chroma .nl{color:#900;font-weight:700}.chroma .nn{color:#555}.chroma .nx{}.chroma .py{}.chroma .nt{color:navy}.chroma .nv{color:teal}.chroma .vc{color:teal}.chroma .vg{color:teal}.chroma .vi{color:teal}.chroma .vm{}.chroma .l{}.chroma .ld{}.chroma .s{color:#d14}.chroma .sa{color:#d14}.chroma .sb{color:#d14}.chroma .sc{color:#d14}.chroma .dl{color:#d14}.chroma .sd{color:#d14}.chroma .s2{color:#d14}.chroma .se{color:#d14}.chroma .sh{color:#d14}.chroma .si{color:#d14}.chroma .sx{color:#d14}.chroma .sr{color:#009926}.chroma .s1{color:#d14}.chroma .ss{color:#990073}.chroma .m{color:#099}.chroma .mb{color:#099}.chroma .mf{color:#099}.chroma .mh{color:#099}.chroma .mi{color:#099}.chroma .il{color:#099}.chroma .mo{color:#099}.chroma .o{color:#000;font-weight:700}.chroma .ow{color:#000;font-weight:700}.chroma .p{}.chroma .c{color:#998;font-style:italic}.chroma .ch{color:#998;font-style:italic}.chroma .cm{color:#998;font-style:italic}.chroma .c1{color:#998;font-style:italic}.chroma .cs{color:#999;font-weight:700;font-style:italic}.chroma .cp{color:#999;font-weight:700;font-style:italic}.chroma .cpf{color:#999;font-weight:700;font-style:italic}.chroma .g{}.chroma .gd{color:#000;background-color:#fdd}.chroma .ge{color:#000;font-style:italic}.chroma .gr{color:#a00}.chroma .gh{color:#999}.chroma .gi{color:#000;background-color:#dfd}.chroma .go{color:#888}.chroma .gp{color:#555}.chroma .gs{font-weight:700}.chroma .gu{color:#aaa}.chroma .gt{color:#a00}.chroma .gl{text-decoration:underline}.chroma .w{color:#bbb}}@media(prefers-reduced-motion:reduce){.paige-header-link{transition:none}}</style></head><body class="d-flex flex-column"><div class="container flex-fill" id=paige-root><div class=row><div class=col><header class=my-3 id=paige-header><nav><ul class="align-items-center justify-content-center nav"><li class=nav-item><a class="nav-link text-decoration-underline" href=/>Home</a></li><li class=nav-item><a aria-current=page class="active text-body-emphasis nav-link text-decoration-underline" href=/books/>Books</a></li><li class=nav-item><a class="nav-link text-decoration-underline" href=/blogs/>Blogs</a></li><li class=nav-item><a class="nav-link text-decoration-underline" href=/resume/>Resume</a></li><li class=nav-item><a class="nav-link text-decoration-underline" href=/github/>Github</a></li><li class=nav-item><a class="nav-link text-decoration-underline" href=/search/>Search</a></li></ul></nav></header><main class=mt-3 id=paige-main><article class="paige-published paige-single" id=paige-article><div class="align-items-center d-flex flex-column mb-0"><div class=mw-100 id=paige-metadata><h1 class="fw-bold text-center" id=paige-title>Essential Math For Data Science</h1><p class="lead text-center" id=paige-description>A solid mathematics grasp helps one appreciate the beauty of ML.</p><div class=mb-3><p class="mb-0 text-center text-secondary" id=paige-reading-time>29 minutes</p></div></div><div class=mw-100 id=paige-toc><div class="border mb-3 pe-3 ps-3 pt-3 rounded"><nav id=TableOfContents><ol><li><ol><li><a href=#exponential>Exponential</a></li><li><a href=#logarithms>Logarithms</a></li><li><a href=#eulers-number-e-and-natural-logarithm>Euler&rsquo;s Number $e$ and Natural Logarithm</a></li><li><a href=#limit>Limit</a></li><li><a href=#derivatives>Derivatives</a></li><li><a href=#chain-rule>Chain Rule</a></li><li><a href=#integrals>Integrals</a></li></ol></li></ol><ol><li><ol><li><a href=#21-understanding-probability>2.1 Understanding Probability</a></li><li><a href=#22-probability-math>2.2 Probability Math</a></li><li><a href=#23-binomial-probability-distribution>2.3 Binomial Probability Distribution</a></li><li><a href=#24-beta-probability-distribution>2.4 Beta Probability Distribution</a></li></ol></li></ol><ol><li><ol><li><a href=#31-what-is-data>3.1 What is Data</a></li><li><a href=#32-descriptive-vs-inferential-statistics>3.2 Descriptive vs Inferential Statistics</a></li><li><a href=#33-populations-samples-and-bias>3.3 Populations, Samples, and Bias</a></li><li><a href=#34-descriptive-statistics>3.4 Descriptive Statistics</a></li><li><a href=#35-inferential-statistics>3.5 Inferential Statistics</a></li><li><a href=#36-t-distribution-dealing-with-small-samples>3.6 T-Distribution: Dealing with Small Samples</a></li><li><a href=#37-big-data-considerations-and-the-texas-sharpshooter-fallacy>3.7 Big Data Considerations and the Texas Sharpshooter Fallacy</a></li></ol></li></ol><ol><li><ol><li><a href=#41-what-is-a-vector>4.1 What is a Vector?</a></li><li><a href=#43-matrix-vector-multiplication>4.3 Matrix Vector Multiplication</a></li><li><a href=#44-determinants>4.4 Determinants</a></li><li><a href=#45-special-type-of-matrices>4.5 Special Type of Matrices</a></li><li><a href=#46-system-of-equations-and-inverse-matrices>4.6 System of Equations and Inverse Matrices</a></li></ol></li><li><a href=#linear-algebra-is-used-to-solve-a-system-equation-via-inverse-matrix>Linear algebra is used to solve a system equation via inverse matrix</a><ol><li><a href=#47-eigenvectors-and-eigenvalues>4.7 Eigenvectors and Eigenvalues</a></li></ol></li></ol><ol><li><ol><li><a href=#51-basic-linear-regression>5.1 Basic Linear Regression</a></li><li><a href=#52-residuals-and-squared-errors>5.2 Residuals and Squared Errors</a></li><li><a href=#53-finding-the-best-fit-line>5.3 Finding the Best Fit Line</a><ol><li><a href=#531-closed-form>5.3.1 closed form</a></li><li><a href=#532-via-linear-algebra-inverse-matrix-technique>5.3.2 Via Linear Algebra (Inverse Matrix Technique)</a></li><li><a href=#533-gradient-descent>5.3.3 Gradient Descent</a></li></ol></li><li><a href=#54-overfitting-and-variance>5.4 Overfitting and Variance</a></li><li><a href=#55-stochasitc-gradient-descent>5.5 Stochasitc Gradient Descent</a></li><li><a href=#56-correlation-coefficient>5.6 Correlation Coefficient</a></li><li><a href=#57-statistical-significance>5.7 Statistical Significance</a></li><li><a href=#58-conefficient-of-determination>5.8 Conefficient of Determination</a></li><li><a href=#59-standard-error-of-the-estimate>5.9 Standard Error of the Estimate</a></li><li><a href=#510-prediction-intervals>5.10 Prediction Intervals</a></li><li><a href=#511-traintest-splits>5.11 Train/Test Splits</a></li><li><a href=#512-multiple-linear-regressions>5.12 Multiple Linear Regressions</a></li></ol></li></ol><ol><li><ol><li><a href=#61-understanding-logistic-regression>6.1 Understanding Logistic Regression</a></li><li><a href=#62-performing-logistic-regression>6.2 Performing Logistic Regression</a></li><li><a href=#621-finding-the-coeffcients>6.2.1 Finding the Coeffcients</a></li><li><a href=#63-multivariable-logistic-regression>6.3 Multivariable Logistic Regression</a></li><li><a href=#64-understanding-the-log-odds>6.4 Understanding the Log-Odds</a></li><li><a href=#65-r-squared>6.5 R-Squared</a></li><li><a href=#66-p-values>6.6 P-Values</a></li><li><a href=#67-traintest-splits>6.7 Train/Test Splits</a></li><li><a href=#68-confusion-matrices>6.8 Confusion Matrices</a></li><li><a href=#69-bayes-theorem-and-classification>6.9 Bayes Theorem and Classification</a></li><li><a href=#610-receiver-operator-characteristicsarea-under-curve>6.10 Receiver Operator Characteristics/Area Under Curve</a></li><li><a href=#611-class-imbalance>6.11 Class Imbalance</a></li></ol></li></ol><ol><li><ol><li><a href=#71-when-to-use-neural-network-and-deep-learning>7.1 When To Use Neural Network and Deep Learning</a></li><li><a href=#72-simple-neural-network>7.2 Simple Neural Network</a><ol><li><a href=#721-activation-function>7.2.1 Activation Function</a></li><li><a href=#722-forward-propagation>7.2.2 Forward Propagation</a></li></ol></li><li><a href=#73-backpropagation>7.3 Backpropagation</a><ol><li><a href=#731-calculate-the-weight-and-bias-derivatives>7.3.1 Calculate the Weight and Bias Derivatives</a></li><li><a href=#732-stochastic-gradient-descent>7.3.2 Stochastic Gradient Descent</a></li></ol></li><li><a href=#74-limitation-of-neural-networks-and-deep-learning>7.4 Limitation of Neural Networks and Deep Learning</a></li><li><a href=#75-conclusion>7.5 Conclusion</a></li></ol></li></ol></nav></div></div><div class=mw-100 id=paige-content><h1 id=1-basic-math-and-calculus-review>1 Basic Math And Calculus Review<a aria-label="Link to this section" class=paige-header-link href=#1-basic-math-and-calculus-review></a></h1><h3 id=exponential>Exponential<a aria-label="Link to this section" class=paige-header-link href=#exponential></a></h3><h3 id=logarithms>Logarithms<a aria-label="Link to this section" class=paige-header-link href=#logarithms></a></h3><p>$$
log_2 8 = x</p><p>$$</p><ul><li><p>base = 2</p></li><li><p>x = 3</p><pre><code>- Benefits
    - Logorithms simplifies operations, which helps computation training
  	  - Multiplication: log(a x b) = log(a) + log(b)
  	  - Division: log(a/b) = log(a) - log(b)
  	  - Expoentiation log(a ^ n) = n * log(a)
  	  - Inverse: log(x^-1) == log(1/x) = -log(x)
    - Log is also good for feature engineering, transforming heavy tail distribution to a gaussian distribution, which is ideal for linear regression models

- Code
</code></pre></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>log</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># 2 raised to what power gives me 8?</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># x = 3.0</span>
</span></span></code></pre></div><h3 id=eulers-number-e-and-natural-logarithm>Euler&rsquo;s Number $e$ and Natural Logarithm<a aria-label="Link to this section" class=paige-header-link href=#eulers-number-e-and-natural-logarithm></a></h3><ul><li><p>$e$ = 2.71828 = euler number;</p><ul><li>e is a constant like $\pi$<ul><li>$e=(1 + 1/n)^n$<ul><li>as n &ndash;> $\infty$, e approaches 2.71828</li></ul></li><li>Application<ul><li>To describe normal distribtuion</li><li>model logistic regression</li><li>Given equation to calculate compound interest, we can simplify it by using $e$
$$A = P *(1 + r/n)^{nt} $$
$$A=P * e^{rt}$$</li></ul></li></ul></li></ul></li><li><p>Natural logarithm</p><ul><li>when use use e as the ==base== of our logarithm; $e$ is the default base for logarithm
$$log_e(10) = ln(10)$$</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>log</span><span class=p>,</span> <span class=n>e</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># e raised to what power gives 10?</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x_detailed</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1>#x = 2.3025</span>
</span></span></code></pre></div><h3 id=limit>Limit<a aria-label="Link to this section" class=paige-header-link href=#limit></a></h3><ul><li>Limit express a value $\infty$ that is forever being approached, but never reached
$$lim_{x \to \infty}1/x = 0$$</li></ul><h3 id=derivatives>Derivatives<a aria-label="Link to this section" class=paige-header-link href=#derivatives></a></h3><ul><li>A derivative tells the slope of a function, and it is useful to measure the rate of change at any point in a function.
$$ f(x) = x ^2 $$
$$ \frac{\partial }{\partial x}f(x) = \frac{\partial}{\partial x}x^2 = 2x $$</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>derivative_x</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>step_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	    <span class=n>m</span> <span class=o>=</span> <span class=p>(</span><span class=n>f</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>step_size</span><span class=p>)</span> <span class=o>-</span> <span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=o>/</span> <span class=p>((</span><span class=n>x</span> <span class=o>+</span> <span class=n>step_size</span><span class=p>)</span> <span class=o>-</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	    <span class=k>return</span> <span class=n>m</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>my_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	    <span class=k>return</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>derivative_x</span><span class=p>(</span><span class=n>my_function</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mf>.00001</span><span class=p>)</span> <span class=c1>#4.0000100</span>
</span></span></code></pre></div><ul><li>partial derivatives<ul><li>Partial derivative enables use to find a slope with respect to multiple variables in several directions<ul><li>Suppose u depends on 3 variables x, y, z. To find the slope of u,
$$\frac{\partial u}{\partial t}
= h^2 \left( \frac{\partial^2 u}{\partial x^2}</li></ul><ul><li>\frac{\partial^2 u}{\partial y^2}</li><li>\frac{\partial^2 u}{\partial z^2} \right)$$</li></ul></li></ul></li></ul><h3 id=chain-rule>Chain Rule<a aria-label="Link to this section" class=paige-header-link href=#chain-rule></a></h3><ul><li><p>Suppose we have 2 equations that are related like so
$$ y = x^2 + 1$$
$$z = y^3 - 2$$
-To find $\frac{\partial}{\partial y}z$
$$ \frac{\partial z}{\partial x} = \frac{\partial z} {\partial y} \frac{\partial y}{\partial x} $$</p></li><li><p>Why do we care?</p><ul><li>In neural network layers, chain rule enables se to &ldquo;untangle&rdquo; the derivative from each layer</li></ul></li></ul><h3 id=integrals>Integrals<a aria-label="Link to this section" class=paige-header-link href=#integrals></a></h3><ul><li>integrations find the area under a curve</li></ul><h1 id=2-probability>2 Probability<a aria-label="Link to this section" class=paige-header-link href=#2-probability></a></h1><h3 id=21-understanding-probability>2.1 Understanding Probability<a aria-label="Link to this section" class=paige-header-link href=#21-understanding-probability></a></h3><ul><li>Statistics vs Probability<ul><li>Statistics is the application of probability to data<ul><li>Ex: data distribtuion will impact what probability tool/approach we take</li></ul></li></ul></li></ul><h3 id=22-probability-math>2.2 Probability Math<a aria-label="Link to this section" class=paige-header-link href=#22-probability-math></a></h3><ul><li><p>Joint probability</p><ul><li>what is the probability event_A ==and== event_B happen ==together== ?</li><li>P(A and B) = P(A) x P(B)</li></ul></li><li><p>Union probability</p><ul><li>what is the probability event_A ==or== event_B happen?</li><li>P(A or B) = P(A) + P(B)</li></ul></li><li><p>Conditional Probability and Baye&rsquo;s Theorem</p><ul><li>P (A | B) = P(B | A) * P(A)/ P(B)</li><li>Bayes theorem  can be used to chain several conditional probabilities together to keep updating our beliefs based on new information</li><li>Ex:<ul><li>Given P(Cofee| Cancer) = 0.85, what is P(cancer | coffee?)</li><li>P(cancer|coffee) = P(coffee | cancer)* P(coffee) / P(cancer)</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=n>p_coffee_drinker</span> <span class=o>=</span> <span class=mf>.65</span>
</span></span><span class=line><span class=cl>	<span class=n>p_cancer</span> <span class=o>=</span> <span class=mf>.005</span>
</span></span><span class=line><span class=cl>	<span class=n>p_coffee_drinker_given_cancer</span> <span class=o>=</span> <span class=mf>.85</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>p_cancer_given_coffee_drinker</span> <span class=o>=</span> <span class=n>p_coffee_drinker_given_cancer</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>	    <span class=n>p_cancer</span> <span class=o>/</span> <span class=n>p_coffee_drinker</span>
</span></span></code></pre></div><ul><li>Joint and Union Conditional Probability<ul><li>P(A and B) = P(A) x P(A|B)</li><li>P(A or B) = P(A) + P(B) - P(A|B) * P(B)</li></ul></li></ul><h3 id=23-binomial-probability-distribution>2.3 Binomial Probability Distribution<a aria-label="Link to this section" class=paige-header-link href=#23-binomial-probability-distribution></a></h3><ul><li>Binomial distribution measures how likely ==k== successes can happen out of ==n== trials given we expect a ==p== probability</li><li>Graph:<ul><li>x-axis = num successes = k</li><li>y-axis = expected p probabilty</li></ul></li><li>Example<ul><li>We run 10 tests, which either pass or fail (ie binomial) and has a success rate of 90%. What is the probabibility of having 8 successes?<ul><li>n=10; p=0.9; k=8</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>binom</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>n</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>	<span class=n>p</span> <span class=o>=</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	    <span class=n>probability</span> <span class=o>=</span> <span class=n>binom</span><span class=o>.</span><span class=n>pmf</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>{0}</span><span class=s2> - </span><span class=si>{1}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>probability</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># OUTPUT:	</span>
</span></span><span class=line><span class=cl>	<span class=c1># 0 - 9.99999999999996e-11</span>
</span></span><span class=line><span class=cl>	<span class=c1># 1 - 8.999999999999996e-09</span>
</span></span><span class=line><span class=cl>	<span class=c1># 2 - 3.644999999999996e-07</span>
</span></span><span class=line><span class=cl>	<span class=c1># 3 - 8.748000000000003e-06</span>
</span></span><span class=line><span class=cl>	<span class=c1># 4 - 0.0001377809999999999</span>
</span></span><span class=line><span class=cl>	<span class=c1># 5 - 0.0014880347999999988</span>
</span></span><span class=line><span class=cl>	<span class=c1># 6 - 0.011160260999999996</span>
</span></span><span class=line><span class=cl>	<span class=c1># 7 - 0.05739562800000001</span>
</span></span><span class=line><span class=cl>	<span class=c1># 8 - 0.19371024449999993</span>
</span></span><span class=line><span class=cl>	<span class=c1># 9 - 0.38742048900000037</span>
</span></span><span class=line><span class=cl>	<span class=c1># 10 - 0.34867844010000004</span>
</span></span></code></pre></div><h3 id=24-beta-probability-distribution>2.4 Beta Probability Distribution<a aria-label="Link to this section" class=paige-header-link href=#24-beta-probability-distribution></a></h3><ul><li>Theta distribtion allows us to calculate the probability of a probability (yikes!)<ul><li>In previous example, what is the probability that p=90%? Beta distribution can help anser that question.</li></ul></li><li>Like all probability distribution, the area under the curve adds up to 1.</li><li>graph<ul><li>Given 8 successes and 2 failures<ul><li>x-axis = p</li><li>y-axis = likelihood of p</li></ul></li></ul></li></ul><h1 id=3-descriptive-and-inferential-statistics>3 Descriptive and Inferential Statistics<a aria-label="Link to this section" class=paige-header-link href=#3-descriptive-and-inferential-statistics></a></h1><h3 id=31-what-is-data>3.1 What is Data<a aria-label="Link to this section" class=paige-header-link href=#31-what-is-data></a></h3><ul><li>==Data does not capture context or explanations==<ul><li>data provides clues, and not truths</li><li>data may lead us to truth or mislead into erronous conclusions<ul><li>==I need to be curious how the data was created, who create it, and what the data is not capturing==<ul><li>only 13% of ML projects succeeds</li></ul></li></ul></li></ul></li></ul><h3 id=32-descriptive-vs-inferential-statistics>3.2 Descriptive vs Inferential Statistics<a aria-label="Link to this section" class=paige-header-link href=#32-descriptive-vs-inferential-statistics></a></h3><ul><li>Descriptive statistics summarizes the data<ul><li>Ex: median, mean, , variance, standard deviation</li></ul></li><li>Inferential statistics attemps to uncover attributes of the ==larger== population based on ==a few samples==.<ul><li>Ex: p-values, confidence interval, central limit theorem</li></ul></li></ul><h3 id=33-populations-samples-and-bias>3.3 Populations, Samples, and Bias<a aria-label="Link to this section" class=paige-header-link href=#33-populations-samples-and-bias></a></h3><ul><li>A population is a particular group of interest we want to study</li><li>A sample is a subset of the population that is ideally random and unbiased</li><li>biases<ul><li>different types<ul><li>confirmation bias: capturing data that supports your belief</li><li>survival bias: caputre only the living subjects</li></ul></li></ul></li></ul><h3 id=34-descriptive-statistics>3.4 Descriptive Statistics<a aria-label="Link to this section" class=paige-header-link href=#34-descriptive-statistics></a></h3><ul><li><p>mean vs weighted mean</p><ul><li>mean<ul><li><p>over sample n: $$\overline{x}=\sum_{i=0}^{n}\frac{x_i}{n}$$</p></li><li><p>over entire population N:
$$\mu=\sum_{i=0}^{N}\frac{x_i}{N}$$
$$\mu = \frac{x_1 + x_2 + .. + x_N}{N} $$</p></li><li><p>weighted mean
$$ weightedmean =\frac{x_1<em>w_1 + x_2</em>w_2 + .. + x_n*w_n}{w_1 + w_2 + .. + w_n} $$</p></li></ul></li></ul></li><li><p>Variance vs standard deviation</p><ul><li>variance : $$\sigma^2=\frac{\sum_{i=0}^{N}(x_i - \mu)^2}{N}$$</li><li>standard deviation = $$s^2=\frac{\sum_{i=0}^{n}(x_i - \overline{x})^2}{n-1} $$</li></ul></li><li><p>Normal distribution</p><ul><li>Why normal distribution is useful<ul><li>It’s symmetrical; both sides are identically mirrored at the mean, which is the center.</li><li>Most mass is at the center around the mean.</li><li>It has a spread (being narrow or wide) that is specified by standard deviation.</li><li>The “tails” are the least likely outcomes and approach zero infinitely but never touch zero</li><li>It resembles a lot of phenomena in nature and daily life, and ==even generalizes nonnormal problems because of the central limit theorem==, which we will talk about shortly.</li></ul></li><li>Probabibilty Density Function (PDF) vs Cumulative Density Function (CDF)<ul><li>PDF for normal distribution $$f(x)=\frac{1}{\sigma}<em>(2</em>\pi)^{0.5}<em>e^{-\frac{1}{2}</em>(\frac{x-\mu^2}{\sigma})} $$</li><li>PDF returns ==liklihood== and NOT probabibility<ul><li>Probability is the area under the PDF</li><li>CDF is the area of the PDF as the x axis value increases; so CDF can be used to return the probability for x &lt; some value.</li></ul></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>mean</span> <span class=o>=</span> <span class=mf>64.43</span> <span class=c1># mean weight of golden retreivers</span>
</span></span><span class=line><span class=cl>		<span class=n>std_dev</span> <span class=o>=</span> <span class=mf>2.99</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>norm</span><span class=o>.</span><span class=n>cdf</span><span class=p>(</span><span class=mf>64.43</span><span class=p>,</span> <span class=n>mean</span><span class=p>,</span> <span class=n>std_dev</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># prints 0.5; means 50% of the area, ie </span>
</span></span></code></pre></div><pre><code>		- Inverse CDF (ppf)
			- There will be situations where we need to look up an area on the CDF and then return the corresponding x-value. This is a backward usage of the CDF
				- CDF: x-axis --&gt; area/probability
				- Inverse CDF: area/probabibility --&gt; x-axis
				- We use the ppf
				- Example:
					- I want to find the weight that 95% of golden retrievers fall under
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>				<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>				
</span></span><span class=line><span class=cl>				<span class=n>x</span> <span class=o>=</span> <span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=mf>.95</span><span class=p>,</span> <span class=n>loc</span><span class=o>=</span><span class=mf>64.43</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=mf>2.99</span><span class=p>)</span>
</span></span><span class=line><span class=cl>				<span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># 69.3481123445849 lb</span>
</span></span></code></pre></div><pre><code>	- z score
		- use to rescale the normal distribution so the mean is 0 and the standard devation is 1. This makes it easy to compare the spread of one normal distribution to another normal distribution, even if they have different means and variances.
</code></pre><h3 id=35-inferential-statistics>3.5 Inferential Statistics<a aria-label="Link to this section" class=paige-header-link href=#35-inferential-statistics></a></h3><ul><li><p>Central Limit Theorem</p><ul><li>As the population N size increases, even if that population does not follow a normal distribution, the normal distribution still makes an appearance. WOW.</li></ul></li><li><p>Confidence Interval</p><ul><li>A confidence interval is a range calculation showing how confidently we believe a sample mean (or other parameter) falls in a range for the population mean.<ul><li>Example:
<code>Based on a sample of 31 golden retrievers with a sample mean of 64.408 and a sample standard deviation of 2.05, I am 95% confident that the population mean lies between 63.686 and 65.1296</code></li></ul></li><li>IMOW:<ul><li>you find the x value which encomposes 95% of the PDF area</li></ul></li><li>Code<ol><li>First find the critical z value $z_c$ which captures 95% of the PDF curve area</li></ol></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>			<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>			<span class=k>def</span> <span class=nf>critical_z_value</span><span class=p>(</span><span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			    <span class=n>norm_dist</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			    <span class=n>left_tail_area</span> <span class=o>=</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span>
</span></span><span class=line><span class=cl>			    <span class=n>upper_area</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>-</span> <span class=p>((</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			    <span class=k>return</span> <span class=n>norm_dist</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=n>left_tail_area</span><span class=p>),</span> <span class=n>norm_dist</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=n>upper_area</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			
</span></span><span class=line><span class=cl>			<span class=nb>print</span><span class=p>(</span><span class=n>critical_z_value</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>.95</span><span class=p>))</span>
</span></span><span class=line><span class=cl>			<span class=c1># (-1.959963984540054, 1.959963984540054)</span>
</span></span></code></pre></div><pre><code>	     2. Use Central limit theorem to proce margein of error, which is the range around the sample mean that contains the population mean at that level of confidence. 
</code></pre><p>$$ E = \frac{+}{-} z_c * \frac{s}{n^{0.5}} $$</p><p>$$ 95% confdence = \mu \frac{+}{} E $$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>			<span class=k>def</span> <span class=nf>confidence_interval</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>sample_mean</span><span class=p>,</span> <span class=n>sample_std</span><span class=p>,</span> <span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			    <span class=c1># Sample size must be greater than 30</span>
</span></span><span class=line><span class=cl>			
</span></span><span class=line><span class=cl>			    <span class=n>lower</span><span class=p>,</span> <span class=n>upper</span> <span class=o>=</span> <span class=n>critical_z_value</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			    <span class=n>lower_ci</span> <span class=o>=</span> <span class=n>lower</span> <span class=o>*</span> <span class=p>(</span><span class=n>sample_std</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>n</span><span class=p>))</span>
</span></span><span class=line><span class=cl>			    <span class=n>upper_ci</span> <span class=o>=</span> <span class=n>upper</span> <span class=o>*</span> <span class=p>(</span><span class=n>sample_std</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>n</span><span class=p>))</span>
</span></span><span class=line><span class=cl>			
</span></span><span class=line><span class=cl>			    <span class=k>return</span> <span class=n>sample_mean</span> <span class=o>+</span> <span class=n>lower_ci</span><span class=p>,</span> <span class=n>sample_mean</span> <span class=o>+</span> <span class=n>upper_ci</span>
</span></span><span class=line><span class=cl>			
</span></span><span class=line><span class=cl>			<span class=nb>print</span><span class=p>(</span><span class=n>confidence_interval</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>.95</span><span class=p>,</span> <span class=n>sample_mean</span><span class=o>=</span><span class=mf>64.408</span><span class=p>,</span> <span class=n>sample_std</span><span class=o>=</span><span class=mf>2.05</span><span class=p>,</span> <span class=n>n</span><span class=o>=</span><span class=mi>31</span><span class=p>))</span>
</span></span><span class=line><span class=cl>			<span class=c1># (63.68635915701992, 65.12964084298008)</span>
</span></span></code></pre></div><ul><li><p>Understanding P Values</p><ul><li>p value is the probability of something occurring by chance rather than because of a hypothesized explanation</li><li>In the context of an experiment,<ul><li>Given<ul><li>our control variable, ie new model</li><li>the result of something happening, ie increase in click rate</li></ul></li><li>A p value is low, ie &lt; 5%,<ul><li>means we ==discard== the $H_0$ null hypothesis<ul><li>$H_0$ null hypotheseis states our variable has no impact; the result is random choice</li></ul></li><li>means we accept the $H_1$ alternative hypothesis, which states the control variable is the reason behind the result.</li></ul></li></ul></li></ul></li><li><p>Hypothesis Testing</p><ul><li>Example: does our pill (the variable) reduce the lenght of a fever?<ul><li>Data<ul><li>fever recovery is gaussian<ul><li>mean = 18 days</li><li>std = 1.5</li></ul></li></ul></li><li>Experiment:<ul><li>we give drug to 40 people and it took 16 days to recover. Did drug have impact?</li></ul></li><li>One tail test<ul><li>to be statistically significant, p &lt; 0.05. What is x, the number of day?</li></ul></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>				<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>				
</span></span><span class=line><span class=cl>				<span class=c1># Cold has 18 day mean recovery, 1.5 std dev</span>
</span></span><span class=line><span class=cl>				<span class=n>mean</span> <span class=o>=</span> <span class=mi>18</span>
</span></span><span class=line><span class=cl>				<span class=n>std_dev</span> <span class=o>=</span> <span class=mf>1.5</span>
</span></span><span class=line><span class=cl>				
</span></span><span class=line><span class=cl>				<span class=c1># What x-value has 5% of area behind it?</span>
</span></span><span class=line><span class=cl>				<span class=n>x</span> <span class=o>=</span> <span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=mf>.05</span><span class=p>,</span> <span class=n>mean</span><span class=p>,</span> <span class=n>std_dev</span><span class=p>)</span>
</span></span><span class=line><span class=cl>				
</span></span><span class=line><span class=cl>				<span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># 15.53271955957279</span>
</span></span></code></pre></div><pre><code>					- Ans= 15.5, which is less than our experiment mean recovery of 16 days, which means our pill is not statistically significant
</code></pre><h3 id=36-t-distribution-dealing-with-small-samples>3.6 T-Distribution: Dealing with Small Samples<a aria-label="Link to this section" class=paige-header-link href=#36-t-distribution-dealing-with-small-samples></a></h3><h3 id=37-big-data-considerations-and-the-texas-sharpshooter-fallacy>3.7 Big Data Considerations and the Texas Sharpshooter Fallacy<a aria-label="Link to this section" class=paige-header-link href=#37-big-data-considerations-and-the-texas-sharpshooter-fallacy></a></h3><h1 id=4-linear-algebra>4 Linear Algebra<a aria-label="Link to this section" class=paige-header-link href=#4-linear-algebra></a></h1><h3 id=41-what-is-a-vector>4.1 What is a Vector?<a aria-label="Link to this section" class=paige-header-link href=#41-what-is-a-vector></a></h3><ul><li>A vector is an arrow in space with a specific direction and length, often representing a piece of data</li><li>Applications of vectors<ul><li>Solvers like the one in Excel or Python PuLP use linear programming, which uses vectors to ==maximize a solution== while meeting those constraints</li><li>Computer graphics <a href=https://docs.manim.community/en/stable/examples.html>manim library</a></li></ul></li><li>Representation<ul><li>v = [x, y, z]<ul><li>implicit to the origin</li></ul></li><li>code</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>		<span class=n>v_nump</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>v_python_list</span> <span class=o>=</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span>
</span></span></code></pre></div><ul><li>Adding vectors<ul><li>IMOW: the origin of the second vector is the end of the 1st vector</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>   <span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>array</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   <span class=n>v</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>3</span><span class=p>,</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>   <span class=n>w</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   <span class=c1># sum the vectors</span>
</span></span><span class=line><span class=cl>   <span class=n>v_plus_w</span> <span class=o>=</span> <span class=n>v</span> <span class=o>+</span> <span class=n>w</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   <span class=c1># display summed vector</span>
</span></span><span class=line><span class=cl>   <span class=nb>print</span><span class=p>(</span><span class=n>v_plus_w</span><span class=p>)</span> <span class=c1># [5, 1]</span>
</span></span></code></pre></div></li></ul><pre tabindex=0><code>


- Scaling vectors
	- IMOW: scales the vector lengh 



- Span and Linear Dependence
	- ==Two vectors in opposite directions (ie one of their cordinate has opposite signs) can be used to create an inifinite spans through multiplication and adding==
	- Terminologies
		- span : the whole spae of possible vectors
		- linear indepdenent: two vectors in different directions (ie one of their coordinate has opposite sign)
		- linear depedent: two vectors in the same direction (or plane for multi-dim vector) 
	- So what?
		- Linear ==independent== vectors can create an infinite span
		- When solving system of equations, linear independent vectors gives us more &#34;flexibility&#34;, which is important to solve the system of equations.
			- Conversely, linear dependent vectors are 
		- Later on, we will learn determinant, which is a tool to check for linear dependence 

### 4.2 Linear Transformations
- Basis Vectors $\hat{i}$ and $\hat{j}$
	- properties
		- lenght of 1
		- perpendicular to earch other
	- We can use $\hat{i}$ and $\hat{j}$  to create any vector we want by scaling and adding them.
	- Ex
``` python
		     i j
	basis = [1 0]
			[0 1]
</code></pre><h3 id=43-matrix-vector-multiplication>4.3 Matrix Vector Multiplication<a aria-label="Link to this section" class=paige-header-link href=#43-matrix-vector-multiplication></a></h3><pre><code>- This transformation of a vector by applying basis vectors is known as _matrix vector multiplication_
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>			    <span class=n>i</span> <span class=n>j</span>
</span></span><span class=line><span class=cl>		<span class=p>[</span><span class=n>x</span><span class=s1>&#39;] = [a b] [x]</span>
</span></span><span class=line><span class=cl>		<span class=p>[</span><span class=n>y</span><span class=s1>&#39;] = [c d] [y]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=p>[</span><span class=n>x</span><span class=s1>&#39;] = [ax + by]</span>
</span></span><span class=line><span class=cl>		<span class=p>[</span><span class=n>y</span><span class=s1>&#39;] = [cx + dy]</span>
</span></span><span class=line><span class=cl>		 
</span></span><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>array</span>
</span></span><span class=line><span class=cl>		<span class=c1># Declare i-hat and j-hat</span>
</span></span><span class=line><span class=cl>		<span class=n>i_hat</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=n>j_hat</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># compose basis matrix using i-hat and j-hat</span>
</span></span><span class=line><span class=cl>		<span class=c1># also need to transpose rows into columns</span>
</span></span><span class=line><span class=cl>		<span class=n>basis</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=n>i_hat</span><span class=p>,</span> <span class=n>j_hat</span><span class=p>])</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># declare vector v</span>
</span></span><span class=line><span class=cl>		<span class=n>v</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># create new vector</span>
</span></span><span class=line><span class=cl>		<span class=c1># by transforming v with dot product</span>
</span></span><span class=line><span class=cl>		<span class=n>new_v</span> <span class=o>=</span> <span class=n>basis</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=n>new_v</span><span class=p>)</span> <span class=c1># [2, 3]</span>
</span></span></code></pre></div><h3 id=44-determinants>4.4 Determinants<a aria-label="Link to this section" class=paige-header-link href=#44-determinants></a></h3><ul><li>During linar transformation, we can increase,decrease,rotate the original spacea area by X times. The X times is the ==determinant==</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>numpy.linalg</span> <span class=kn>import</span> <span class=n>det</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>array</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>i_hat</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>3</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=n>j_hat</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>basis</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span><span class=n>i_hat</span><span class=p>,</span> <span class=n>j_hat</span><span class=p>])</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>determinant</span> <span class=o>=</span> <span class=n>det</span><span class=p>(</span><span class=n>basis</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=n>determinant</span><span class=p>)</span> <span class=c1># prints 6.0; we increase area by 6.  Other transformations can rotate</span>
</span></span></code></pre></div><h3 id=45-special-type-of-matrices>4.5 Special Type of Matrices<a aria-label="Link to this section" class=paige-header-link href=#45-special-type-of-matrices></a></h3><ul><li>Square matrix:<ul><li>numRows = numCols</li><li>application: eigendecomposition</li></ul></li><li>Identity matrix<ul><li>diagonal are 1&rsquo;s; everything else is 0</li><li>application:<ul><li>when you have an identity matrix, you essentially have undone a transformation and found your starting basis vectors $\hat{i}$ and $\hat{j}$. This will play a big role in solving systems of equations in the next section.</li></ul></li></ul></li><li>Inverse matrix<ul><li>application: inverse matrix undoes the transformation of another matrix</li><li>$A^{-1}$ x $A^{1}$ = identity matrix</li></ul></li><li>Diagonal matrix<ul><li>only diagonal has non-zero values</li><li>application: represents a simple scalar along the vector space</li></ul></li><li>Triangular matrix<ul><li>only diagonal and the upper right diagonal have non-zero values</li><li>application: easier to solve in system of equations; good for decomposition tasks like <a href=https://en.wikipedia.org/wiki/LU_decomposition>Lower Upper decompositison</a></li></ul></li><li>Sparse matrix<ul><li>matrix is mostly 0s</li><li>There is no interesting transformation, but this represents an opportunity to optimize the memory via more efficient representation.</li></ul></li></ul><h3 id=46-system-of-equations-and-inverse-matrices>4.6 System of Equations and Inverse Matrices<a aria-label="Link to this section" class=paige-header-link href=#46-system-of-equations-and-inverse-matrices></a></h3><ul><li><h2 id=linear-algebra-is-used-to-solve-a-system-equation-via-inverse-matrix>Linear algebra is used to solve a system equation via inverse matrix<a aria-label="Link to this section" class=paige-header-link href=#linear-algebra-is-used-to-solve-a-system-equation-via-inverse-matrix></a></h2></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=n>Given</span>                                            
</span></span><span class=line><span class=cl>		<span class=mi>4</span><span class=n>x</span> <span class=o>+</span> <span class=mi>2</span><span class=n>y</span> <span class=o>+</span> <span class=mi>4</span><span class=n>z</span> <span class=o>=</span> <span class=mi>44</span>
</span></span><span class=line><span class=cl>		<span class=mi>5</span><span class=n>x</span> <span class=o>+</span> <span class=mi>3</span><span class=n>y</span> <span class=o>+</span> <span class=mi>7</span><span class=n>z</span> <span class=o>=</span> <span class=mi>56</span> 
</span></span><span class=line><span class=cl>		<span class=mi>9</span><span class=n>x</span> <span class=o>+</span> <span class=mi>3</span><span class=n>y</span> <span class=o>+</span> <span class=mi>6</span><span class=n>z</span> <span class=o>=</span> <span class=mi>71</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>A</span> <span class=o>=</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>	    <span class=mi>5</span> <span class=mi>3</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>		<span class=mi>9</span> <span class=mi>3</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>B</span> <span class=o>=</span> <span class=mi>44</span>
</span></span><span class=line><span class=cl>	    <span class=mi>56</span>
</span></span><span class=line><span class=cl>	    <span class=mi>72</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>AX</span> <span class=o>=</span> <span class=n>B</span>
</span></span><span class=line><span class=cl>	<span class=n>X</span> <span class=o>=</span> <span class=p>(</span><span class=n>A</span><span class=o>^-</span><span class=mi>1</span><span class=p>)(</span><span class=n>B</span><span class=p>)</span> <span class=c1># !!! WOW</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>A</span> <span class=o>=</span> <span class=n>Matrix</span><span class=p>([</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>9</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>6</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>array</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>numpy.linalg</span> <span class=kn>import</span> <span class=n>inv</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>A</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	    <span class=p>[</span><span class=mi>9</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>6</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=p>])</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>B</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>	    <span class=mi>44</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	    <span class=mi>56</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	    <span class=mi>72</span>
</span></span><span class=line><span class=cl>	<span class=p>])</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>X</span> <span class=o>=</span> <span class=n>inv</span><span class=p>(</span><span class=n>A</span><span class=p>)</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=c1># [ 2. 34. -8.]</span>
</span></span></code></pre></div><h3 id=47-eigenvectors-and-eigenvalues>4.7 Eigenvectors and Eigenvalues<a aria-label="Link to this section" class=paige-header-link href=#47-eigenvectors-and-eigenvalues></a></h3><ul><li><p>Matrix decomposition is breaking up a matrix into its basic components, much like factoring numbers (e.g., 10 can be factored to 2 × 5).</p><ul><li>There are multiple matrix decomposition techniques<ul><li>Linear regression: QR decomposition (Chap 5)</li><li>eigendecomposition (ie used by PCA) (this chapter)</li></ul></li></ul></li><li><p>In eigen decomposition, the original matrix A is decomposed to ==eigenvalue $\lambda$== and ==eigenvector== $\upsilon$
$$A\upsilon =\lambda\upsilon$$</p><ul><li>Example:
$$
A = 1 2
4 5
$$</li></ul></li></ul><p>$$
\lambda = eigenvalue = [-0.464, 6.464]
$$</p><p>$$ \upsilon = eigenvector = [ [0.0806, 0.0343], [0.59, -0.939]] $$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>					<span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>array</span><span class=p>,</span> <span class=n>diag</span>
</span></span><span class=line><span class=cl>					<span class=kn>from</span> <span class=nn>numpy.linalg</span> <span class=kn>import</span> <span class=n>eig</span><span class=p>,</span> <span class=n>inv</span>
</span></span><span class=line><span class=cl>					
</span></span><span class=line><span class=cl>					<span class=n>A</span> <span class=o>=</span> <span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>					    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>					    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>]</span>
</span></span><span class=line><span class=cl>					<span class=p>])</span>
</span></span><span class=line><span class=cl>					
</span></span><span class=line><span class=cl>					<span class=n>eigenvals</span><span class=p>,</span> <span class=n>eigenvecs</span> <span class=o>=</span> <span class=n>eig</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></span><span class=line><span class=cl>					
</span></span><span class=line><span class=cl>					<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;EIGENVALUES&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>					<span class=nb>print</span><span class=p>(</span><span class=n>eigenvals</span><span class=p>)</span>
</span></span><span class=line><span class=cl>					<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>EIGENVECTORS&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>					<span class=nb>print</span><span class=p>(</span><span class=n>eigenvecs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>					
</span></span><span class=line><span class=cl>					<span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>					EIGENVALUES
</span></span></span><span class=line><span class=cl><span class=s2>					[-0.46410162  6.46410162]
</span></span></span><span class=line><span class=cl><span class=s2>					
</span></span></span><span class=line><span class=cl><span class=s2>					EIGENVECTORS
</span></span></span><span class=line><span class=cl><span class=s2>					[[-0.80689822 -0.34372377]
</span></span></span><span class=line><span class=cl><span class=s2>					 [ 0.59069049 -0.9390708 ]]
</span></span></span><span class=line><span class=cl><span class=s2>					&#34;&#34;&#34;</span>
</span></span></code></pre></div><h1 id=5-linear-regression>5 Linear Regression<a aria-label="Link to this section" class=paige-header-link href=#5-linear-regression></a></h1><h3 id=51-basic-linear-regression>5.1 Basic Linear Regression<a aria-label="Link to this section" class=paige-header-link href=#51-basic-linear-regression></a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Import points</span>
</span></span><span class=line><span class=cl>	<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/3goOAnt&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl>	<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Extract output column (all rows, last column)</span>
</span></span><span class=line><span class=cl>	<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Fit a line to the points</span>
</span></span><span class=line><span class=cl>	<span class=n>fit</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># m = 1.7867224, b = -16.51923513</span>
</span></span><span class=line><span class=cl>	<span class=n>m</span> <span class=o>=</span> <span class=n>fit</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=n>b</span> <span class=o>=</span> <span class=n>fit</span><span class=o>.</span><span class=n>intercept_</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;m = </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>m</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;b = </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>b</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># show in chart</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=s1>&#39;o&#39;</span><span class=p>)</span> <span class=c1># scatterplot</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>m</span><span class=o>*</span><span class=n>X</span><span class=o>+</span><span class=n>b</span><span class=p>)</span> <span class=c1># line</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=52-residuals-and-squared-errors>5.2 Residuals and Squared Errors<a aria-label="Link to this section" class=paige-header-link href=#52-residuals-and-squared-errors></a></h3><p>$$ residual_{squared} = (y_{predict} - y_{actual})^2$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Import points</span>
</span></span><span class=line><span class=cl>	<span class=n>points</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://bit.ly/2KF29Bd&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>itertuples</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Test with a given line</span>
</span></span><span class=line><span class=cl>	<span class=n>m</span> <span class=o>=</span> <span class=mf>1.93939</span>
</span></span><span class=line><span class=cl>	<span class=n>b</span> <span class=o>=</span> <span class=mf>4.73333</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>sum_of_squares</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># calculate sum of squares</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>points</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	    <span class=n>y_actual</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span>
</span></span><span class=line><span class=cl>	    <span class=n>y_predict</span> <span class=o>=</span> <span class=n>m</span><span class=o>*</span><span class=n>p</span><span class=o>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>	    <span class=n>residual_squared</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_predict</span> <span class=o>-</span> <span class=n>y_actual</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>	    <span class=n>sum_of_squares</span> <span class=o>+=</span> <span class=n>residual_squared</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;sum of squares = </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>sum_of_squares</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=c1># sum of squares = 28.096969704500005</span>
</span></span></code></pre></div><h3 id=53-finding-the-best-fit-line>5.3 Finding the Best Fit Line<a aria-label="Link to this section" class=paige-header-link href=#53-finding-the-best-fit-line></a></h3><h4 id=531-closed-form>5.3.1 closed form<a aria-label="Link to this section" class=paige-header-link href=#531-closed-form></a></h4><ul><li>not computationally efficient
$$m = \frac{n\sum{xy} - \sum{x}\sum{y}}{n\sum{x^2}-(\sum{x})^2} $$</li></ul><p>$$b = \frac{\sum{y}}{n} - m\frac{\sum{x}}{n} $$</p><h4 id=532-via-linear-algebra-inverse-matrix-technique>5.3.2 Via Linear Algebra (Inverse Matrix Technique)<a aria-label="Link to this section" class=paige-header-link href=#532-via-linear-algebra-inverse-matrix-technique></a></h4><ul><li>Even though ML uses stochastic descent to solve the system of equations, it is instructional to see how linear algebra can be used to arrive at the close form solution</li><li>more numerical stable<ul><li>Numerical stability is how well an algorithm keeps errors minimized, rather than amplifying errors in approximations. Remember that computers work only to so many decimal places and have to approximate, so it becomes important our algorithms do not deteriorate with compounding errors in those approximations</li></ul></li></ul><p>$$b=(X^TX)^{-1} X^T y $$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>numpy.linalg</span> <span class=kn>import</span> <span class=n>inv</span>
</span></span><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Import points</span>
</span></span><span class=line><span class=cl>		<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/3goOAnt&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Add placeholder &#34;1&#34; column to generate intercept</span>
</span></span><span class=line><span class=cl>		<span class=n>X_1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>))])</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract output column (all rows, last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Calculate coefficents for slope and intercept</span>
</span></span><span class=line><span class=cl>		<span class=n>b</span> <span class=o>=</span> <span class=n>inv</span><span class=p>(</span><span class=n>X_1</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span> <span class=o>@</span> <span class=n>X_1</span><span class=p>)</span> <span class=o>@</span> <span class=p>(</span><span class=n>X_1</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span> <span class=o>@</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=n>b</span><span class=p>)</span> <span class=c1># [1.93939394, 4.73333333]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Predict against the y-values</span>
</span></span><span class=line><span class=cl>		<span class=n>y_predict</span> <span class=o>=</span> <span class=n>X_1</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></span></code></pre></div><ul><li>Enhance with QR Matrix Decomposition<ul><li>As the matrix increase in dimension, we want to decompose our original matrix Z
$$X = QR $$
$$ b = R^{-1} Q^T y$$</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>numpy.linalg</span> <span class=kn>import</span> <span class=n>qr</span><span class=p>,</span> <span class=n>inv</span>
</span></span><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Import points</span>
</span></span><span class=line><span class=cl>		<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/3goOAnt&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Add placeholder &#34;1&#34; column to generate intercept</span>
</span></span><span class=line><span class=cl>		<span class=n>X_1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>))])</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract output column (all rows, last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># calculate coefficents for slope and intercept</span>
</span></span><span class=line><span class=cl>		<span class=c1># using QR decomposition</span>
</span></span><span class=line><span class=cl>		<span class=n>Q</span><span class=p>,</span> <span class=n>R</span> <span class=o>=</span> <span class=n>qr</span><span class=p>(</span><span class=n>X_1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>b</span> <span class=o>=</span> <span class=n>inv</span><span class=p>(</span><span class=n>R</span><span class=p>)</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>Q</span><span class=o>.</span><span class=n>transpose</span><span class=p>())</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=533-gradient-descent>5.3.3 Gradient Descent<a aria-label="Link to this section" class=paige-header-link href=#533-gradient-descent></a></h4><ul><li><p>Gradient descent is an optimization technique that uses derivatives and iterations to minimize/maximize a set of parameters against an objective</p><ul><li>so what is the objective?<ul><li>when f(x) is linear equation, it&rsquo;s the minimization of the sum of squares. ==This residual squared is the graph you are trying to minimize.==
$$ residual_{squared} = (y_{predict} - y_{actual})^2$$</li></ul></li></ul></li><li><p>Ex: Gradient descet to find min of a parabola</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=mi>3</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>+</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dx_f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The learning rate</span>
</span></span><span class=line><span class=cl><span class=n>L</span> <span class=o>=</span> <span class=mf>0.001</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The number of iterations to perform gradient descent</span>
</span></span><span class=line><span class=cl><span class=n>iterations</span> <span class=o>=</span> <span class=mi>100_000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=c1># start at a random x</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=o>-</span><span class=mi>15</span><span class=p>,</span><span class=mi>15</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># get slope</span>
</span></span><span class=line><span class=cl>    <span class=n>d_x</span> <span class=o>=</span> <span class=n>dx_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># update x by subtracting the (learning rate) * (slope)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>d_x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=c1># prints 2.999999999999889 4.0</span>
</span></span></code></pre></div></li><li><p>For linear equation, it&rsquo;s the minimization of the sum of squares</p><ul><li>Ex: Gradient desceint for linear regression</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Import points from CSV</span>
</span></span><span class=line><span class=cl><span class=n>points</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://bit.ly/2KF29Bd&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>itertuples</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Building the model</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The learning Rate</span>
</span></span><span class=line><span class=cl><span class=n>L</span> <span class=o>=</span> <span class=mf>.001</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The number of iterations</span>
</span></span><span class=line><span class=cl><span class=n>iterations</span> <span class=o>=</span> <span class=mi>100_000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>points</span><span class=p>))</span>  <span class=c1># Number of elements in X</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Perform Gradient Descent</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># TWC: Find the derivatives of our sum of squares function with respect to _m_ and _b_</span>
</span></span><span class=line><span class=cl>    <span class=c1># slope of squared residual with respect to m</span>
</span></span><span class=line><span class=cl>    <span class=n>D_m</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>x</span> <span class=o>*</span> <span class=p>((</span><span class=n>m</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>b</span><span class=p>)</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># slope of squared residual with respect to b; </span>
</span></span><span class=line><span class=cl>    <span class=n>D_b</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=p>((</span><span class=n>m</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>b</span><span class=p>)</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># update m and b</span>
</span></span><span class=line><span class=cl>    <span class=n>m</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>D_m</span>
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>D_b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y = </span><span class=si>{0}</span><span class=s2>x + </span><span class=si>{1}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>b</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># y = 1.9393939393939548x + 4.733333333333227</span>
</span></span></code></pre></div></li></ul><h3 id=54-overfitting-and-variance>5.4 Overfitting and Variance<a aria-label="Link to this section" class=paige-header-link href=#54-overfitting-and-variance></a></h3><ul><li>The big-picture objective is not to minimize the sum of squares but to make accurate predictions on new data<ul><li>If we fit a nonlinear curve to the data (overfitting), it will get 0 error, but probably will not generalize well to real data.</li><li>Overfitted model are more sensitive to outliers, thereby increase variance in our prediction.</li></ul></li><li></li></ul><h3 id=55-stochasitc-gradient-descent>5.5 Stochasitc Gradient Descent<a aria-label="Link to this section" class=paige-header-link href=#55-stochasitc-gradient-descent></a></h3><h3 id=56-correlation-coefficient>5.6 Correlation Coefficient<a aria-label="Link to this section" class=paige-header-link href=#56-correlation-coefficient></a></h3><h3 id=57-statistical-significance>5.7 Statistical Significance<a aria-label="Link to this section" class=paige-header-link href=#57-statistical-significance></a></h3><h3 id=58-conefficient-of-determination>5.8 Conefficient of Determination<a aria-label="Link to this section" class=paige-header-link href=#58-conefficient-of-determination></a></h3><h3 id=59-standard-error-of-the-estimate>5.9 Standard Error of the Estimate<a aria-label="Link to this section" class=paige-header-link href=#59-standard-error-of-the-estimate></a></h3><h3 id=510-prediction-intervals>5.10 Prediction Intervals<a aria-label="Link to this section" class=paige-header-link href=#510-prediction-intervals></a></h3><h3 id=511-traintest-splits>5.11 Train/Test Splits<a aria-label="Link to this section" class=paige-header-link href=#511-traintest-splits></a></h3><h3 id=512-multiple-linear-regressions>5.12 Multiple Linear Regressions<a aria-label="Link to this section" class=paige-header-link href=#512-multiple-linear-regressions></a></h3><h1 id=6-logistic-regression-and-classification>6 Logistic Regression and Classification<a aria-label="Link to this section" class=paige-header-link href=#6-logistic-regression-and-classification></a></h1><h3 id=61-understanding-logistic-regression>6.1 Understanding Logistic Regression<a aria-label="Link to this section" class=paige-header-link href=#61-understanding-logistic-regression></a></h3><ul><li>A regression that predicts the probability of an outcome given 1+ independent variable.</li><li>Characteristics<ul><li>label = binary, or categorical # (multi-class)</li><li>features: $x_i \in X$</li><li>output = ==probability== -> can be converted into discrete value</li><li>fairly resilient to outliers</li></ul></li></ul><h3 id=62-performing-logistic-regression>6.2 Performing Logistic Regression<a aria-label="Link to this section" class=paige-header-link href=#62-performing-logistic-regression></a></h3><ul><li><p>Logistic Function creates the sigmoid curve</p><ul><li>sigmoid curve transforms an input value x into an output range [0,1]</li><li>$$p(\mathbf{x} = \langle x_{1}, x_{2}, \dots, x_{n}\rangle) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}}$$<ul><li>$\beta_0 + \beta_1$ are learned parameters</li><li>x is the input indpendent variable(s), feature(s)</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>predict_probability</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>b0</span><span class=p>,</span> <span class=n>b1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	    <span class=n>p</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>math</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>	    <span class=k>return</span> <span class=n>p</span>
</span></span></code></pre></div></li></ul><h3 id=621-finding-the-coeffcients>6.2.1 Finding the Coeffcients<a aria-label="Link to this section" class=paige-header-link href=#621-finding-the-coeffcients></a></h3><ul><li>Previously, in linear regression, we used lease sqaures to find the linear equation.<ul><li>We can either used<ul><li>close form$$m = \frac{n\sum{xy} - \sum{x}\sum{y}}{n\sum{x^2}-(\sum{x})^2} $$</li></ul></li></ul></li></ul><p>$$b = \frac{\sum{y}}{n} - m\frac{\sum{x}}{n} $$</p><pre><code>	- inverse matrix 
</code></pre><p>$$b=(X^TX)^{-1} X^T y$$</p><pre><code>	- approximation: (gradient descent)
</code></pre><ul><li>In logistic regression<ul><li>we use <em>maximum likelihood estimation (MLE)</em><ul><li>maximizes the likelihood a given logistic curve would output the observed data (for binary, data label is 0,1)</li><li>MLE(logistic regression) vs least squared (linear regression)</li></ul></li><li>We use gradient descent (or a library) to solve (there&rsquo;s no closed form)<ul><li>sklearn library</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>		<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>		<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Load the data</span>
</span></span><span class=line><span class=cl>		<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/33ebs2R&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Extract output column (all rows, last column)</span>
</span></span><span class=line><span class=cl>		<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># Perform logistic regression</span>
</span></span><span class=line><span class=cl>		<span class=c1># Turn off penalty</span>
</span></span><span class=line><span class=cl>		<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># print beta1</span>
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>flatten</span><span class=p>())</span> <span class=c1># 0.69267212</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=c1># print beta0</span>
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>intercept_</span><span class=o>.</span><span class=n>flatten</span><span class=p>())</span> <span class=c1># -3.17576395</span>
</span></span></code></pre></div><ul><li>MLE (details)<ul><li><p>High level:</p><ul><li>claculate parameters that bring our logistic curve to data points as closely as possible (vs regression: as close to the residual loss squared)</li><li>IMOW: calculate parameter to maximize the likihood probability to fit observed data to the sigmoid curve</li></ul></li><li><p>For binary
$$JointLiklihood = \prod_{i=1}^{n=totalSamples}(\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{y_i} (\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{1-y_i}$$</p><ul><li>1st term applies when data label is 1</li><li>2nd term appleis when data label is 0</li></ul></li><li><p>Mathematical optmization to prevent floating underflow: use log</p><ul><li>multiple saller number gets you smaller number; ME235!</li><li>log transforms multiplication into addition
$$JointLiklihood = \sum_{i=1}^{n=totalSamples}\log(\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{y_i} (\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{1-y_i})$$</li></ul></li></ul></li><li>code1: Joint liklihood definition</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=n>joint_likelihood</span> <span class=o>=</span> <span class=n>Sum</span><span class=p>(</span><span class=n>log</span><span class=p>((</span><span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b</span> <span class=o>+</span> <span class=n>m</span> <span class=o>*</span> <span class=n>x</span><span class=p>(</span><span class=n>i</span><span class=p>)))))</span><span class=o>**</span><span class=n>y</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=o>*</span> \
</span></span><span class=line><span class=cl>		<span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b</span> <span class=o>+</span> <span class=n>m</span> <span class=o>*</span> <span class=n>x</span><span class=p>(</span><span class=n>i</span><span class=p>))))))</span><span class=o>**</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>y</span><span class=p>(</span><span class=n>i</span><span class=p>))),</span> <span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>n</span><span class=p>))</span>
</span></span></code></pre></div><ul><li>code2: use gradient descent (simpy)</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sympy</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>points</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://tinyurl.com/y2cocoo7&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>itertuples</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>n</span> <span class=o>=</span> <span class=n>symbols</span><span class=p>(</span><span class=s1>&#39;b1 b0 i n&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>symbols</span><span class=p>(</span><span class=s1>&#39;x y&#39;</span><span class=p>,</span> <span class=bp>cls</span><span class=o>=</span><span class=n>Function</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>joint_likelihood</span> <span class=o>=</span> <span class=n>Sum</span><span class=p>(</span><span class=n>log</span><span class=p>((</span><span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>x</span><span class=p>(</span><span class=n>i</span><span class=p>)))))</span> <span class=o>**</span> <span class=n>y</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>	<span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>x</span><span class=p>(</span><span class=n>i</span><span class=p>))))))</span> <span class=o>**</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>y</span><span class=p>(</span><span class=n>i</span><span class=p>))),</span> <span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>n</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Partial derivative for m, with points substituted</span>
</span></span><span class=line><span class=cl><span class=n>d_b1</span> <span class=o>=</span> <span class=n>diff</span><span class=p>(</span><span class=n>joint_likelihood</span><span class=p>,</span> <span class=n>b1</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>subs</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>points</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>doit</span><span class=p>()</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>points</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>x</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>points</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Partial derivative for m, with points substituted</span>
</span></span><span class=line><span class=cl><span class=n>d_b0</span> <span class=o>=</span> <span class=n>diff</span><span class=p>(</span><span class=n>joint_likelihood</span><span class=p>,</span> <span class=n>b0</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>subs</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>points</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>doit</span><span class=p>()</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>points</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>x</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>		   <span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>points</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># compile using lambdify for faster computation</span>
</span></span><span class=line><span class=cl><span class=n>d_b1</span> <span class=o>=</span> <span class=n>lambdify</span><span class=p>([</span><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>],</span> <span class=n>d_b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>d_b0</span> <span class=o>=</span> <span class=n>lambdify</span><span class=p>([</span><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>],</span> <span class=n>d_b0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Perform Gradient Descent</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl><span class=n>b0</span> <span class=o>=</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl><span class=n>L</span> <span class=o>=</span> <span class=mf>.01</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10_000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>b1</span> <span class=o>+=</span> <span class=n>d_b1</span><span class=p>(</span><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>)</span> <span class=o>*</span> <span class=n>L</span>
</span></span><span class=line><span class=cl>    <span class=n>b0</span> <span class=o>+=</span> <span class=n>d_b0</span><span class=p>(</span><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>)</span> <span class=o>*</span> <span class=n>L</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>b1</span><span class=p>,</span> <span class=n>b0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 0.6926693075370812 -3.175751550409821</span>
</span></span></code></pre></div><h3 id=63-multivariable-logistic-regression>6.3 Multivariable Logistic Regression<a aria-label="Link to this section" class=paige-header-link href=#63-multivariable-logistic-regression></a></h3><ul><li>I frame the previous 6.2 section as logistic regression</li><li>Example: Employment retention</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>employee_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://tinyurl.com/y6r7qjrp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># grab independent variable columns</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>employee_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># grab dependent &#34;did_quit&#34; variable column</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>employee_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build logistic regression</span>
</span></span><span class=line><span class=cl><span class=n>fit</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print coefficients:</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;COEFFICIENTS: </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>fit</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>flatten</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;INTERCEPT: </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>fit</span><span class=o>.</span><span class=n>intercept_</span><span class=o>.</span><span class=n>flatten</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Interact and test with new employee data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict_employee_will_stay</span><span class=p>(</span><span class=n>sex</span><span class=p>,</span> <span class=n>age</span><span class=p>,</span> <span class=n>promotions</span><span class=p>,</span> <span class=n>years_employed</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>prediction</span> <span class=o>=</span> <span class=n>fit</span><span class=o>.</span><span class=n>predict</span><span class=p>([[</span><span class=n>sex</span><span class=p>,</span> <span class=n>age</span><span class=p>,</span> <span class=n>promotions</span><span class=p>,</span> <span class=n>years_employed</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>    <span class=n>probabilities</span> <span class=o>=</span> <span class=n>fit</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>([[</span><span class=n>sex</span><span class=p>,</span> <span class=n>age</span><span class=p>,</span> <span class=n>promotions</span><span class=p>,</span> <span class=n>years_employed</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>prediction</span> <span class=o>==</span> <span class=p>[[</span><span class=mi>1</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;WILL LEAVE: </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>probabilities</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;WILL STAY: </span><span class=si>{0}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>probabilities</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test a prediction</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>n</span> <span class=o>=</span> <span class=nb>input</span><span class=p>(</span><span class=s2>&#34;Predict employee will stay or leave </span><span class=si>{sex}</span><span class=s2>,</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=n>age</span><span class=p>},{</span><span class=n>promotions</span><span class=p>},{</span><span class=n>years</span> <span class=n>employed</span><span class=p>}:</span> <span class=s2>&#34;)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>sex</span><span class=p>,</span> <span class=n>age</span><span class=p>,</span> <span class=n>promotions</span><span class=p>,</span> <span class=n>years_employed</span><span class=p>)</span> <span class=o>=</span> <span class=n>n</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>predict_employee_will_stay</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=n>sex</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>age</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>promotions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>          <span class=nb>int</span><span class=p>(</span><span class=n>years_employed</span><span class=p>)))</span>
</span></span></code></pre></div><h3 id=64-understanding-the-log-odds>6.4 Understanding the Log-Odds<a aria-label="Link to this section" class=paige-header-link href=#64-understanding-the-log-odds></a></h3><ul><li><p>Log odds used by the logit function</p></li><li><p>Since 1900s, mathematicians wants transform a linear function (==1 neural network layer==), and scale the output to the range of [0,1], corresponding to a probability</p><ul><li>$$JointLiklihood = \sum_{i=1}^{n=totalSamples}\log(\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{y_i} (\frac{1}{1 + e^{-(\beta_0 + \beta_1x_1+..+\beta_nx_n)}})^{1-y_i})$$<ul><li>linear function: $\beta_0 + \beta_1x_1+..+\beta_nx_n$</li><li>==log odds== = $e^{linear function} = e^{\beta_0 + \beta_1x_1+..+\beta_nx_n}$<ul><li>logit function uses log odds; they are not the same</li><li>if we take the log of both side, $$log(\frac{p}{1-p}) = \beta_0 + \beta_1x_1+..+\beta_nx_n$$![[book-math-linear-to-log.png]]</li></ul></li></ul></li></ul></li><li><p>how to convert probability to odd?</p><ul><li>logit function gives us probability p</li><li>$$odd = \frac{p}{1-p}$$</li><li>So in the employment retention example above,<ul><li>if timeOfEmployment=6<ul><li>then logit function returns p = 72.7%</li></ul></li><li>then $odd = \frac{0.727}{1-0.727}$ = 2.665<ul><li>2.66 times more likely to leave</li></ul></li></ul></li><li>More example: see diagram above</li></ul></li></ul><h3 id=65-r-squared>6.5 R-Squared<a aria-label="Link to this section" class=paige-header-link href=#65-r-squared></a></h3><p>-$R^2$  indicates how well a given independent variable explains a dependent variable</p><p>$$R^2 = \frac{logLiklihood - logLiklihoodFit}{logLiklihood}$$
$$ logLiklihoodFit = \sum_{i=0}^n log(f(x_i)) \times y_i+log(1-f(x_i)) \times (1-y_i) $$
$$logLiklihood = \frac{\sum y_i}{n}\times y_i + (1-\frac{\sum y_i}{n})\times (1-y_i) $$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>log</span><span class=p>,</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>patient_data</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/33ebs2R&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>                                <span class=o>.</span><span class=n>itertuples</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Declare fitted logistic regression</span>
</span></span><span class=line><span class=cl><span class=n>b0</span> <span class=o>=</span> <span class=o>-</span><span class=mf>3.17576395</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=mf>0.69267212</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>logistic_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate the log likelihood of the fit</span>
</span></span><span class=line><span class=cl><span class=n>log_likelihood_fit</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>log</span><span class=p>(</span><span class=n>logistic_function</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                         <span class=n>log</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>logistic_function</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                         <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate the log likelihood without fit</span>
</span></span><span class=line><span class=cl><span class=n>likelihood</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>log_likelihood</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>log</span><span class=p>(</span><span class=n>likelihood</span><span class=p>)</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>log</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>likelihood</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate R-Square</span>
</span></span><span class=line><span class=cl><span class=n>r2</span> <span class=o>=</span> <span class=p>(</span><span class=n>log_likelihood</span> <span class=o>-</span> <span class=n>log_likelihood_fit</span><span class=p>)</span> <span class=o>/</span> <span class=n>log_likelihood</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>r2</span><span class=p>)</span>  <span class=c1># 0.306456105756576</span>
</span></span></code></pre></div><h3 id=66-p-values>6.6 P-Values<a aria-label="Link to this section" class=paige-header-link href=#66-p-values></a></h3><ul><li><p>We need to investigate how likely we would have seen this data by chance rather than because of an actual relationship. This means we need a p-value.</p></li><li><p>Chi Squared distribution $\chi^2$</p><ul><li>Degree of freedom = DOF = (# of parameters in our logistic regression - 1)</li><li>$\chi^2$ distribution with DOF = 1<ul><li>sum each value in a normal distribution and saured
$$ pValue = \chi^2(2(logLiklihoodFit) - logLiklihood)$$</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>log</span><span class=p>,</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>chi2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>patient_data</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/33ebs2R&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>itertuples</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Declare fitted logistic regression</span>
</span></span><span class=line><span class=cl><span class=n>b0</span> <span class=o>=</span> <span class=o>-</span><span class=mf>3.17576395</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=mf>0.69267212</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>logistic_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate the log likelihood of the fit</span>
</span></span><span class=line><span class=cl><span class=n>log_likelihood_fit</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>log</span><span class=p>(</span><span class=n>logistic_function</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                         <span class=n>log</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>logistic_function</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                         <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate the log likelihood without fit</span>
</span></span><span class=line><span class=cl><span class=n>likelihood</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>log_likelihood</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>log</span><span class=p>(</span><span class=n>likelihood</span><span class=p>)</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>log</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>likelihood</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=o>.</span><span class=n>y</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>                     <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>patient_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># calculate p-value</span>
</span></span><span class=line><span class=cl><span class=n>chi2_input</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>log_likelihood_fit</span> <span class=o>-</span> <span class=n>log_likelihood</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>p_value</span> <span class=o>=</span> <span class=n>chi2</span><span class=o>.</span><span class=n>pdf</span><span class=p>(</span><span class=n>chi2_input</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=c1># 1 degree of freedom (n - 1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>p_value</span><span class=p>)</span>  <span class=c1># 0.0016604875618753787</span>
</span></span></code></pre></div><h3 id=67-traintest-splits>6.7 Train/Test Splits<a aria-label="Link to this section" class=paige-header-link href=#67-traintest-splits></a></h3><ul><li>Use k-fold validation to reuse all the data for training and test</li><li>Ex: 3fold cross validation</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>KFold</span><span class=p>,</span> <span class=n>cross_val_score</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Load the data</span>
</span></span><span class=line><span class=cl>	<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://tinyurl.com/y6r7qjrp&#34;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># &#34;random_state&#34; is the random seed, which we fix to 7</span>
</span></span><span class=line><span class=cl>	<span class=n>kfold</span> <span class=o>=</span> <span class=n>KFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>results</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=n>kfold</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Accuracy Mean: </span><span class=si>%.3f</span><span class=s2> (stdev=</span><span class=si>%.3f</span><span class=s2>)&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span> <span class=n>results</span><span class=o>.</span><span class=n>std</span><span class=p>()))</span>
</span></span></code></pre></div><h3 id=68-confusion-matrices>6.8 Confusion Matrices<a aria-label="Link to this section" class=paige-header-link href=#68-confusion-matrices></a></h3><pre><code>					Actual:1       Actual:0
</code></pre><p>Predict: 1 TP FN Sensitivity</p><p>Predict: 0 FP TN Specificity</p><pre><code>                     Precision                    Accuracy     
</code></pre><p>$$Precision=\frac{TP}{TP+FP}$$
$$Sensitivty=\frac{TP}{TP+FN}$$
$$Specificity=\frac{TN}{TN+FP}$$
$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$</p><p>$$F1Score=\frac{2\times Precision\times Recall}{Precision+ Recall}$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>confusion_matrix</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Load the data</span>
</span></span><span class=line><span class=cl>	<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/3cManTi&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl>	<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Extract output column (all rows, last column)\</span>
</span></span><span class=line><span class=cl>	<span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>solver</span><span class=o>=</span><span class=s1>&#39;liblinear&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>,</span> <span class=n>Y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>.33</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	    <span class=n>random_state</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>prediction</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>	The confusion matrix evaluates accuracy within each category.
</span></span></span><span class=line><span class=cl><span class=s2>	[[truepositives falsenegatives]
</span></span></span><span class=line><span class=cl><span class=s2>	 [falsepositives truenegatives]]
</span></span></span><span class=line><span class=cl><span class=s2>	
</span></span></span><span class=line><span class=cl><span class=s2>	The diagonal represents correct predictions,
</span></span></span><span class=line><span class=cl><span class=s2>	so we want those to be higher
</span></span></span><span class=line><span class=cl><span class=s2>	&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>	<span class=n>matrix</span> <span class=o>=</span> <span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>Y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=o>=</span><span class=n>prediction</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=n>matrix</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=69-bayes-theorem-and-classification>6.9 Bayes Theorem and Classification<a aria-label="Link to this section" class=paige-header-link href=#69-bayes-theorem-and-classification></a></h3><ul><li>One can use Bayes’ Theorem to bring in outside information to further validate findings on a confusion matrix</li></ul><h3 id=610-receiver-operator-characteristicsarea-under-curve>6.10 Receiver Operator Characteristics/Area Under Curve<a aria-label="Link to this section" class=paige-header-link href=#610-receiver-operator-characteristicsarea-under-curve></a></h3><ul><li>graph used tune the probability threshold to balance for the TPR vs FPR<ul><li>y-axis = TPR = Sensitivity</li><li>x-axis = FPR = (1- Specificty)</li></ul></li><li>AUC is used to choose which model has better performance (tree vs linear regression)</li><li>Code</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=n>results</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=n>kfold</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;AUC: </span><span class=si>%.3f</span><span class=s2> (</span><span class=si>%.3f</span><span class=s2>)&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span> <span class=n>results</span><span class=o>.</span><span class=n>std</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>	<span class=c1># AUC: 0.791 (0.051)</span>
</span></span></code></pre></div><h3 id=611-class-imbalance>6.11 Class Imbalance<a aria-label="Link to this section" class=paige-header-link href=#611-class-imbalance></a></h3><ul><li>Class imbalance is still an open problem with no great solution</li><li>Possibilities<ul><li>collect more data</li><li>try different models</li><li>confusion matrix and AUC to track</li><li>generate synthetic samples of minority data (SMOTE algoritm)</li><li>duplication minority samples</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>   	<span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=o>=</span> <span class=o>...</span>
</span></span><span class=line><span class=cl>  	<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>,</span> <span class=n>Y_test</span> <span class=o>=</span>  \
</span></span><span class=line><span class=cl>  		<span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>.33</span><span class=p>,</span> <span class=n>stratify</span><span class=o>=</span><span class=n>Y</span><span class=p>)</span>
</span></span></code></pre></div></li></ul><h1 id=7-neural-network>7 Neural Network<a aria-label="Link to this section" class=paige-header-link href=#7-neural-network></a></h1><ul><li>A neural network is a multilayered ==regression== containing layers of weights, biases, and nonlinear functions that reside between input variables and output variables</li><li>Each node resembles a linear function before being passed to a nonlinear function (called an activation function)</li></ul><h3 id=71-when-to-use-neural-network-and-deep-learning>7.1 When To Use Neural Network and Deep Learning<a aria-label="Link to this section" class=paige-header-link href=#71-when-to-use-neural-network-and-deep-learning></a></h3><ul><li>Neural network and DL can be used for classification or regression</li><li>Linear regression, logistic regression, and GBM trees do well on structured data<ul><li>structure data can be represented as a table</li></ul></li><li>Use NN for unstructure data:<ul><li>text: predict the next token</li><li>images: classify a text</li></ul></li></ul><h3 id=72-simple-neural-network>7.2 Simple Neural Network<a aria-label="Link to this section" class=paige-header-link href=#72-simple-neural-network></a></h3><ul><li>node: a linear equation consiting of $$ X_1W_1 + X_2W_2+B_1 $$<ul><li>where W are the weights, which are the arrows/path to the node</li><li>X is the input from the previous layer</li></ul></li><li>1 layer = N nodes</li><li>M layers</li><li>So far, it looks similar to linear regression</li></ul><h4 id=721-activation-function>7.2.1 Activation Function<a aria-label="Link to this section" class=paige-header-link href=#721-activation-function></a></h4><ul><li>An activation function is a nonlinear function that transforms or compresses the weighted and summed values in a node, helping the neural network separate the data effectively so it can be classified.</li><li>Types of Activation function<table><thead><tr><th style=text-align:left>Name</th><th style=text-align:center>Typical Layer</th><th style=text-align:right>Description</th><th style=text-align:right>Notes</th></tr></thead><tbody><tr><td style=text-align:left>Logistic</td><td style=text-align:center>Output</td><td style=text-align:right>S-shaped sigmoid curve</td><td style=text-align:right>output in [0,1]; used in binary classifcation</td></tr><tr><td style=text-align:left>Softmax</td><td style=text-align:center>Output</td><td style=text-align:right>Ensures all outputs nodes sumes to 1</td><td style=text-align:right>Useful for multiple classifcations and scale outputs add to 1.0</td></tr><tr><td style=text-align:left>Tangent Hyperbolic</td><td style=text-align:center>Output</td><td style=text-align:right>tanh, S-shaped sigmoid curve between -1 and 1</td><td style=text-align:right>Assists in “centering” data by bringing mean close to 0</td></tr><tr><td style=text-align:left>Relu</td><td style=text-align:center>Hidden</td><td style=text-align:right>Turns negative values to 0</td><td style=text-align:right>Popular activation faster than sigmoid and tanh, mitigates vanishing gradient problems and computationally cheap</td></tr><tr><td style=text-align:left>Leaky Relu</td><td style=text-align:center>Hidden</td><td style=text-align:right>Multiples neg values by 0.01</td><td style=text-align:right>Controversial variant of ReLU that marginalizes rather than eliminates negative values</td></tr></tbody></table></li></ul><h4 id=722-forward-propagation>7.2.2 Forward Propagation<a aria-label="Link to this section" class=paige-header-link href=#722-forward-propagation></a></h4><ul><li>Mental model with L layers<ul><li>input X &ndash;> Hidden: $Relu( W @ X + B)<em>{layer-1}$ &ndash;> output: $logistic(W@X + B)</em>{layer_L}$</li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>	<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>	<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>all_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://tinyurl.com/y2qmhfsr&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Extract the input columns, scale down by 255</span>
</span></span><span class=line><span class=cl>	<span class=n>all_inputs</span> <span class=o>=</span> <span class=p>(</span><span class=n>all_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span><span class=o>.</span><span class=n>values</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>all_outputs</span> <span class=o>=</span> <span class=n>all_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Split train and test data sets</span>
</span></span><span class=line><span class=cl>	<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>,</span> <span class=n>Y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>all_inputs</span><span class=p>,</span> <span class=n>all_outputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	    <span class=n>test_size</span><span class=o>=</span><span class=mi>1</span><span class=o>/</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>n</span> <span class=o>=</span> <span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1># number of training records</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Build neural network with weights and biases</span>
</span></span><span class=line><span class=cl>	<span class=c1># with random initialization</span>
</span></span><span class=line><span class=cl>	<span class=n>w_hidden</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>w_output</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>b_hidden</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>b_output</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Activation functions</span>
</span></span><span class=line><span class=cl>	<span class=n>relu</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>logistic</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Runs inputs through the neural network to get predicted outputs</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward_prop</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	    <span class=n>Z1</span> <span class=o>=</span> <span class=n>w_hidden</span> <span class=o>@</span> <span class=n>X</span> <span class=o>+</span> <span class=n>b_hidden</span>
</span></span><span class=line><span class=cl>	    <span class=n>A1</span> <span class=o>=</span> <span class=n>relu</span><span class=p>(</span><span class=n>Z1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	    <span class=n>Z2</span> <span class=o>=</span> <span class=n>w_output</span> <span class=o>@</span> <span class=n>A1</span> <span class=o>+</span> <span class=n>b_output</span>
</span></span><span class=line><span class=cl>	    <span class=n>A2</span> <span class=o>=</span> <span class=n>logistic</span><span class=p>(</span><span class=n>Z2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	    <span class=k>return</span> <span class=n>Z1</span><span class=p>,</span> <span class=n>A1</span><span class=p>,</span> <span class=n>Z2</span><span class=p>,</span> <span class=n>A2</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Calculate accuracy</span>
</span></span><span class=line><span class=cl>	<span class=n>test_predictions</span> <span class=o>=</span> <span class=n>forward_prop</span><span class=p>(</span><span class=n>X_test</span><span class=o>.</span><span class=n>transpose</span><span class=p>())[</span><span class=mi>3</span><span class=p>]</span> <span class=c1># grab only output layer, A2</span>
</span></span><span class=line><span class=cl>	<span class=n>test_comparisons</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>equal</span><span class=p>((</span><span class=n>test_predictions</span> <span class=o>&gt;=</span> <span class=mf>.5</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>),</span> <span class=n>Y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>accuracy</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>test_comparisons</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span> <span class=o>/</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;ACCURACY: &#34;</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=73-backpropagation>7.3 Backpropagation<a aria-label="Link to this section" class=paige-header-link href=#73-backpropagation></a></h3><h4 id=731-calculate-the-weight-and-bias-derivatives>7.3.1 Calculate the Weight and Bias Derivatives<a aria-label="Link to this section" class=paige-header-link href=#731-calculate-the-weight-and-bias-derivatives></a></h4><ul><li>For each layer, we do a partial derivative of the cost function<ul><li>$C=(A_{layer_i-1}^2-Y_{layer_i-1})$<ul><li>C is the cost function</li><li>$A_{layer-1}$ is the output of layer i-1<ul><li>= ReLU($Z_{layer-1} = (W @ X + B)_{layer-2}$)</li></ul></li></ul></li></ul></li></ul><h4 id=732-stochastic-gradient-descent>7.3.2 Stochastic Gradient Descent<a aria-label="Link to this section" class=paige-header-link href=#732-stochastic-gradient-descent></a></h4><p>Raw</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>all_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;https://tinyurl.com/y2qmhfsr&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Learning rate controls how slowly we approach a solution</span>
</span></span><span class=line><span class=cl><span class=c1># Make it too small, it will take too long to run.</span>
</span></span><span class=line><span class=cl><span class=c1># Make it too big, it will likely overshoot and miss the solution.</span>
</span></span><span class=line><span class=cl><span class=n>L</span> <span class=o>=</span> <span class=mf>0.05</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract the input columns, scale down by 255</span>
</span></span><span class=line><span class=cl><span class=n>all_inputs</span> <span class=o>=</span> <span class=p>(</span><span class=n>all_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span><span class=o>.</span><span class=n>values</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>all_outputs</span> <span class=o>=</span> <span class=n>all_data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Split train and test data sets</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>,</span> <span class=n>Y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>all_inputs</span><span class=p>,</span> <span class=n>all_outputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>test_size</span><span class=o>=</span><span class=mi>1</span> <span class=o>/</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build neural network with weights and biases</span>
</span></span><span class=line><span class=cl><span class=c1># with random initialization</span>
</span></span><span class=line><span class=cl><span class=n>w_hidden</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w_output</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>b_hidden</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b_output</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Activation functions</span>
</span></span><span class=line><span class=cl><span class=n>relu</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logistic</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Runs inputs through the neural network to get predicted outputs</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward_prop</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>Z1</span> <span class=o>=</span> <span class=n>w_hidden</span> <span class=o>@</span> <span class=n>X</span> <span class=o>+</span> <span class=n>b_hidden</span>
</span></span><span class=line><span class=cl>    <span class=n>A1</span> <span class=o>=</span> <span class=n>relu</span><span class=p>(</span><span class=n>Z1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>Z2</span> <span class=o>=</span> <span class=n>w_output</span> <span class=o>@</span> <span class=n>A1</span> <span class=o>+</span> <span class=n>b_output</span>
</span></span><span class=line><span class=cl>    <span class=n>A2</span> <span class=o>=</span> <span class=n>logistic</span><span class=p>(</span><span class=n>Z2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>Z1</span><span class=p>,</span> <span class=n>A1</span><span class=p>,</span> <span class=n>Z2</span><span class=p>,</span> <span class=n>A2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Derivatives of Activation functions</span>
</span></span><span class=line><span class=cl><span class=n>d_relu</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>d_logistic</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># returns slopes for weights and biases</span>
</span></span><span class=line><span class=cl><span class=c1># using chain rule</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>backward_prop</span><span class=p>(</span><span class=n>Z1</span><span class=p>,</span> <span class=n>A1</span><span class=p>,</span> <span class=n>Z2</span><span class=p>,</span> <span class=n>A2</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>dC_dA2</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>A2</span> <span class=o>-</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>Y</span>
</span></span><span class=line><span class=cl>    <span class=n>dA2_dZ2</span> <span class=o>=</span> <span class=n>d_logistic</span><span class=p>(</span><span class=n>Z2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dZ2_dA1</span> <span class=o>=</span> <span class=n>w_output</span>
</span></span><span class=line><span class=cl>    <span class=n>dZ2_dW2</span> <span class=o>=</span> <span class=n>A1</span>
</span></span><span class=line><span class=cl>    <span class=n>dZ2_dB2</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>dA1_dZ1</span> <span class=o>=</span> <span class=n>d_relu</span><span class=p>(</span><span class=n>Z1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dZ1_dW1</span> <span class=o>=</span> <span class=n>X</span>
</span></span><span class=line><span class=cl>    <span class=n>dZ1_dB1</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dC_dW2</span> <span class=o>=</span> <span class=n>dC_dA2</span> <span class=o>@</span> <span class=n>dA2_dZ2</span> <span class=o>@</span> <span class=n>dZ2_dW2</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dC_dB2</span> <span class=o>=</span> <span class=n>dC_dA2</span> <span class=o>@</span> <span class=n>dA2_dZ2</span> <span class=o>*</span> <span class=n>dZ2_dB2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dC_dA1</span> <span class=o>=</span> <span class=n>dC_dA2</span> <span class=o>@</span> <span class=n>dA2_dZ2</span> <span class=o>@</span> <span class=n>dZ2_dA1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dC_dW1</span> <span class=o>=</span> <span class=n>dC_dA1</span> <span class=o>@</span> <span class=n>dA1_dZ1</span> <span class=o>@</span> <span class=n>dZ1_dW1</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dC_dB1</span> <span class=o>=</span> <span class=n>dC_dA1</span> <span class=o>@</span> <span class=n>dA1_dZ1</span> <span class=o>*</span> <span class=n>dZ1_dB1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dC_dW1</span><span class=p>,</span> <span class=n>dC_dB1</span><span class=p>,</span> <span class=n>dC_dW2</span><span class=p>,</span> <span class=n>dC_dB2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Execute gradient descent</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100_000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># randomly select one of the training data</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X_sample</span> <span class=o>=</span> <span class=n>X_train</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=o>.</span><span class=n>transpose</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>Y_sample</span> <span class=o>=</span> <span class=n>Y_train</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># run randomly selected training data through neural network</span>
</span></span><span class=line><span class=cl>    <span class=n>Z1</span><span class=p>,</span> <span class=n>A1</span><span class=p>,</span> <span class=n>Z2</span><span class=p>,</span> <span class=n>A2</span> <span class=o>=</span> <span class=n>forward_prop</span><span class=p>(</span><span class=n>X_sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># distribute error through backpropagation</span>
</span></span><span class=line><span class=cl>    <span class=c1># and return slopes for weights and biases</span>
</span></span><span class=line><span class=cl>    <span class=n>dW1</span><span class=p>,</span> <span class=n>dB1</span><span class=p>,</span> <span class=n>dW2</span><span class=p>,</span> <span class=n>dB2</span> <span class=o>=</span> <span class=n>backward_prop</span><span class=p>(</span><span class=n>Z1</span><span class=p>,</span> <span class=n>A1</span><span class=p>,</span> <span class=n>Z2</span><span class=p>,</span> <span class=n>A2</span><span class=p>,</span> <span class=n>X_sample</span><span class=p>,</span> <span class=n>Y_sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># update weights and biases</span>
</span></span><span class=line><span class=cl>    <span class=n>w_hidden</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>dW1</span>
</span></span><span class=line><span class=cl>    <span class=n>b_hidden</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>dB1</span>
</span></span><span class=line><span class=cl>    <span class=n>w_output</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>dW2</span>
</span></span><span class=line><span class=cl>    <span class=n>b_output</span> <span class=o>-=</span> <span class=n>L</span> <span class=o>*</span> <span class=n>dB2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate accuracy</span>
</span></span><span class=line><span class=cl><span class=n>test_predictions</span> <span class=o>=</span> <span class=n>forward_prop</span><span class=p>(</span><span class=n>X_test</span><span class=o>.</span><span class=n>transpose</span><span class=p>())[</span><span class=mi>3</span><span class=p>]</span>  <span class=c1># grab only A2</span>
</span></span><span class=line><span class=cl><span class=n>test_comparisons</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>equal</span><span class=p>((</span><span class=n>test_predictions</span> <span class=o>&gt;=</span> <span class=mf>.5</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>),</span> <span class=n>Y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>accuracy</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>test_comparisons</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span> <span class=o>/</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;ACCURACY: &#34;</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span></code></pre></div><ul><li>Using scikit</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=c1># load data</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.neural_network</span> <span class=kn>import</span> <span class=n>MLPClassifier</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/3GsNzGt&#39;</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&#34;,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract input variables (all rows, all columns but last column)</span>
</span></span><span class=line><span class=cl><span class=c1># Note we should do some linear scaling here</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract output column (all rows, last column)</span>
</span></span><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Separate training and testing data</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>,</span> <span class=n>Y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mi>1</span><span class=o>/</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nn</span> <span class=o>=</span> <span class=n>MLPClassifier</span><span class=p>(</span><span class=n>solver</span><span class=o>=</span><span class=s1>&#39;sgd&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>hidden_layer_sizes</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=p>),</span>
</span></span><span class=line><span class=cl>                   <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>max_iter</span><span class=o>=</span><span class=mi>100_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>learning_rate_init</span><span class=o>=</span><span class=mf>.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print weights and biases</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>coefs_</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>intercepts_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Training set score: </span><span class=si>%f</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>nn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>Y_train</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Test set score: </span><span class=si>%f</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>nn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>Y_test</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=74-limitation-of-neural-networks-and-deep-learning>7.4 Limitation of Neural Networks and Deep Learning<a aria-label="Link to this section" class=paige-header-link href=#74-limitation-of-neural-networks-and-deep-learning></a></h3><ul><li>NN can overfit<ul><li>layers, nodes, and activation functions makes it flexible fitting to data in a nonlinear manner</li></ul></li></ul><h3 id=75-conclusion>7.5 Conclusion<a aria-label="Link to this section" class=paige-header-link href=#75-conclusion></a></h3><h1 id=8-career-advice-and-path-forward>8 Career Advice and Path Forward<a aria-label="Link to this section" class=paige-header-link href=#8-career-advice-and-path-forward></a></h1><ul><li>data science is software engineering with proficiency in statistics, machine learning, and optimization</li></ul></div></div></article></main><footer class=mb-3 id=paige-footer><div class=mb-3 id=paige-prev-next><p class="mb-0 text-center text-secondary" id=paige-next><a class=link-secondary href=http://localhost:1313/books/machine-learning/hands-on-large-language-model/>Hands On Large Language Model</a> &#8250;</p><p class="mb-0 text-center text-secondary" id=paige-prev>&#8249; <a class=link-secondary href=http://localhost:1313/books/machine-learning/data-science-from-scratch/>DataScience From Scratch</a></p></div></footer></div></div></div><script crossorigin=anonymous defer integrity="sha256-gfPiYYSP+RNtNeRBt45mJombFJmNf0V0Ipu785Obi0M=" referrerpolicy=no-referrer src=/js/paige/bootstrap/bootstrap.bundle.min.81f3e261848ff9136d35e441b78e6626899b14998d7f4574229bbbf3939b8b43.js></script><script crossorigin=anonymous defer integrity="sha256-kqnbKZyzZFmn0J2JtRxwdeNh0b3FknYDyAd+mGKI73c=" referrerpolicy=no-referrer src=/js/paige/katex/katex.min.min.92a9db299cb36459a7d09d89b51c7075e361d1bdc5927603c8077e986288ef77.js></script><script crossorigin=anonymous defer integrity="sha256-pQMCGuz+5jpyog4zEVyH/xvxJRsPM9M7d7t6+TiyEA8=" onload=renderMathInElement(document.body) referrerpolicy=no-referrer src=/js/paige/katex/auto-render.min.min.a503021aecfee63a72a20e33115c87ff1bf1251b0f33d33b77bb7af938b2100f.js></script><noscript>JavaScript is required</noscript></body></html>